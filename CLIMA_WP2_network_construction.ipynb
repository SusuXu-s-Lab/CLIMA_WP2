{"cells":[{"cell_type":"markdown","metadata":{"id":"YNNEIVp8v74Y"},"source":["#Libraries and Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10789,"status":"ok","timestamp":1746082952783,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"-X5Mpuk7RBEc","outputId":"12a1a03e-8d42-4643-a708-89849a9c98de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-geohash in /usr/local/lib/python3.11/dist-packages (0.8.5)\n","Requirement already satisfied: pygeohash in /usr/local/lib/python3.11/dist-packages (3.0.1)\n","Requirement already satisfied: fastdtw in /usr/local/lib/python3.11/dist-packages (0.3.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastdtw) (2.0.2)\n"]}],"source":["!pip install python-geohash\n","!pip install pygeohash\n","!pip install fastdtw"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WOHX1RnO8GCo","executionInfo":{"status":"ok","timestamp":1746082957875,"user_tz":240,"elapsed":5085,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import json\n","import pyarrow.parquet as pq\n","from collections import defaultdict\n","from google.colab  import drive\n","from datetime import datetime, timedelta\n","import folium\n","from folium.plugins import MarkerCluster\n","from collections import defaultdict\n","from folium.plugins import HeatMap\n","import joblib\n","import math, bisect, random\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","import pdb\n","from tqdm import tqdm\n","from itertools import combinations\n","import geohash\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import scipy.signal as signal\n","import hashlib\n","from networkx.algorithms.community import greedy_modularity_communities, modularity\n","import itertools\n","import pygeohash as pgh\n","import math, random, numpy as np, pandas as pd\n","from itertools import combinations\n","from tqdm import tqdm\n","from fastdtw import fastdtw\n","from numba import njit, prange\n","from numba.typed import List as NbList"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1746082958446,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"kpXP_NhdvFoI","outputId":"f9449101-e642-4823-dea7-61d472f354ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount('/content/drive')\n","myDriveFile = \"/content/drive/MyDrive/CLIMATOY/\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SaLUy2bjKXMg","executionInfo":{"status":"ok","timestamp":1746082958466,"user_tz":240,"elapsed":20,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["# Define the pre disaster date range\n","start_date = datetime(2019, 1, 1)\n","end_date = datetime(2020, 1, 1)\n","\n","# 1. Filter Holiday records from 2019-12-24 18:00 to 22:00\n","start_time = pd.Timestamp(\"2019-12-24 18:00:00\")\n","end_time   = pd.Timestamp(\"2019-12-24 22:00:00\")\n","# start_time = pd.Timestamp(\"2020-01-01 18:00:00\")\n","# end_time   = pd.Timestamp(\"2020-01-01 22:00:00\")\n","\n","# Base path\n","myDriveFile = \"/content/drive/MyDrive/CLIMATOY/\"\n","base_path = myDriveFile + 'toy datasets/'\n","\n","# Define the bounding box for Florida\n","# min_lat, max_lat = 25.396308, 29.000968\n","# min_lon, max_lon = -83.634896, -80.031362\n","\n","\n","#  bounding box for Longisland\n","min_lat, max_lat = 40.55, 41.05\n","min_lon, max_lon = -73.95, -71.85\n","\n","# A small county in the middle of Long Island\n","# min_lat, max_lat = 40.75, 40.85\n","# min_lon, max_lon = -73.05, -72.85\n","\n","\n","def hash_user_id(user_id):\n","    return hashlib.sha256(str(user_id).encode()).hexdigest()[:10]\n"]},{"cell_type":"markdown","metadata":{"id":"dd7vse5XwCR6"},"source":["# Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFTNvXwtyxyW"},"outputs":[],"source":["#Loading Data and do filtering based on study region\n","\n","\n","dfs = []\n","\n","current_date = start_date\n","while current_date <= end_date:\n","    date_str = current_date.strftime('%Y%m%d')\n","    parquet_file_path = f\"{base_path}{date_str}/\"\n","\n","    try:\n","        print(f\"Reading data for {date_str}...\")\n","        df_temp = pd.read_parquet(parquet_file_path, engine='pyarrow')\n","\n","        # Add a date column to keep track of the source date (optional)\n","        df_temp['source_date'] = date_str\n","        dfs.append(df_temp)\n","        print(f\"Successfully read data with shape: {df_temp.shape}\")\n","\n","    except Exception as e:\n","        print(f\"Error reading data for {date_str}: {e}\")\n","\n","    current_date += timedelta(days=1)\n","\n","# Check if we have any dataframes to concatenate\n","if len(dfs) > 0:\n","    combined_df = pd.concat(dfs, ignore_index=True)\n","    print(\"\\nCombined Dataframe Info:\")\n","    print(f\"Shape: {combined_df.shape}\")\n","    print(f\"Column Names: {combined_df.columns.tolist()}\")\n","    print(\"\\nFirst 5 rows of combined data:\")\n","    print(combined_df.head(5))\n","\n","    combined_df.to_parquet(myDriveFile+\"combined_data.parquet\")\n","else:\n","    print(\"No data was read successfully.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22593,"status":"ok","timestamp":1746082981068,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"ipbo7Vhoz3Ar","outputId":"229b1983-cfed-44da-a15d-1d810ddf2e32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original DataFrame shape: (5361762, 30)\n","Filtered DataFrame shape: (1700081, 30)\n","Filtered 3661681 rows\n"]}],"source":["parquet_file_path = myDriveFile + \"combined_data.parquet\"\n","combined_df = pd.read_parquet(parquet_file_path, engine='pyarrow')\n","\n","# Filter rows based on the bounding box\n","filtered_df = combined_df[\n","    (combined_df[\"latitude_2\"] >= min_lat) & (combined_df[\"latitude_2\"] <= max_lat) |\n","    (combined_df[\"longitude_2\"] >= min_lon) & (combined_df[\"longitude_2\"] <= max_lon)\n","]\n","\n","print(f\"Original DataFrame shape: {combined_df.shape}\")\n","print(f\"Filtered DataFrame shape: {filtered_df.shape}\")\n","print(f\"Filtered {combined_df.shape[0] - filtered_df.shape[0]} rows\")\n","\n","# output_path = myDriveFile + \"filtered_florida_data.parquet\"\n","# filtered_df.to_parquet(output_path)\n","# print(f\"Saved filtered data to {output_path}\")\n","\n","# # If you also want a CSV version\n","# csv_output_path = myDriveFile + \"filtered_florida_data.csv\"\n","# filtered_df.to_csv(csv_output_path, sep=\"\\t\", index=False)\n","# print(f\"Also saved as CSV to {csv_output_path}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17722,"status":"ok","timestamp":1746082998789,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"vHTC9gUJxc_a","outputId":"658fcbb8-93a4-4b8e-fa0d-2fa8464e8a1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-eb752a8c70f3>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_df[\"geohash_1\"] = geohash_1_list\n","<ipython-input-6-eb752a8c70f3>:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_df[\"geohash_2\"] = geohash_2_list\n","<ipython-input-6-eb752a8c70f3>:33: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_df['unix_time_1'] = filtered_df['utc_timestamp_1'] + filtered_df['utc_offset_1']\n","<ipython-input-6-eb752a8c70f3>:34: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_df['unix_time_2'] = filtered_df['utc_timestamp_2'] + filtered_df['utc_offset_2']\n"]},{"output_type":"stream","name":"stdout","text":["1700081\n","70087\n"]}],"source":["geohash_1_list = []\n","geohash_2_list = []\n","\n","for lat1, lon1, lat2, lon2 in zip(\n","    filtered_df[\"latitude_1\"], filtered_df[\"longitude_1\"],\n","    filtered_df[\"latitude_2\"], filtered_df[\"longitude_2\"]\n","):\n","    geohash_1_list.append(geohash.encode(lat1, lon1, precision=9))\n","    geohash_2_list.append(geohash.encode(lat2, lon2, precision=9))\n","\n","filtered_df[\"geohash_1\"] = geohash_1_list\n","filtered_df[\"geohash_2\"] = geohash_2_list\n","\n","\n","# Keep only the specified columns\n","columns_to_keep = [\n","    'device_id',\n","    'linked_trip_id',\n","    'utc_timestamp_1',\n","    'utc_offset_1',\n","    'utc_timestamp_2',\n","    'utc_offset_2',\n","    'latitude_1',\n","    'longitude_1',\n","    'geohash_1',\n","    'latitude_2',\n","    'longitude_2',\n","    'geohash_2'\n","]\n","filtered_df = filtered_df[columns_to_keep]\n","\n","# Create unix_time columns (adding offset to timestamps)\n","filtered_df['unix_time_1'] = filtered_df['utc_timestamp_1'] + filtered_df['utc_offset_1']\n","filtered_df['unix_time_2'] = filtered_df['utc_timestamp_2'] + filtered_df['utc_offset_2']\n","\n","# Generate human-readable timestamps\n","def unix_to_mdy_hms(unix_time):\n","    return datetime.fromtimestamp(unix_time).strftime('%m/%d/%Y %H:%M:%S')\n","\n","filtered_df['timestamp_1'] = filtered_df['unix_time_1'].apply(unix_to_mdy_hms)\n","filtered_df['timestamp_2'] = filtered_df['unix_time_2'].apply(unix_to_mdy_hms)\n","\n","# Add original_device_id column (ground truth)\n","filtered_df['original_device_id'] = filtered_df['device_id']\n","\n","# Convert timestamps to datetime for date operations\n","filtered_df['datetime_1'] = pd.to_datetime(filtered_df['timestamp_1'])\n","filtered_df['date_1'] = filtered_df['datetime_1'].dt.date\n","\n","# Hash coding\n","filtered_df['device_id']=filtered_df['device_id'].apply(hash_user_id)\n","\n","# Get the total number of unique device_ids\n","all_device_ids = filtered_df['device_id'].unique()\n","total_device_ids = len(all_device_ids)\n","print(len(filtered_df))\n","print(total_device_ids)"]},{"cell_type":"markdown","metadata":{"id":"CEdi5svD_oJ5"},"source":["#Shuffling and Deshuffling"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"HMER6XCPEZN6","executionInfo":{"status":"ok","timestamp":1746085380848,"user_tz":240,"elapsed":1,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["def link_device_trajectories_optimized(df, max_time_gap_seconds=3600, geohash_digit_tolerance=8):\n","    \"\"\"\n","    Links device trajectories based on temporal proximity and geohash similarity.\n","    When a device ID disappears, finds the new device ID with the closest geohash match within the specified time window.\n","    Also handles the case of device ID shuffling:\n","      1. Before the function starts, it reads a JSON file (default: device_id_shuffle_map.json).\n","         If a device_id in df is found in this mapping (i.e., it's a new ID), it will be replaced with its original ID.\n","      2. During the linking process, if a device ID link is detected (i.e., original ID differs from new ID),\n","         the new-to-original ID pair will be added to the JSON mapping file.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame containing trajectory data with device_id, unix_time_1,\n","        unix_time_2, geohash_1, geohash_2\n","    max_time_gap_seconds : int\n","        Maximum allowed time gap between trajectories to be linked (in seconds)\n","    geohash_digit_tolerance : int\n","        Minimum number of matching digits required in geohash comparison\n","\n","    Returns:\n","    --------\n","    pandas.DataFrame\n","        DataFrame with linked device IDs\n","    dict\n","        Mapping of device IDs that were linked\n","    \"\"\"\n","    import numpy as np\n","    from collections import defaultdict\n","    import os\n","    import json\n","\n","    # --- New: Load original-to-shuffled ID mapping from JSON and replace new IDs in df ---\n","    shuffle_file = \"device_id_shuffle_map.json\"\n","    if os.path.exists(shuffle_file):\n","        with open(shuffle_file, 'r') as f:\n","            shuffle_mapping = json.load(f)\n","        print(f\"Loaded existing shuffle mapping from {shuffle_file}.\")\n","    else:\n","        shuffle_mapping = {}\n","        print(f\"No existing shuffle mapping found. Starting with an empty mapping.\")\n","\n","    # Replace new device IDs in df with original IDs using the loaded mapping\n","    df['device_id'] = df['device_id'].apply(lambda x: shuffle_mapping.get(x, x))\n","\n","    # -----------------------------------------------------------------------------------\n","\n","    print(\"Starting trajectory linking process...\")\n","    print(f\"Parameters: max_time_gap={max_time_gap_seconds}s, geohash_digits={geohash_digit_tolerance}\")\n","\n","    linked_df = df.copy()\n","    linked_df = linked_df.sort_values(['device_id', 'unix_time_1'])\n","\n","    device_groups = linked_df.groupby('device_id')\n","\n","    device_last_points = {}\n","    for device_id, group in device_groups:\n","        last_row = group.loc[group['unix_time_2'].idxmax()]\n","        device_last_points[device_id] = {\n","            'time': last_row['unix_time_2'],\n","            'geohash': last_row['geohash_2'],\n","            'last_seen_idx': last_row.name\n","        }\n","\n","    device_first_points = {}\n","    for device_id, group in device_groups:\n","        first_row = group.loc[group['unix_time_1'].idxmin()]\n","        device_first_points[device_id] = {\n","            'time': first_row['unix_time_1'],\n","            'geohash': first_row['geohash_1'],\n","            'first_seen_idx': first_row.name\n","        }\n","\n","    print(f\"Identified last points for {len(device_last_points)} device IDs\")\n","    print(f\"Identified first points for {len(device_first_points)} device IDs\")\n","\n","    geohash_similarity_cache = {}\n","    def geohash_similarity(geohash1, geohash2):\n","        cache_key = (geohash1, geohash2)\n","        if cache_key in geohash_similarity_cache:\n","            return geohash_similarity_cache[cache_key]\n","\n","        min_len = min(len(geohash1), len(geohash2))\n","        for i in range(min_len):\n","            if geohash1[i] != geohash2[i]:\n","                result = i\n","                geohash_similarity_cache[cache_key] = result\n","                return result\n","        result = min_len\n","        geohash_similarity_cache[cache_key] = result\n","        return result\n","\n","    time_buckets = defaultdict(list)\n","    for device_id, first_point in device_first_points.items():\n","        time_bucket = first_point['time'] // max_time_gap_seconds\n","        time_buckets[time_bucket].append((device_id, first_point))\n","\n","    linked_device_pairs = []\n","    unlinked_device_count = 0\n","    already_linked_new_devices = set()\n","\n","    for device_id, last_point in device_last_points.items():\n","        last_time_bucket = last_point['time'] // max_time_gap_seconds\n","        relevant_buckets = [last_time_bucket, last_time_bucket + 1]\n","\n","        best_match = None\n","        best_similarity = -1\n","\n","        for bucket in relevant_buckets:\n","            if bucket not in time_buckets:\n","                continue\n","\n","            for new_device_id, first_point in time_buckets[bucket]:\n","                if (new_device_id == device_id or\n","                    first_point['time'] <= last_point['time'] or\n","                    new_device_id in already_linked_new_devices):\n","                    continue\n","\n","                time_diff = first_point['time'] - last_point['time']\n","                if time_diff > max_time_gap_seconds:\n","                    continue\n","\n","                similarity = geohash_similarity(last_point['geohash'], first_point['geohash'])\n","                if similarity >= geohash_digit_tolerance and similarity > best_similarity:\n","                    best_match = new_device_id\n","                    best_similarity = similarity\n","\n","        if best_match:\n","            linked_device_pairs.append((device_id, best_match))\n","            already_linked_new_devices.add(best_match)\n","        else:\n","            unlinked_device_count += 1\n","\n","    print(f\"Found {len(linked_device_pairs)} potential device links\")\n","    print(f\"Could not find suitable matches for {unlinked_device_count} device IDs\")\n","\n","    device_id_mapping = {}\n","    def find_root_device(device_id):\n","        path = []\n","        current = device_id\n","        while current in device_id_mapping:\n","            path.append(current)\n","            current = device_id_mapping[current]\n","        for node in path:\n","            device_id_mapping[node] = current\n","        return current\n","\n","    for old_id, new_id in linked_device_pairs:\n","        device_id_mapping[new_id] = old_id\n","\n","    all_device_ids = set(linked_df['device_id'].unique())\n","    root_mapping = {device_id: find_root_device(device_id) for device_id in all_device_ids}\n","\n","    linked_df['linked_device_id'] = linked_df['device_id'].map(root_mapping).fillna(linked_df['device_id'])\n","\n","    print(\"Trajectory linking completed\")\n","\n","    # --- New: Update JSON mapping file ---\n","    # For all new-to-original mappings where IDs differ, update shuffle_mapping\n","    for new_id, original_id in device_id_mapping.items():\n","        if new_id != original_id and new_id not in shuffle_mapping:\n","            shuffle_mapping[new_id] = original_id\n","    with open(shuffle_file, 'w') as f:\n","        json.dump(shuffle_mapping, f, indent=4)\n","    print(f\"Updated shuffle mapping written to {shuffle_file}.\")\n","    # -----------------------------------------------------------------------------------\n","\n","    return linked_df, device_id_mapping\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"lGUiWSXfKc0i","executionInfo":{"status":"ok","timestamp":1746082998833,"user_tz":240,"elapsed":2,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["def apply_residency_filter(df, min_lat, max_lat, min_lon, max_lon):\n","\n","    in_box = (\n","        df['latitude_2'].between(min_lat, max_lat) &\n","        df['longitude_2'].between(min_lon, max_lon)\n","    )\n","\n","    total_counts   = df['device_id'].value_counts()\n","    in_box_counts  = df[in_box]['device_id'].value_counts()\n","\n","    stats = (\n","        pd.DataFrame({\n","            'total':   total_counts,\n","            'in_box':  in_box_counts\n","        })\n","        .fillna(0)\n","    )\n","\n","    valid_devices = stats[ stats['in_box'] >= stats['total'] / 2].index\n","\n","    return df[ df['device_id'].isin(valid_devices) ].copy()\n","\n","def apply_residency_filter2(df, min_appearances=5):\n","    device_counts = df['device_id'].value_counts()\n","\n","    valid_devices = device_counts[device_counts >= min_appearances].index\n","    filtered_df = df[df['device_id'].isin(valid_devices)].copy()\n","\n","    return filtered_df"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1834,"status":"ok","timestamp":1746083000697,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"q6aB0lccKiNI","outputId":"96595d33-38e8-4f11-ae9c-225163cc7ec1"},"outputs":[{"output_type":"stream","name":"stdout","text":["575085\n"]}],"source":["filtered_df = apply_residency_filter(filtered_df, min_lat, max_lat, min_lon, max_lon)\n","\n","filtered_df = apply_residency_filter2(filtered_df)\n","print(len(filtered_df))"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6420,"status":"ok","timestamp":1746085390728,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"3vou1pAOF6Kf","outputId":"cd0b2f4e-f2af-4de0-f29d-aef274b75dfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded existing shuffle mapping from device_id_shuffle_map.json.\n","Starting trajectory linking process...\n","Parameters: max_time_gap=3600s, geohash_digits=8\n","Identified last points for 12263 device IDs\n","Identified first points for 12263 device IDs\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12263/12263 [00:00<00:00, 163348.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Found 0 potential device links\n","Could not find suitable matches for 12263 device IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Trajectory linking completed\n","Updated shuffle mapping written to device_id_shuffle_map.json.\n"]}],"source":["linked_df, _ = link_device_trajectories_optimized(\n","    filtered_df,\n","    max_time_gap_seconds=3600,\n","    geohash_digit_tolerance=8\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"wOj6_djK4j7C"},"source":["#Residency Detection and Home Address Detection"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Ftq_vocC74Ps","executionInfo":{"status":"ok","timestamp":1746083020916,"user_tz":240,"elapsed":14374,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["def identify_home_locations(df, min_appearances=7, night_start_hour=12, night_end_hour=6,\n","                            min_home_visits=3, location_precision=3, w1=0.6, w2=0.4):\n","    \"\"\"\n","    Identify home locations for each device using (primarily) night-time visits.\n","    If a device has *no* night-time data, its day-time data are used instead.\n","    \"\"\"\n","    result_df = df.copy()\n","\n","    # --- 1. Preprocessing ----------------------------------------------------------\n","    if not pd.api.types.is_datetime64_any_dtype(result_df['timestamp_2']):\n","        result_df['timestamp_2'] = pd.to_datetime(result_df['timestamp_2'])\n","\n","    # Keep only devices with sufficient appearances\n","    device_counts = result_df['device_id'].value_counts()\n","    valid_devices = device_counts[device_counts >= min_appearances].index\n","    result_df = result_df[result_df['device_id'].isin(valid_devices)]\n","\n","    # Round lat/lon to given precision as spatial cell\n","    result_df['location_cell'] = (\n","        result_df['latitude_2'].round(location_precision).astype(str) + '_' +\n","        result_df['longitude_2'].round(location_precision).astype(str)\n","    )\n","\n","    # Extract hour and date from timestamp\n","    result_df['hour'] = result_df['timestamp_2'].dt.hour\n","    result_df['date'] = result_df['timestamp_2'].dt.date\n","\n","    # --- 2. Keep only night-time visits --------------------------------------------\n","    if night_start_hour > night_end_hour:  # Crosses midnight\n","        night_mask = (result_df['hour'] >= night_start_hour) | (result_df['hour'] < night_end_hour)\n","    else:  # Does not cross midnight\n","        night_mask = (result_df['hour'] >= night_start_hour) & (result_df['hour'] < night_end_hour)\n","\n","    night_visits = result_df[night_mask].copy()\n","\n","    # Find devices without any night-time visits → use their daytime data as fallback\n","    all_devices = set(result_df['device_id'].unique())\n","    night_devices = set(night_visits['device_id'].unique())\n","    devices_no_night = all_devices - night_devices\n","\n","    if devices_no_night:\n","        backup_visits = result_df[result_df['device_id'].isin(devices_no_night)].copy()\n","        night_visits = pd.concat([night_visits, backup_visits], ignore_index=True)\n","\n","    # --- 3. Count visits per location per device -----------------------------------\n","    visit_counts = (\n","        night_visits\n","        .groupby(['device_id', 'location_cell'])\n","        .size()\n","        .reset_index(name='visits')\n","    )\n","\n","    home_candidates = visit_counts[visit_counts['visits'] >= min_home_visits].copy()\n","\n","    # If a device has no location reaching the threshold → choose most visited location\n","    missing_dev = all_devices - set(home_candidates['device_id'].unique())\n","    if missing_dev:\n","        fallback = (\n","            visit_counts[visit_counts['device_id'].isin(missing_dev)]\n","            .sort_values(['device_id', 'visits'], ascending=[True, False])\n","            .groupby('device_id')\n","            .head(1)\n","        )\n","        home_candidates = pd.concat([home_candidates, fallback], ignore_index=True)\n","\n","    # --- 4. Compute temporal consistency -------------------------------------------\n","    distinct_days = (\n","        night_visits\n","        .groupby(['device_id', 'location_cell'])['date']\n","        .nunique()\n","        .reset_index(name='distinct_days')\n","    )\n","    total_days = max((result_df['date'].max() - result_df['date'].min()).days + 1, 1)\n","\n","    home_candidates = home_candidates.merge(distinct_days, on=['device_id', 'location_cell'], how='left')\n","    home_candidates['distinct_days'] = home_candidates['distinct_days'].fillna(1)\n","    home_candidates['temporal_consistency'] = home_candidates['distinct_days'] / total_days\n","\n","    # --- 5. Compute weighted score (penalize daytime-only data) --------------------\n","    penalty_mask = home_candidates['device_id'].isin(devices_no_night)\n","\n","    home_candidates['score'] = np.where(\n","        penalty_mask,\n","        w1 * home_candidates['visits'] * 0.5 + w2 * home_candidates['temporal_consistency'] * 0.5,\n","        w1 * home_candidates['visits']       + w2 * home_candidates['temporal_consistency']\n","    )\n","\n","    # --- 6. Select the best-scored location for each device ------------------------\n","    best_locations = home_candidates.loc[\n","        home_candidates.groupby('device_id')['score'].idxmax()\n","    ][['device_id', 'location_cell', 'score']]\n","\n","    # Retrieve original latitude and longitude of the selected location\n","    loc_samples = (\n","        night_visits\n","        .groupby(['device_id', 'location_cell'])\n","        .agg({'latitude_2': 'first', 'longitude_2': 'first'})\n","        .reset_index()\n","    )\n","\n","    home_locations = best_locations.merge(loc_samples, on=['device_id', 'location_cell'])\n","    home_locations = home_locations.rename(\n","        columns={'latitude_2': 'home_latitude', 'longitude_2': 'home_longitude'}\n","    )[['device_id', 'home_latitude', 'home_longitude']]\n","\n","    # --- 7. Merge home locations back to the original dataframe ---------------------\n","    result_df = result_df.merge(home_locations, on='device_id', how='left')\n","    result_df = result_df.drop(columns=['hour', 'date', 'location_cell'])\n","\n","    return result_df\n","\n","result_df_with_homes = identify_home_locations(linked_df)\n","\n","result_df_with_homes.to_csv(\"Individual_device_list_with_home_address.csv\", index=False)\n","\n","# result_df_with_homes = pd.read_csv(\"Individual_device_list_with_home_address.csv\")\n","# result_df_with_homes = pd.read_csv(home_path)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6140,"status":"ok","timestamp":1746084110706,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"},"user_tz":240},"id":"JksAS--hf-Ft","outputId":"86d19697-5252-4017-f6c5-2f76f6dd36e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["    device_id                                    linked_trip_id  \\\n","0  0000545fff   eec136fa14ec05798086027ed66bc6fa9d9c9412_KDvEGY   \n","1  0000545fff  eec136fa14ec05798086027ed66bc6fa9d9c9412_El4pxKg   \n","2  0000545fff  eec136fa14ec05798086027ed66bc6fa9d9c9412_lORjmm7   \n","3  0000545fff  eec136fa14ec05798086027ed66bc6fa9d9c9412_MQ9OKo5   \n","4  0000545fff  eec136fa14ec05798086027ed66bc6fa9d9c9412_wj4NQz8   \n","\n","   utc_timestamp_1  utc_offset_1  utc_timestamp_2  utc_offset_2  latitude_1  \\\n","0       1578006509        -18000       1578008634        -18000   39.007526   \n","1       1578147468        -18000       1578149219        -18000   39.005367   \n","2       1578157658        -18000       1578158991        -18000   39.189762   \n","3       1578164291        -18000       1578165898        -18000   39.071743   \n","4       1578252243        -18000       1578253147        -18000   39.176006   \n","\n","   longitude_1  geohash_1  latitude_2  ...         timestamp_2  \\\n","0   -77.039597  dqcjyvj7r   39.188156  ... 2020-01-02 18:43:54   \n","1   -77.038536  dqcjyuwq8   39.188030  ... 2020-01-04 09:46:59   \n","2   -77.191345  dqcngqqbz   39.178562  ... 2020-01-04 12:29:51   \n","3   -77.139816  dqcnk8u92   39.178551  ... 2020-01-04 14:24:58   \n","4   -77.294502  dqcnc5gbg   39.181370  ... 2020-01-05 14:39:07   \n","\n","                         original_device_id          datetime_1      date_1  \\\n","0  eec136fa14ec05798086027ed66bc6fa9d9c9412 2020-01-02 18:08:29  2020-01-02   \n","1  eec136fa14ec05798086027ed66bc6fa9d9c9412 2020-01-04 09:17:48  2020-01-04   \n","2  eec136fa14ec05798086027ed66bc6fa9d9c9412 2020-01-04 12:07:38  2020-01-04   \n","3  eec136fa14ec05798086027ed66bc6fa9d9c9412 2020-01-04 13:58:11  2020-01-04   \n","4  eec136fa14ec05798086027ed66bc6fa9d9c9412 2020-01-05 14:24:03  2020-01-05   \n","\n","  linked_device_id home_latitude home_longitude home_geohash_8  \\\n","0       0000545fff     39.188156     -77.189194       dqcngtbx   \n","1       0000545fff     39.188156     -77.189194       dqcngtbx   \n","2       0000545fff     39.188156     -77.189194       dqcngtbx   \n","3       0000545fff     39.188156     -77.189194       dqcngtbx   \n","4       0000545fff     39.188156     -77.189194       dqcngtbx   \n","\n","  home_latitude_8 home_longitude_8  \n","0       39.188147       -77.189083  \n","1       39.188147       -77.189083  \n","2       39.188147       -77.189083  \n","3       39.188147       -77.189083  \n","4       39.188147       -77.189083  \n","\n","[5 rows x 25 columns]\n","Number of home count: 10963\n"]}],"source":["# Assume your DataFrame is named result_df_with_homes\n","# It contains the following columns:\n","# device_id, linked_trip_id, latitude, longitude, timestamp, home_latitude, home_longitude\n","\n","# 1. Encode home_latitude and home_longitude into a 9-character Geohash\n","result_df_with_homes[\"home_geohash_8\"] = result_df_with_homes.apply(\n","    lambda row: geohash.encode(row[\"home_latitude\"], row[\"home_longitude\"], precision=8),\n","    axis=1\n",")\n","\n","# 2. Decode the 9-character Geohash to the center point (lat, lon) of the grid\n","#    decode() returns a tuple (center_lat, center_lon)\n","result_df_with_homes[\"home_center\"] = result_df_with_homes[\"home_geohash_8\"].apply(geohash.decode)\n","\n","# 3. Split the tuple into two columns: home_latitude_8, home_longitude_8\n","result_df_with_homes[\"home_latitude_8\"] = result_df_with_homes[\"home_center\"].apply(lambda x: x[0])\n","result_df_with_homes[\"home_longitude_8\"] = result_df_with_homes[\"home_center\"].apply(lambda x: x[1])\n","\n","# 4. If the intermediate column home_center is no longer needed, delete it\n","result_df_with_homes.drop(columns=[\"home_center\"], inplace=True)\n","\n","# Select the columns to retain\n","cols_to_keep = ['device_id', 'home_latitude', 'home_longitude',\n","                'home_geohash_8', 'home_latitude_8', 'home_longitude_8']\n","\n","# Extract these columns and remove duplicate device_id entries\n","result_df_subset = result_df_with_homes[cols_to_keep].copy().drop_duplicates(subset=['device_id'])\n","\n","# home_latitude_8, home_longitude_8 are center coordinate of the grid cell by geohash, home_latitude, home_longitude are orginal home coordinate\n","# View the processed data\n","print(result_df_with_homes.head())\n","print(\"Number of home count:\", result_df_subset['device_id'].nunique())\n","# result_df_subset.to_csv(\"all_household_list.csv\", index=False)\n","result_df_subset_save=result_df_subset[['device_id','home_geohash_8']]\n","result_df_subset_save.to_csv(\"user_home_relation.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"FKekVCXCrSM2"},"source":["# POI+DTW method to construct individual network"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"OKd6E9RhD2t8","executionInfo":{"status":"ok","timestamp":1746083033774,"user_tz":240,"elapsed":6208,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["\n","# Suppose your new dataset is in a DataFrame called filtered_df,\n","# which has columns for two check-ins per row. We only want the second one.\n","\n","# 1. Rename columns to match Gowalla style\n","#    user_id -> device_id or original_device_id (choose one)\n","#    timestamp -> timestamp_2\n","#    lat -> latitude_2\n","#    lon -> longitude_2\n","#    We'll unify them as: user_id, timestamp, lat, lon, poi (poi = geohash_2)\n","\n","# renamed_df = filtered_df.rename(columns={\n","#     'device_id': 'user_id',        # or 'original_device_id' if preferred\n","#     'timestamp_2': 'timestamp',\n","#     'latitude_2': 'lat',\n","#     'longitude_2': 'lon'\n","# })\n","\n","renamed_df = result_df_with_homes.rename(columns={\n","    'device_id': 'user_id',\n","    'timestamp_2': 'timestamp',\n","    'latitude_2': 'lat',\n","    'longitude_2': 'lon'\n","})\n","\n","# 2. Convert the timestamp to the Gowalla-like format: YYYY-MM-DDTHH:MM:SSZ\n","renamed_df['timestamp'] = pd.to_datetime(renamed_df['timestamp'])\n","renamed_df['timestamp'] = renamed_df['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n","\n","# 3. Compute 8-digit geohash as the new 'poi'\n","renamed_df['poi'] = renamed_df.apply(lambda row: pgh.encode(row['lat'], row['lon'], precision=9), axis=1)\n","\n","# 4. Filter to keep only the relevant columns\n","columns_needed = ['user_id', 'timestamp', 'lat', 'lon', 'poi']\n","checkins = renamed_df[columns_needed].copy()\n"]},{"cell_type":"code","source":["'''\n","Position of Interest (PoI) + Dynamic Time Warping (DTW) Type 1\n","'''\n","\n","# Group by user_id and filter users with fewer than 10 check-ins\n","\n","\n","\n","# -------------------------------------------------\n","# 0. Parameters\n","# -------------------------------------------------\n","min_trj_len = 10          # A user must have at least N check-ins to be considered active\n","topk        = 3           # Take the k smallest DTW values\n","radius      = 2           # Fast dtw allows ±2 steps of alignment shift. Set this value to bigger one will increase runningtime. Original DTW doesn't have this constraint.\n","\n","# -------------------------------------------------\n","# 1. Data preparation (vectorized for speed and memory efficiency)\n","# -------------------------------------------------\n","checkins['timestamp'] = pd.to_datetime(checkins['timestamp'])\n","active_mask = checkins.groupby('user_id')['user_id'].transform('size') >= min_trj_len\n","checkins = checkins.loc[active_mask].copy()\n","\n","# —— 1.1  Map PoI to index & coordinates —— #\n","uniq_poi = checkins.drop_duplicates('poi')[['poi', 'lat', 'lon']]\n","uniq_poi['idx'] = pd.factorize(uniq_poi['poi'])[0]\n","poi2idx   = dict(zip(uniq_poi['poi'], uniq_poi['idx']))\n","idx2coord = uniq_poi[['lat', 'lon']].to_numpy()        # shape (N,2)\n","checkins['poi_idx'] = checkins['poi'].map(poi2idx)     # int32\n","\n","# —— 1.2  Get daily trajectories (as integer sequences) —— #\n","checkins['date'] = checkins['timestamp'].dt.date\n","user_daily_idx_py = (\n","    checkins.groupby(['user_id', 'date'])['poi_idx']\n","            .agg(list)\n","            .groupby(level=0).agg(list)\n","            .to_dict()                                  # {uid: [[idx...], ...]}\n",")\n","\n","# —— 1.3  Convert to numba typed-list (required for JIT) —— #\n","def to_nb_list(days_py):\n","    nb_days = NbList()\n","    for day in days_py:\n","        nb_days.append(np.asarray(day, dtype=np.int32))\n","    return nb_days\n","\n","user_daily_nb = {uid: to_nb_list(days) for uid, days in user_daily_idx_py.items()}\n","\n","# —— 1.4  Candidate pairs (users sharing ≥1 PoI) —— #\n","user_all_pois = (\n","    checkins.groupby('user_id')['poi_idx']\n","            .agg(lambda s: set(s))\n","            .to_dict()\n",")\n","inv_index = (\n","    checkins.groupby('poi_idx')['user_id']\n","            .agg(lambda s: set(s))\n",")\n","candidate_pairs = set()\n","for users_set in inv_index:\n","    if len(users_set) > 1:\n","        candidate_pairs.update(combinations(sorted(users_set), 2))\n","candidate_pairs = list(candidate_pairs)\n","random.shuffle(candidate_pairs)\n","\n","# -------------------------------------------------\n","# 2. Numba-JIT DTW (with Sakoe-Chiba window)\n","# -------------------------------------------------\n","lat_rad = np.radians(idx2coord[:, 0]).astype(np.float64)\n","lon_rad = np.radians(idx2coord[:, 1]).astype(np.float64)\n","R_EARTH = 6371.0\n","\n","@njit(inline='always')\n","def haversine_idx(i, j, lat_r, lon_r):\n","    if i == j:\n","        return 0.0\n","    dlat = lat_r[j] - lat_r[i]\n","    dlon = lon_r[j] - lon_r[i]\n","    a = (math.sin(dlat * 0.5) ** 2 +\n","         math.cos(lat_r[i]) * math.cos(lat_r[j]) *\n","         math.sin(dlon * 0.5) ** 2)\n","    return 2.0 * R_EARTH * math.asin(math.sqrt(a))\n","\n","@njit\n","def dtw_window(seq1, seq2, w, lat_r, lon_r):\n","    n, m = len(seq1), len(seq2)\n","    w    = max(w, abs(n - m))\n","    big  = 1e20\n","    dp   = np.full((n + 1, m + 1), big, dtype=np.float64)\n","    dp[0, 0] = 0.0\n","\n","    for i in range(1, n + 1):\n","        j_lo = max(1, i - w)\n","        j_hi = min(m, i + w) + 1\n","        a_idx = seq1[i - 1]\n","        for j in range(j_lo, j_hi):\n","            b_idx = seq2[j - 1]\n","            cost  = haversine_idx(a_idx, b_idx, lat_r, lon_r)\n","            best  = dp[i - 1, j]\n","            if dp[i, j - 1] < best:\n","                best = dp[i, j - 1]\n","            if dp[i - 1, j - 1] < best:\n","                best = dp[i - 1, j - 1]\n","            dp[i, j] = cost + best\n","\n","    # Backtrace to count path length\n","    i, j, path_len = n, m, 0\n","    while i or j:\n","        path_len += 1\n","        if i == 0:\n","            j -= 1\n","        elif j == 0:\n","            i -= 1\n","        else:\n","            a, b, c = dp[i - 1, j - 1], dp[i - 1, j], dp[i, j - 1]\n","            if a <= b and a <= c:\n","                i, j = i - 1, j - 1\n","            elif b < c:\n","                i -= 1\n","            else:\n","                j -= 1\n","    return dp[n, m] / path_len\n","\n","@njit\n","def aggregate_topk(trajs_u, trajs_v, k, radius, lat_r, lon_r):\n","    tot = len(trajs_u) * len(trajs_v)\n","    dists = np.empty(tot, dtype=np.float32)\n","    p = 0\n","    for i in range(len(trajs_u)):\n","        for j in range(len(trajs_v)):\n","            dists[p] = dtw_window(trajs_u[i], trajs_v[j],\n","                                  radius, lat_r, lon_r)\n","            p += 1\n","    k = k if k < tot else tot\n","    for i in range(k):\n","        min_idx = i\n","        for j in range(i + 1, tot):\n","            if dists[j] < dists[min_idx]:\n","                min_idx = j\n","        dists[i], dists[min_idx] = dists[min_idx], dists[i]\n","    return dists[:k].mean()\n","\n","# -------------------------------------------------\n","# 3. Compute DTW cost for all candidate pairs\n","# -------------------------------------------------\n","print(\"Fast-DTW running …\")\n","candidate_costs = []\n","for (u, v) in tqdm(candidate_pairs):\n","    cost_uv = aggregate_topk(\n","        user_daily_nb[u],\n","        user_daily_nb[v],\n","        topk, radius, lat_rad, lon_rad\n","    )\n","    candidate_costs.append(cost_uv)\n","\n","all_costs = np.array(candidate_costs, dtype=np.float32)\n","\n","\n","# 15% of the data will be positive\n","desired_ratio=0.3\n","# Sort the cost value\n","sorted_costs = np.sort(all_costs)\n","# Calculate place of the desired threshold\n","threshold_index = int(len(sorted_costs) * desired_ratio)\n","print(threshold_index)\n","fixed_threshold = sorted_costs[threshold_index]\n","print(f\"Threshold selected: {fixed_threshold}\")\n","\n","\n","# # Plot histogram of aggregated DTW costs to observe the distribution\n","# plt.figure()\n","# counts, bins, patches = plt.hist(all_costs, bins=100, alpha=0.7, color='skyblue', edgecolor='black')\n","# plt.xlabel('Aggregated DTW Cost')\n","# plt.ylabel('Frequency')\n","# plt.title('Distribution of Aggregated DTW Cost for Candidate Pairs')\n","# plt.show()\n","\n","\n","# 6. Predict social links based on threshold: candidate pairs with DTW cost < threshold are predicted as social links\n","predictions = [1 if cost <= fixed_threshold else 0 for cost in all_costs]\n","predictions = np.array(predictions)\n","print(\"Number of candidate pairs predicted as social links:\", np.sum(predictions))\n","\n","\n","\n","# Generate prediction results for candidate pairs based on the fixed threshold\n","dtw_results = []\n","scores=all_costs\n","for i, (u, v) in enumerate(candidate_pairs):\n","    score = scores[i]\n","    prediction = 1 if score <= fixed_threshold else 0\n","    dtw_results.append({\n","        \"threshold\": fixed_threshold,\n","        \"user_u\": u,\n","        \"user_v\": v,\n","        \"score\": score,\n","        \"prediction\": prediction\n","    })\n","\n","df_dtw_results = pd.DataFrame(dtw_results)\n","\n","# ---------------------------\n","# Part 2: Use Holiday Data to Compute Bonding Links\n","# ---------------------------\n","\n","# Filter Holiday records\n","holiday_df = result_df_with_homes[(result_df_with_homes['timestamp_2'] >= start_time) & (result_df_with_homes['timestamp_2'] < end_time)]\n","print(\"Records during holiday:\", len(holiday_df))\n","\n","# Compute geometric center (average lat/lon) for each user during this time window\n","geom_centers = holiday_df.groupby('device_id').agg({\n","    'latitude_2': 'mean',\n","    'longitude_2': 'mean'\n","}).reset_index()\n","\n","# Compute 7-character Geohash (grid division)\n","geom_centers[\"geohash_7\"] = [\n","    geohash.encode(float(lat), float(lon), precision=7)\n","    for lat, lon in zip(geom_centers[\"latitude_2\"], geom_centers[\"longitude_2\"])\n","]\n","print(\"Number of holiday bonding nodes area:\", len(geom_centers))\n","\n","# Construct user pairs in the same 7-character Geohash grid (bonding candidate pairs)\n","gh_to_devices = geom_centers.groupby('geohash_7')['device_id'].apply(list).to_dict()\n","bonding_candidate_pairs = []\n","for gh, devices in gh_to_devices.items():\n","    if len(devices) > 1:\n","        pairs = list(combinations(sorted(devices), 2))\n","        bonding_candidate_pairs.extend(pairs)\n","print(\"Number of holiday bonding user pairs (edges):\", len(bonding_candidate_pairs))\n","\n","bonding_set = set(bonding_candidate_pairs)\n","\n","# ---------------------------\n","# Part 3: Combine DTW predictions and bonding links to determine connection type\n","# ---------------------------\n","# If a candidate pair is predicted as connected (prediction == 1) under the fixed DTW threshold\n","# and the two users are in the same bonding grid, classify them as a bonding connection\n","\n","def determine_connection_type(row):\n","    pair = tuple(sorted([row[\"user_u\"], row[\"user_v\"]]))\n","    if pair in bonding_set:\n","        return 1\n","    else:\n","        return 0\n","\n","df_dtw_results[\"Holiday bonding\"] = df_dtw_results.apply(determine_connection_type, axis=1)\n","\n","# Save final results to CSV\n","df_dtw_results.to_csv(\"Individual_social_network.csv\", index=False)\n","print(\"DTW Prediction details saved to Individual_social_network.csv\")\n","\n","# ============ 5. For different thresholds, compute prediction results and save positive/negative samples ============\n","# Generate a series of thresholds covering the score range\n","min_score, max_score = scores.min(), scores.max()\n","thresholds = np.linspace(min_score, max_score, num=20)\n","\n","# print(\"Threshold\\tPredicted Positive\\tPredicted Negative\")\n","all_rows = []  # Used to store candidate pair info at each threshold\n","\n","for thresh in thresholds:\n","    # Predict as positive if score >= thresh; otherwise, negative\n","    pred_positive = np.sum(scores >= thresh)\n","    pred_negative = len(scores) - pred_positive\n","    # print(f\"{thresh:.2f}\\t\\t{pred_positive}\\t\\t\\t{pred_negative}\")\n","\n","    # Save prediction result for each candidate pair under the current threshold\n","    for i, (u, v) in enumerate(candidate_pairs):\n","        score = scores[i]\n","        prediction = \"positive\" if score >= thresh else \"negative\"\n","        all_rows.append({\n","            \"threshold\": thresh,\n","            \"user_u\": u,\n","            \"user_v\": v,\n","            \"score\": score,\n","            \"prediction\": prediction\n","        })\n","\n","# Save results to a CSV file\n","df = pd.DataFrame(all_rows)\n","df.to_csv(\"Individual_social_network_multiple_threshold.csv\", index=False)\n","print(\"Prediction details saved to Individual_social_network_multiple_threshold.csv\")\n","\n","# # # (Optional) Plot the relationship between threshold and number of predictions\n","# positives = [np.sum(scores <= t) for t in thresholds]\n","# negatives = [len(scores) - np.sum(scores <= t) for t in thresholds]\n","\n","# plt.figure()\n","# plt.plot(thresholds, positives, label='Predicted Positive')\n","# plt.plot(thresholds, negatives, label='Predicted Negative')\n","# plt.xlabel('Threshold (Score)')\n","# plt.ylabel('Number of Candidate Pairs')\n","# plt.title('Prediction Counts at Different Thresholds')\n","# plt.legend()\n","# plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AmmsxAvL82U","executionInfo":{"status":"ok","timestamp":1746084104467,"user_tz":240,"elapsed":195773,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"0e584e11-fedd-4b0a-cd9a-6bb487d90634"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Fast-DTW running …\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 331595/331595 [02:26<00:00, 2259.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["49739\n","Threshold selected: 0.49549904465675354\n","Number of candidate pairs predicted as social links: 49740\n","Records in 2020-01-01 18:00-22:00: 2308\n","Number of holiday bonding nodes area: 1505\n","Number of holiday bonding user pairs (edges): 756\n","DTW Prediction details saved to Individual_links.csv\n","Prediction details saved to Individual_links_DTW_all_threshold.csv\n"]}]},{"cell_type":"markdown","source":["# Individual backbone extraction"],"metadata":{"id":"ewRfsPY1SzrU"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"a2UAxrd7uxF1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746084348452,"user_tz":240,"elapsed":29,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"ecdb9f5b-abfa-4e9b-ede0-2870fd75d444"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-37-8b8ee50a03d2>:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  graph_df['score'] = graph_df['score']\n","<ipython-input-37-8b8ee50a03d2>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  graph_df['similarity'] = max_cost - graph_df['score']\n"]}],"source":["graph_df = df_dtw_results[df_dtw_results['prediction'] == 1]\n","\n","\n","# Convert negative DTW cost to cost and compute similarity\n","graph_df['score'] = graph_df['score']\n","max_cost = graph_df['score'].max()\n","graph_df['similarity'] = max_cost - graph_df['score']\n","# graph_df.to_csv('graph_df.csv', index=False)"]},{"cell_type":"code","source":["# EADM Backbone Extraction Method\n","\n","import pandas as pd\n","import itertools\n","from tqdm import tqdm   # pip install tqdm\n","\n","time_graph = graph_df[['user_u', 'user_v']]\n","time_alldf = filtered_df[['device_id', 'latitude_2', 'longitude_2', 'geohash_2', 'timestamp_2']]\n","\n","# ---------- Preprocessing ----------\n","time_alldf[\"timestamp_2\"] = pd.to_datetime(time_alldf[\"timestamp_2\"])\n","time_alldf[\"hour_bin\"] = time_alldf[\"timestamp_2\"].dt.floor(\"H\")\n","\n","ids_in_graph = set(time_graph[\"user_u\"]) | set(time_graph[\"user_v\"])\n","loc_sub = time_alldf[time_alldf[\"device_id\"].isin(ids_in_graph)]\n","\n","# For the same (geohash, hour, device), keep only the earliest time → avoid multiple rows per person\n","loc_sub = (\n","    loc_sub\n","    .groupby([\"geohash_2\", \"hour_bin\", \"device_id\"], as_index=False)\n","    [\"timestamp_2\"].min()\n",")\n","\n","edge_set = {tuple(sorted(x)) for x in time_graph[[\"user_u\", \"user_v\"]].to_numpy()}\n","\n","# ---------- Scanning ----------\n","records = []\n","grouped = loc_sub.groupby([\"geohash_2\", \"hour_bin\"], sort=False)\n","\n","for (gh, hr), grp in tqdm(grouped, total=len(grouped), desc=\"Scanning groups\"):\n","    ids = grp[\"device_id\"].tolist()\n","    ts = dict(zip(ids, grp[\"timestamp_2\"]))        # Used to access timestamp\n","    # Pairwise combinations within the group, much fewer than total number of edges\n","    for u, v in itertools.combinations(ids, 2):\n","        pair = tuple(sorted((u, v)))\n","        if pair in edge_set:\n","            records.append({\n","                \"user_u\": pair[0],\n","                \"user_v\": pair[1],\n","                \"time_encounter\": max(ts[u], ts[v])\n","            })\n","\n","encounter_df = pd.DataFrame(records)\n","\n","from collections import defaultdict\n","from scipy.stats import poisson\n","from statsmodels.stats.multitest import multipletests\n","\n","# ------------------------------------------------------------\n","# Main function: extract_backbone_eadm\n","# ------------------------------------------------------------\n","def extract_backbone_eadm(df,\n","                           time_col='time_encounter',\n","                           src_col='user_u',\n","                           dst_col='user_v',\n","                           resample='1min',       # Granularity for discretizing the original timeline\n","                           alpha=0.01,            # Global significance threshold\n","                           partition='bb',        # 'bb' | 'equal'\n","                           n_intervals=10):       # Number of intervals when partition='equal'\n","    \"\"\"\n","    Implements EADM from Nadini et al. (2020) and returns the backbone network.\n","    \"\"\"\n","    # ----- 0. Preprocessing -----\n","    df = df[[src_col, dst_col, time_col]].copy()\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df = df.sort_values(time_col)\n","\n","    # Only count one edge for the same node pair at the same time\n","    df['time_bin'] = df[time_col].dt.floor(resample)\n","    df = df.drop_duplicates([src_col, dst_col, 'time_bin'])\n","\n","    # ----- 1. Determine interval boundaries Δ -----\n","    if partition == 'bb':\n","        try:\n","            from astropy.stats import bayesian_blocks\n","            # Map each edge event to a timestamp in seconds\n","            event_ts = (df['time_bin'] - df['time_bin'].min()).dt.total_seconds().values\n","            edges = bayesian_blocks(event_ts, fitness='events')\n","            edges = edges + df['time_bin'].min().timestamp()      # Convert to absolute seconds\n","            edge_ts = pd.to_datetime(edges, unit='s')\n","        except ImportError:\n","            print('Astropy not installed. Falling back to equal partitioning.')\n","            partition = 'equal'\n","\n","    if partition == 'equal':\n","        t0, t1 = df['time_bin'].min(), df['time_bin'].max()\n","        edge_ts = pd.date_range(t0, t1, periods=n_intervals+1)\n","\n","    # Assign interval labels to each record\n","    df['interval'] = pd.cut(df['time_bin'], bins=edge_ts,\n","                            right=False, labels=False, include_lowest=True)\n","\n","    # ----- 2. Count edge weights within each interval -----\n","    weights = (\n","        df.groupby(['interval', src_col, dst_col])\n","          .size()\n","          .reset_index(name='w')\n","    )\n","\n","    # ----- 3. Compute s_i(Δ) and W(Δ) -----\n","    s = defaultdict(lambda: defaultdict(int))   # s[Δ][node]\n","    W = defaultdict(int)                       # W[Δ]\n","    for _, r in weights.iterrows():\n","        Δ = int(r['interval']); u = r[src_col]; v = r[dst_col]; w = r['w']\n","        s[Δ][u] += w\n","        s[Δ][v] += w\n","        W[Δ]    += w\n","\n","    # Length of each Δ (in minutes) — unused in Poisson but kept for consistency\n","    τ = {Δ: (edge_ts[Δ+1] - edge_ts[Δ]).total_seconds()/60\n","         for Δ in range(len(edge_ts)-1)}\n","\n","    # ----- 4. Expected edge weight λ_ij = Σ_Δ a_i(Δ) a_j(Δ) -----\n","    λ = defaultdict(float)\n","    for Δ in s:\n","        if W[Δ] < 1:                # Skip empty intervals\n","            continue\n","        denom = 2 * W[Δ] - 1\n","        # Precompute all a_i values in the interval\n","        a = {i: s[Δ][i] / np.sqrt(denom) for i in s[Δ]}\n","        nodes = list(a.keys())\n","        for i_idx in range(len(nodes)):\n","            for j_idx in range(i_idx+1, len(nodes)):\n","                i = nodes[i_idx]; j = nodes[j_idx]\n","                λ[(i, j)] += a[i] * a[j]          # Accumulate total expectation\n","\n","    # ----- 5. Observed edge weight w_ij -----\n","    w_obs = (weights\n","             .groupby([src_col, dst_col])['w']\n","             .sum()\n","             .to_dict())\n","    # Ensure key order (i < j)\n","    w_obs = {tuple(sorted(k)): v for k, v in w_obs.items()}\n","\n","    # ----- 6. Compute p-value (Poisson) -----\n","    pairs, w_vals, lam_vals = [], [], []\n","    for pair, w in w_obs.items():\n","        lam = λ.get(pair, 0.0)\n","        pairs.append(pair); w_vals.append(w); lam_vals.append(lam)\n","\n","    p_raw = [1 - poisson.cdf(w-1, lam) if lam > 0 else 0.0\n","             for w, lam in zip(w_vals, lam_vals)]\n","\n","    # ----- 7. Multiple testing correction (Bonferroni) -----\n","    reject, p_adj, _, _ = multipletests(p_raw,\n","                                        alpha=alpha,\n","                                        method='bonferroni')\n","\n","    # ----- 8. Output backbone network -----\n","    backbone = [(u, v, w, p)\n","                for keep, (u, v), w, p in zip(reject, pairs, w_vals, p_adj)\n","                if keep]\n","\n","    backbone_df = pd.DataFrame(backbone,\n","                               columns=[src_col, dst_col,\n","                                        'weight', 'p_adj'])\n","    return backbone_df\n","\n","eadm_bb_df = extract_backbone_eadm(encounter_df,\n","                                  resample='1min',\n","                                  partition='equal',\n","                                  n_intervals=4)   # Use 'equal' for small datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GicFBHd6j2CZ","executionInfo":{"status":"ok","timestamp":1746083279843,"user_tz":240,"elapsed":68037,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"b6560c0a-0e93-42d2-e6e7-b472af20bbc3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-1611148cdd53>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  time_alldf[\"timestamp_2\"] = pd.to_datetime(time_alldf[\"timestamp_2\"])\n","<ipython-input-16-1611148cdd53>:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  time_alldf[\"hour_bin\"] = time_alldf[\"timestamp_2\"].dt.floor(\"H\")\n","<ipython-input-16-1611148cdd53>:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  time_alldf[\"hour_bin\"] = time_alldf[\"timestamp_2\"].dt.floor(\"H\")\n","Scanning groups: 100%|██████████| 491291/491291 [00:37<00:00, 13016.85it/s]\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"hOelwCO5CFJC","executionInfo":{"status":"ok","timestamp":1746083301701,"user_tz":240,"elapsed":8970,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}}},"outputs":[],"source":["# Gloss Backbone Extraction Method\n","\n","alpha = 0.05  # Significance level\n","\n","# 2. Compute global p-value for each edge: p = P(W >= w_ij) = (#edges with weight >= w_ij) / M\n","df = graph_df.copy()\n","M = len(df)\n","df['p_value'] = df['similarity'].apply(lambda w: (df['similarity'] >= w).sum() / M)\n","df.loc[1, 'p_value'] = 0  # set first p-value to avoid there is not p-value smaller than 0, which will lead to bug\n","# 3. Filter backbone edges: keep edges with p_value < alpha\n","gloss_backbone_df = df[df['p_value'] < alpha].copy()"]},{"cell_type":"code","source":["# Construct bonding links based on extracted backbone network\n","\n","# Create a set for fast lookup to check if an edge exists in the EADM backbone\n","eadm_edges = set(tuple(sorted(edge)) for edge in eadm_bb_df[['user_u', 'user_v']].values)\n","\n","# For each edge in graph_df, check whether it appears in eadm_edges\n","graph_df['connection type'] = graph_df.apply(\n","    lambda row: int(tuple(sorted((row['user_u'], row['user_v']))) in eadm_edges),\n","    axis=1\n",")\n","graph_df.to_csv('final_individual_social_network.csv',index=False)"],"metadata":{"id":"mk55UCM-7rxO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746084359039,"user_tz":240,"elapsed":757,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"3dd23845-a735-427f-f495-ea07cca6f686"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-38-53e1ee5df7e6>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  graph_df['connection type'] = graph_df.apply(\n"]}]},{"cell_type":"markdown","source":["# Household Social Network Construction"],"metadata":{"id":"MlSwWMrX7WyO"}},{"cell_type":"code","source":["import pandas as pd\n","from itertools import combinations\n","\n","# --------------------------\n","# 1. Read the CSV file and generate the individual_links list\n","# --------------------------\n","# Keep only user pairs predicted as connected, and extract user_u, user_v, and connection type columns\n","individual_links = graph_df[['user_u', 'user_v', 'connection type']].values.tolist()\n","print(\"Total individual links (predicted connected):\", len(individual_links))\n","print(individual_links)\n","# --------------------------\n","# 2. Build a mapping from device to home (home geohash)\n","# Assume result_df_subset contains columns device_id and home_geohash_8\n","# --------------------------\n","device_to_home = result_df_subset.set_index('device_id')['home_geohash_8'].to_dict()\n","\n","# --------------------------\n","# 3. Count the number of devices in each home\n","# --------------------------\n","home_device_count = result_df_subset['home_geohash_8'].value_counts().to_dict()\n","print(\"Home device count:\", home_device_count)\n","\n","# --------------------------\n","# 4. Traverse individual_links to generate inter-home connection counts\n","#    Only consider connections between different homes.\n","#    For each home pair, if there is at least one bonding link (conn_type==1), mark type as 1.\n","# --------------------------\n","home_link_counts = {}\n","for d1, d2, conn_type in individual_links:\n","    if d1 in device_to_home and d2 in device_to_home:\n","        h1 = device_to_home[d1]\n","        h2 = device_to_home[d2]\n","        if h1 != h2:\n","            # Ensure consistent order\n","            home_pair = tuple(sorted([h1, h2]))\n","            if home_pair not in home_link_counts:\n","                home_link_counts[home_pair] = {'count': 0, 'type': 0}\n","            home_link_counts[home_pair]['count'] += 1\n","            # If at least one bonding link exists, set type to 1\n","            if conn_type == 1:\n","                home_link_counts[home_pair]['type'] = 1\n","\n","# --------------------------\n","# 5. Generate list of home candidate pairs\n","# --------------------------\n","home_candidate_pair = list(home_link_counts.keys())\n","print(\"Number of home candidate pairs:\", len(home_candidate_pair))\n","\n","# --------------------------\n","# 6. Construct home_edges DataFrame and compute avg_link (average connection strength)\n","#    avg_link = link_count / (number of devices in home 1 + number of devices in home 2)\n","# --------------------------\n","home_edges_data = []\n","for (h1, h2), info in home_link_counts.items():\n","    link_count = info['count']\n","    home_type = info['type']\n","    # Compute the number of possible device pairs (sum of device counts from both homes)\n","    num_possible = home_device_count.get(h1, 0) + home_device_count.get(h2, 0)\n","    avg_link = link_count / num_possible if num_possible > 0 else 0\n","    home_edges_data.append({\n","        \"home_1\": h1,\n","        \"home_2\": h2,\n","        \"link_count\": link_count,\n","        \"num_possible\": num_possible,\n","        \"avg_link\": avg_link,\n","        \"type\": home_type\n","    })\n","\n","home_edges = pd.DataFrame(home_edges_data)\n","\n","# --------------------------\n","# Output results\n","# --------------------------\n","print('How many devices are there in each home:')\n","print(home_device_count)\n","print(\"Home candidate pairs:\")\n","print(home_candidate_pair)\n","print(\"Home edges:\")\n","print(home_edges)\n","\n","home_edges.to_csv(\"Household_social_network.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XpZh8tK6-2IrCqRiDhKxj6HoKGq5_r_E"},"id":"AdsQSUSN7bII","executionInfo":{"status":"ok","timestamp":1746083817889,"user_tz":240,"elapsed":1182,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"0e38d5a5-898a-4909-b18a-ee7fbddb003d"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Household Decision State Generation"],"metadata":{"id":"UxE_pq-c-ieP"}},{"cell_type":"code","source":["# 1. Preprocessing: parse time, filter daytime (9–17)\n","linked_df['timestamp_2'] = pd.to_datetime(linked_df['timestamp_2'])\n","linked_df['hour'] = linked_df['timestamp_2'].dt.hour\n","day_df = linked_df[(linked_df['hour'] >= 9) & (linked_df['hour'] < 17)].copy()\n","\n","# 2. Prepare: home geohash → set of resident devices\n","home_residents = (\n","    result_df_subset\n","    .groupby('home_geohash_8')['device_id']\n","    .apply(set)\n","    .to_dict()\n",")\n","\n","# Add 7-character geohash to home data\n","result_df_subset['home_geohash_7'] = result_df_subset['home_geohash_8'].str[:7]\n","\n","# Extract 7-character geohash from each trajectory point\n","day_df['geohash_7'] = day_df['geohash_2'].str[:7]\n","\n","# Merge using 7-character geohash to filter visits near homes\n","visits = (\n","    day_df\n","      .merge(\n","         result_df_subset[['home_geohash_7','home_geohash_8']],\n","         left_on='geohash_7', right_on='home_geohash_7',\n","         how='inner'\n","      )\n","      [['home_geohash_8','device_id','timestamp_2']]\n",")\n","\n","\n","# ——————————————————————————————————————————————————————————————————————————\n","# 4. Detect (1) Non-resident temporary visits\n","#    — If a home has a visiting device not in home_residents on the same day\n","# ——————————————————————————————————————————————————————————————————————————\n","def detect_non_resident(\n","    visits_df: pd.DataFrame,\n","    home_res_map: dict,\n","    min_duration_hours: float = 3\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Keep only non-resident devices that stayed in a home_geohash_8 area\n","    for at least min_duration_hours within the same day.\n","\n","    visits_df must contain ['home_geohash_8','device_id','timestamp_2'],\n","    home_res_map is {home_geohash_8: set(device_id)}.\n","    \"\"\"\n","    # Ensure timestamp column is datetime and extract date\n","    df = visits_df.copy()\n","    df['timestamp_2'] = pd.to_datetime(df['timestamp_2'])\n","    df['date'] = df['timestamp_2'].dt.date\n","\n","    records = []\n","    # Group by home\n","    for home8, grp in tqdm(df.groupby('home_geohash_8')):\n","        # Filter out non-resident devices\n","        non_res = grp[~grp['device_id'].isin(home_res_map.get(home8, set()))]\n","        if non_res.empty:\n","            continue\n","\n","        # For each device_id + date, compute duration between first and last visit\n","        dur = (\n","            non_res\n","            .groupby(['device_id','date'])['timestamp_2']\n","            .agg(start='min', end='max')\n","            .reset_index()\n","        )\n","        dur['duration_h'] = (\n","            dur['end'] - dur['start']\n","        ).dt.total_seconds() / 3600.0\n","\n","        # Keep devices with at least one visit ≥ min_duration_hours\n","        long_stays = dur[dur['duration_h'] >= min_duration_hours]\n","        devices = set(long_stays['device_id'])\n","        if devices:\n","            records.append({\n","                'home_geohash_8':    home8,\n","                'non_resident_count': len(devices),\n","                'non_residents':     devices\n","            })\n","\n","    return pd.DataFrame(records)\n","\n","def detect_night_absence(\n","    linked_df: pd.DataFrame,\n","    result_df_subset: pd.DataFrame,\n","    night_start: int = 21,\n","    night_end: int = 6,\n","    threshold_days: int = 2,\n","    hash_precision: int = 7\n","):\n","    \"\"\"\n","    Detect continuous nighttime absence.\n","\n","    Returns\n","    -------\n","    device_absence_df : DataFrame\n","        Columns: [home_geohash_8, device_id, absent_start_date, absent_end_date]\n","        Represents periods when a device was absent for threshold_days consecutive nights.\n","\n","    home_absence_df : DataFrame\n","        Columns: [home_geohash_8, absent_start_date, absent_end_date]\n","        Represents periods when all devices in a home were absent during the same window.\n","    \"\"\"\n","    # 1. Copy and parse timestamps, extract hour\n","    df = linked_df.copy()\n","    df['timestamp_2'] = pd.to_datetime(df['timestamp_2'])\n","    df['hour'] = df['timestamp_2'].dt.hour\n","\n","    # 2. Filter nighttime records and annotate date\n","    night = df[(df['hour'] >= night_start) | (df['hour'] < night_end)].copy()\n","    night['date'] = night['timestamp_2'].dt.date\n","\n","    # 3. Truncate geohash\n","    df_prefix = f'geo_pref'\n","    home_prefix = f'home_pref'\n","    night[df_prefix] = night['geohash_2'].str[:hash_precision]\n","    homes = result_df_subset[['home_geohash_8','device_id']].copy()\n","    homes[home_prefix] = homes['home_geohash_8'].str[:hash_precision]\n","\n","    # 4. Identify nighttime check-ins at home\n","    at_home = night.merge(\n","        homes,\n","        left_on=[df_prefix,'device_id'],\n","        right_on=[home_prefix,'device_id'],\n","        how='inner'\n","    )[['home_geohash_8','device_id','date']]\n","\n","    # 5. Construct complete list of nighttime dates\n","    all_dates = pd.date_range(\n","        night['date'].min(),\n","        night['date'].max(),\n","        freq='D'\n","    ).date\n","\n","    # 6. Aggregate (home,device) → set of check-in dates\n","    visited_map = (\n","        at_home\n","        .groupby(['home_geohash_8','device_id'])['date']\n","        .agg(set)\n","        .to_dict()\n","    )\n","\n","    # 7. Device-level: sliding window to find first continuous absence\n","    device_records = []\n","    home_residents = homes.groupby('home_geohash_8')['device_id'].apply(set).to_dict()\n","    for home8, devices in home_residents.items():\n","        for dev in devices:\n","            seen = visited_map.get((home8, dev), set())\n","            for i in range(len(all_dates) - threshold_days + 1):\n","                window = all_dates[i:i+threshold_days]\n","                if all(day not in seen for day in window):\n","                    device_records.append({\n","                        'home_geohash_8':    home8,\n","                        'device_id':         dev,\n","                        'absent_start_date': window[0],\n","                        'absent_end_date':   window[-1]\n","                    })\n","                    break\n","    device_absence_df = pd.DataFrame(device_records)\n","\n","    # 8. Home-level: sliding window to find periods when all members are absent\n","    home_records = []\n","    for home8, devices in home_residents.items():\n","        for i in range(len(all_dates) - threshold_days + 1):\n","            window = all_dates[i:i+threshold_days]\n","            # Check that all devices in the home were absent in the window\n","            if all(\n","                all(day not in visited_map.get((home8, dev), set()) for day in window)\n","                for dev in devices\n","            ):\n","                home_records.append({\n","                    'home_geohash_8':    home8,\n","                    'absent_start_date': window[0],\n","                    'absent_end_date':   window[-1]\n","                })\n","                break\n","    home_absence_df = pd.DataFrame(home_records)\n","\n","    return device_absence_df, home_absence_df\n","\n","# — Example Call —\n","non_resident_visits = detect_non_resident(visits, home_residents)\n","device_df, home_df = detect_night_absence(linked_df, result_df_subset, night_start = 23, night_end = 6, threshold_days = 15, hash_precision=8)\n","print('total number of home', len(result_df_subset))\n","\n","repairs_df = pd.merge(\n","    non_resident_visits[['home_geohash_8', 'non_resident_count', 'non_residents']],\n","    home_df[['home_geohash_8', 'absent_start_date', 'absent_end_date']],\n","    on='home_geohash_8',\n","    how='inner'\n",")\n","\n","print(\"Homes possibly under repair:\")\n","print(repairs_df)\n","repairs_df.to_csv('house_reparing.csv',index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IZEK5_D-2oc","executionInfo":{"status":"ok","timestamp":1746083694797,"user_tz":240,"elapsed":25787,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"afc6d67c-96f8-4c3e-9394-2f968d54ffc6"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7438/7438 [00:21<00:00, 349.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total number of home 10963\n","Homes possibly under repair:\n","     home_geohash_8  non_resident_count             non_residents  \\\n","0          dqchfbt4                   1              {95ae32f21a}   \n","1          dqcj74sq                   1              {6df0e269cd}   \n","2          dqcj9vcu                   1              {839d7117d0}   \n","3          dqcje4pd                   1              {d654898715}   \n","4          dqcjgnc3                   2  {f0378e32da, 195be66481}   \n","...             ...                 ...                       ...   \n","5301       dqcpe05v                   1              {041a5b86fa}   \n","5302       dqcqu7zp                   1              {aee3176ac4}   \n","5303       dqcqy030                   1              {0774514ab1}   \n","5304       dr101fe0                   1              {8fda22dfc2}   \n","5305       dr101fe1                   1              {7677526223}   \n","\n","     absent_start_date absent_end_date  \n","0           2020-01-01      2020-01-15  \n","1           2020-01-01      2020-01-15  \n","2           2020-01-01      2020-01-15  \n","3           2020-01-01      2020-01-15  \n","4           2020-01-01      2020-01-15  \n","...                ...             ...  \n","5301        2020-01-01      2020-01-15  \n","5302        2020-01-01      2020-01-15  \n","5303        2020-01-01      2020-01-15  \n","5304        2020-01-01      2020-01-15  \n","5305        2020-01-01      2020-01-15  \n","\n","[5306 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["from typing import Dict, Set, List, Tuple\n","\n","# --- tiny, vectorised Haversine helper ------------------------------------------\n","EARTH_R = 6_371_000.0  # mean Earth radius in metres\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    \"\"\"Vectorised Haversine distance (metres).\"\"\"\n","    lat1, lon1, lat2, lon2 = map(np.radians, (lat1, lon1, lat2, lon2))\n","    dlat  = lat2 - lat1\n","    dlon  = lon2 - lon1\n","    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n","    return 2 * EARTH_R * np.arcsin(np.sqrt(a))\n","\n","# -------------------------------------------------------------------------------\n","\n","\n","def detect_rental_homes_medprec(\n","    linked_df: pd.DataFrame,\n","    result_df_subset: pd.DataFrame,\n","    owner_absence_days: int = 7,\n","    min_strangers: int = 2,\n","    night_start: int = 0,\n","    night_end: int = 6,\n","    dist_threshold_m: float = 60.0,    # spatial radius\n","    min_stay_hrs: float = 1.0          # stranger must stay ≥ 4 h (cumulative per night)\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Medium-precision rental detector.\n","\n","    Stranger condition\n","    ------------------\n","    A device counts as a stranger **only if its cumulative dwell time within\n","    `dist_threshold_m` of the home, *inside the night window*, is ≥ `min_stay_hrs`\n","    on any one of the checked nights.\n","    \"\"\"\n","\n","    # -------- 1.  Keep only night-time rows --------------------------------------\n","    df = linked_df.copy()\n","    df[\"timestamp_2\"] = pd.to_datetime(df[\"timestamp_2\"])\n","    df[\"hour\"]   = df[\"timestamp_2\"].dt.hour\n","    df[\"date\"]   = df[\"timestamp_2\"].dt.date\n","\n","    night_mask   = (df[\"hour\"] >= night_start) | (df[\"hour\"] < night_end)\n","    cols_keep    = [\n","        \"device_id\", \"timestamp_2\", \"geohash_2\",\n","        \"date\", \"latitude_2\", \"longitude_2\"\n","    ]\n","    night_df = df.loc[night_mask, cols_keep].copy()\n","    night_df[\"gh5\"] = night_df[\"geohash_2\"].str[:5]   # for owner\n","    night_df[\"gh6\"] = night_df[\"geohash_2\"].str[:6]   # coarse stranger filter\n","\n","    # -------- 2.  Home / owner tables -------------------------------------------\n","    homes = result_df_subset[\n","        [\"home_geohash_8\", \"device_id\", \"home_latitude\", \"home_longitude\"]\n","    ].copy()\n","    homes[\"home5\"] = homes[\"home_geohash_8\"].str[:5]\n","    homes[\"home6\"] = homes[\"home_geohash_8\"].str[:6]\n","\n","    home_to_owners: Dict[str, Set[str]] = (\n","        homes.groupby(\"home_geohash_8\")[\"device_id\"].apply(set).to_dict()\n","    )\n","\n","    # -------- 3.  Recent nights --------------------------------------------------\n","    night_dates: List[pd.Timestamp] = sorted(night_df[\"date\"].unique())\n","    if len(night_dates) < owner_absence_days:\n","        raise ValueError(\"Not enough days of data\")\n","\n","    recent_nights = set(night_dates[-owner_absence_days:])\n","    night_recent  = night_df.loc[night_df[\"date\"].isin(recent_nights)].copy()\n","\n","    # -------- 4.  Owner absence --------------------------------------------------\n","    owners_at_night = (\n","        night_recent.merge(\n","            homes[[\"home_geohash_8\", \"device_id\", \"home5\"]],\n","            left_on=[\"device_id\", \"gh5\"],\n","            right_on=[\"device_id\", \"home5\"],\n","            how=\"inner\",\n","        )[[\"home_geohash_8\", \"date\"]]\n","        .drop_duplicates()\n","    )\n","\n","    seen_dates: Dict[str, Set[pd.Timestamp]] = (\n","        owners_at_night.groupby(\"home_geohash_8\")[\"date\"].apply(set).to_dict()\n","    )\n","\n","    absent_homes = {\n","        h: sorted(recent_nights - seen_dates.get(h, set()))\n","        for h in home_to_owners\n","        if len(recent_nights - seen_dates.get(h, set())) == owner_absence_days\n","    }\n","\n","    # -------- 5.  Stranger presence ≥ x hour ---------------------------------------\n","    gh6_groups: Dict[str, pd.DataFrame] = dict(tuple(night_recent.groupby(\"gh6\")))\n","\n","    rental_candidates = []\n","    four_hours = timedelta(hours=min_stay_hrs)\n","\n","    for home8, absent_dates in tqdm(absent_homes.items()):\n","        owners = home_to_owners[home8]\n","        row    = homes.loc[homes[\"home_geohash_8\"] == home8].iloc[0]\n","        hlat, hlon = row[\"home_latitude\"], row[\"home_longitude\"]\n","        h6         = row[\"home6\"]\n","\n","        # 5-A coarse gh6 filter\n","        if h6 not in gh6_groups:\n","            continue\n","        df_candidates = gh6_groups[h6]\n","\n","        # 5-B precise radius filter\n","        dists = haversine(\n","            hlat, hlon,\n","            df_candidates[\"latitude_2\"].values,\n","            df_candidates[\"longitude_2\"].values,\n","        )\n","        close_df = df_candidates.loc[dists <= dist_threshold_m]\n","\n","        # 5-C dwell-time per stranger, night by night\n","        #     group → (device, date)  so we can accumulate **within one night**\n","        grouped = close_df.groupby([\"device_id\", \"date\"])\n","\n","        long_stayers: Set[str] = set()\n","        for (dev, day), sub in grouped:\n","            if dev in owners:\n","                continue\n","            # sort by time to avoid negative spans if timestamps unordered\n","            span = sub[\"timestamp_2\"].sort_values()\n","            dwell = span.max() - span.min()\n","            if dwell >= four_hours:\n","                long_stayers.add(dev)\n","\n","        if len(long_stayers) >= min_strangers:\n","            rental_candidates.append(\n","                {\n","                    \"home_geohash_8\":    home8,\n","                    \"owner_absent_dates\": absent_dates,\n","                    \"stranger_count\":     len(long_stayers),\n","                    \"stranger_devices\":   long_stayers,\n","                }\n","            )\n","\n","    return pd.DataFrame(rental_candidates)\n","# ---------------- Example ---------------------------------------------------------\n","sold_df = detect_rental_homes_medprec(\n","    linked_df,\n","    result_df_subset,\n","    owner_absence_days=7,\n","    min_strangers=1,\n","    night_start=21,\n","    night_end=8\n",")\n","print('House possibily sold:')\n","print(sold_df)\n","\n","sold_df.to_csv('Sold_house.csv',index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeMfgwUf_sIN","executionInfo":{"status":"ok","timestamp":1746083779937,"user_tz":240,"elapsed":13472,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"c0236784-5942-4a66-c1e8-5eaa3237351f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4043/4043 [00:12<00:00, 318.98it/s]"]},{"output_type":"stream","name":"stdout","text":["House possibily sold:\n","    home_geohash_8                                 owner_absent_dates  \\\n","0         dqcn7qmn  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","1         dqcn7qmp  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","2         dqcn8zhp  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","3         dqcn9ks8  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","4         dqcn9w6q  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","..             ...                                                ...   \n","381       dqcp4jn2  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","382       dqcp4jrw  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","383       dqcp5j4w  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","384       dqcp725w  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","385       dqcrn66v  [2020-01-27, 2020-01-28, 2020-01-29, 2020-01-3...   \n","\n","     stranger_count                      stranger_devices  \n","0                 1                          {82d5c827d6}  \n","1                 1                          {82d5c827d6}  \n","2                 2              {039be6f30e, 4e3603a384}  \n","3                 1                          {f42da578aa}  \n","4                 2              {acbdb061af, 8166130681}  \n","..              ...                                   ...  \n","381               1                          {59e40acd19}  \n","382               3  {ecc3aac214, 5255e1237d, 63b475f2e9}  \n","383               1                          {7462e53926}  \n","384               1                          {bfed82c140}  \n","385               1                          {d81903ea05}  \n","\n","[386 rows x 4 columns]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def detect_moved(\n","    linked_df: pd.DataFrame,\n","    result_df_subset: pd.DataFrame,\n","    min_lat: float,\n","    max_lat: float,\n","    min_lon: float,\n","    max_lon: float,\n","    days_threshold: int = 30\n","):\n","    \"\"\"\n","    Detect moved devices/households:\n","      - Device level: If a device_id hasn't entered the specified region for more than days_threshold days,\n","        it's considered as moved.\n","      - Household level: If all devices in a household are considered moved, then the household is considered fully moved.\n","\n","    Parameters\n","    ----------\n","    linked_df : DataFrame\n","        Raw trajectory data, must contain ['device_id','latitude_2','longitude_2','timestamp_2']\n","    result_df_subset : DataFrame\n","        Home-device mapping, must contain ['home_geohash_8','device_id']\n","    min_lat, max_lat, min_lon, max_lon : float\n","        Geographic boundaries of the target region\n","    days_threshold : int\n","        Threshold in days for absence from the region (default is 30)\n","\n","    Returns\n","    -------\n","    moved_devices_df : DataFrame\n","        columns = [\n","          'device_id',\n","          'last_in_region_date',   # Date of last visit inside region\n","          'days_since_last'        # Days since that date to the latest date in dataset\n","        ]\n","    moved_households_df : DataFrame\n","        columns = [\n","          'home_geohash_8',\n","          'total_devices',\n","          'moved_device_count'\n","        ]\n","        Only includes households where all devices are considered moved\n","    \"\"\"\n","    # 1. Parse timestamp and extract date\n","    df = linked_df.copy()\n","    df['timestamp_2'] = pd.to_datetime(df['timestamp_2'])\n","    df['date'] = df['timestamp_2'].dt.date\n","\n","    # 2. Get the latest date in the dataset\n","    overall_max = df['date'].max()\n","\n","    # 3. Filter records within the specified region\n","    mask = (\n","        (df['latitude_2']  >= min_lat ) & (df['latitude_2']  <= max_lat) &\n","        (df['longitude_2'] >= min_lon ) & (df['longitude_2'] <= max_lon)\n","    )\n","    in_region = df[mask]\n","\n","    # 4. Device-level: compute last date in region & days since then\n","    records_dev = []\n","    for dev, grp in in_region.groupby('device_id'):\n","        last_date = grp['date'].max()\n","        days_since = (overall_max - last_date).days\n","        if days_since > days_threshold:\n","            records_dev.append({\n","                'device_id':           dev,\n","                'last_in_region_date': last_date,\n","                'days_since_last':     days_since\n","            })\n","    moved_devices_df = pd.DataFrame(records_dev)\n","\n","    # 5. Household-level: count how many homes have all devices moved\n","    home_devs = (\n","        result_df_subset\n","        .groupby('home_geohash_8')['device_id']\n","        .apply(list)\n","        .to_dict()\n","    )\n","\n","    moved_set = set(moved_devices_df['device_id'])\n","    records_home = []\n","    for home, devices in home_devs.items():\n","        total = len(devices)\n","        moved_cnt = sum(1 for d in devices if d in moved_set)\n","        if moved_cnt == total and total > 0:\n","            records_home.append({\n","                'home_geohash_8':      home,\n","                'total_devices':       total,\n","                'moved_device_count':  moved_cnt\n","            })\n","    moved_households_df = pd.DataFrame(records_home)\n","\n","    return moved_devices_df, moved_households_df\n","\n","\n","moved_devs, moved_homes = detect_moved(\n","    linked_df,\n","    result_df_subset,\n","    min_lat, max_lat, min_lon, max_lon,\n","    days_threshold=20\n",")\n","\n","\n","print(\"Households where all members moved:\")\n","print(moved_homes)\n","moved_homes.to_csv('Migrated_home.csv',index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cUsIAPp_4De","executionInfo":{"status":"ok","timestamp":1746083445240,"user_tz":240,"elapsed":1797,"user":{"displayName":"Ruxiao Chen","userId":"00965981112874551125"}},"outputId":"159519ef-ed36-43fa-ae51-c09ce4adcda3"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Households where all members moved:\n","    home_geohash_8  total_devices  moved_device_count\n","0         dqc5buen              1                   1\n","1         dqcj6024              3                   3\n","2         dqcj6kw9              1                   1\n","3         dqcj6qn8              1                   1\n","4         dqcjgqh5              1                   1\n","..             ...            ...                 ...\n","212       dqcp5qq5              1                   1\n","213       dqcp60r4              1                   1\n","214       dqcp759v              1                   1\n","215       dqcqy030              1                   1\n","216       dqcqy2gy              1                   1\n","\n","[217 rows x 3 columns]\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["MlSwWMrX7WyO","UxE_pq-c-ieP"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
