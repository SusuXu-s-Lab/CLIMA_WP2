# -*- coding: utf-8 -*-
"""CLIMA_WP2_Network_Dynamics_AR(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127A3HOj1JnBGIIMz042lyWZhjVUH54wR

## Libraries and Packages
"""

!pip install python-geohash
!pip install pygeohash
#!pip install pyro-ppl
!pip install numpyro

import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from google.colab  import drive
from datetime import datetime, timedelta
import joblib
import math, bisect, random
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
import pdb
from tqdm import tqdm
from itertools import combinations
import geohash
import networkx as nx
import scipy.stats as stats
import scipy.signal as signal
from scipy.stats import beta, norm
import matplotlib.pyplot as plt
import torch
import pickle
import json
import jax
import jax.numpy as jnp
from jax.experimental import sparse
from jax import random, lax
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, Predictive
from scipy import sparse
import os
import time
import warnings
import seaborn as sns
import os
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.cm as cm

"""##Synthesized Data Generation"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import os
from matplotlib.colors import LinearSegmentedColormap
from scipy.stats import norm
from sklearn.cluster import KMeans # Import KMeans here

# --- Data Preparation Functions ---

def assign_households_to_groups(households_df, num_groups=5, random_seed=None):
    """
    Assign households to Census Block Groups (CBGs) based on geographical proximity using K-means.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with lat/lon coordinates.
    num_groups : int, default=5
        Number of groups to create.
    random_seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added 'group_id' column.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.
    """
    print(f"Assigning households to {num_groups} groups...")

    # Set random seed if provided for KMeans reproducibility
    if random_seed is not None:
        kmeans_random_state = random_seed
    else:
        kmeans_random_state = None # Allow KMeans to use its default random state

    # Extract coordinates
    coords = households_df[['lat', 'lon']].values

    # Use K-means clustering
    # Note: n_init='auto' is recommended in newer scikit-learn versions
    kmeans = KMeans(n_clusters=num_groups, random_state=kmeans_random_state, n_init='auto')
    group_assignments = kmeans.fit_predict(coords)

    # Get cluster centroids
    centroids = kmeans.cluster_centers_

    # Add group_id to dataframe
    households_df['group_id'] = group_assignments

    # Create a dictionary mapping group_id to centroid
    group_centroids = {i: (centroids[i, 0], centroids[i, 1]) for i in range(num_groups)}

    # Print summary
    print(f"Group assignments complete. Households per group:")
    group_counts = households_df['group_id'].value_counts().sort_index()
    for group_id, count in group_counts.items():
        print(f"  Group {group_id}: {count} households")

    return households_df, group_centroids

def generate_group_attributes(households_df, group_centroids):
    """
    Generate group-level attributes by aggregating household characteristics.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with group-level attributes (means, counts, feature vectors).
    """
    print("Generating group-level attributes...")

    # Get unique group IDs
    group_ids = sorted(households_df['group_id'].unique()) # Sort for consistent order

    # Initialize lists for group attributes
    group_data = []

    # Generate group attributes by aggregating household attributes
    for g_id in group_ids:
        # Get households in this group
        group_households = households_df[households_df['group_id'] == g_id]

        # Calculate group-level attributes (aggregations)
        avg_income = group_households['income'].mean()
        avg_resource_access = group_households['resource_access'].mean()
        avg_damage_structural = group_households['damage_structural'].mean()
        avg_damage_utility = group_households['damage_utility'].mean()

        # Get centroid
        centroid_lat, centroid_lon = group_centroids[g_id]

        # Create group feature vectors (needed for the hierarchical threshold model)
        H_g = np.array([avg_income, avg_resource_access])  # Socioeconomic features
        D_g = np.array([avg_damage_structural, avg_damage_utility])  # Damage features

        # Store group data
        group_data.append({
            'group_id': g_id,
            'centroid_lat': centroid_lat,
            'centroid_lon': centroid_lon,
            'avg_income': avg_income,
            'avg_resource_access': avg_resource_access,
            'avg_damage_structural': avg_damage_structural,
            'avg_damage_utility': avg_damage_utility,
            'household_count': len(group_households),
            'H_g': H_g, # Store the numpy array directly
            'D_g': D_g  # Store the numpy array directly
        })

    # Create DataFrame
    group_df = pd.DataFrame(group_data).set_index('group_id') # Use group_id as index

    return group_df

def generate_household_attributes(household_ids, household_locs, seed=None):
    """
    Generate binary household attributes for simulation using specified probabilities.

    Parameters:
    -----------
    household_ids : list
        List of household IDs.
    household_locs : dict
        Dictionary mapping household ID to (lat, lon) location.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with household attributes (ID, lat, lon, income, etc.).
    """
    print("Generating binary household attributes...")

    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)

    n_households = len(household_ids)

    # Define probabilities for each binary attribute
    p_income = 0            # 20% chance of high income (1)
    p_resource_access = 0.0  # 30% chance of good resource access (1)
    p_damage_structural = 0.# 15% chance of structural damage (1)
    p_damage_utility = 1.0   # 50% chance of utility damage (1)

    # Generate binary attributes directly using numpy's random choice
    incomes = np.random.choice([0, 1], size=n_households, p=[1 - p_income, p_income])
    resource_access = np.random.choice([0, 1], size=n_households, p=[1 - p_resource_access, p_resource_access])
    damage_structural = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_structural, p_damage_structural])
    damage_utility = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_utility, p_damage_utility])

    # Extract location data efficiently
    lats = np.array([household_locs[h_id][0] for h_id in household_ids])
    lons = np.array([household_locs[h_id][1] for h_id in household_ids])

    # Create DataFrame
    households_df = pd.DataFrame({
        'household_id': household_ids,
        'lat': lats,
        'lon': lons,
        'income': incomes,                  # 1=high, 0=low
        'resource_access': resource_access, # 1=good, 0=poor
        'damage_structural': damage_structural, # 1=damaged, 0=not damaged
        'damage_utility': damage_utility,   # 1=damaged, 0=not damaged
    })

    return households_df

def initialize_decision_states(households_df, damage_threshold, seed=None):
    """
    Initialize household decision states Y = [Y^O, Y^S, Y^R] and history/transition tracking.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data.
    damage_threshold : float
        Threshold for determining initial recovery state based on average damage.
    seed : int, optional
        Random seed for reproducibility of initial vacant/for-sale selection.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added decision state columns (Y_O, Y_S, Y_R) and tracking columns.
    """
    print("Initializing decision states...")
    n_households = len(households_df)

    # Set random seed if provided for initial state randomization
    if seed is not None:
        np.random.seed(seed)

    # Initialize all states to 0
    occupancy_state = np.zeros(n_households, dtype=np.int8) # Y_O: 0=occupied, 1=vacant
    sales_state = np.zeros(n_households, dtype=np.int8)     # Y_S: 0=not for sale, 1=for sale

    # Randomly select a small number of initial vacant/for-sale properties
    num_vacant = np.random.randint(10, 25) # e.g., 1 or 2, or up to 2%
    num_for_sale = np.random.randint(15, 20)# e.g., 1, or up to 1%

    if n_households > 0: # Avoid errors with empty dataframes
        vacant_indices = np.random.choice(n_households, num_vacant, replace=False)
        # Ensure for_sale indices don't overlap with vacant ones initially if desired (optional)
        available_for_sale_indices = np.setdiff1d(np.arange(n_households), vacant_indices)
        if len(available_for_sale_indices) >= num_for_sale:
             for_sale_indices = np.random.choice(available_for_sale_indices, num_for_sale, replace=False)
        else: # Fallback if too few non-vacant houses left
             for_sale_indices = np.random.choice(n_households, num_for_sale, replace=False)


        # Set selected indices to 1
        occupancy_state[vacant_indices] = 1
        sales_state[for_sale_indices] = 1

    # Initialize recovery state based on damage levels
    # Y_R: 1=repaired/undamaged, 0=needs repair
    # If average damage is *below* threshold, it's considered 'repaired' or okay initially.
    avg_damage = (households_df['damage_structural'] + households_df['damage_utility']) / 2
    recovery_state = (avg_damage < damage_threshold).astype(np.int8)

    # Add to the dataframe
    households_df['Y_O'] = occupancy_state
    households_df['Y_S'] = sales_state
    households_df['Y_R'] = recovery_state

    # Initialize historical state tracking for bonding influence (max state seen so far)
    # Starts with the current initial state.
    households_df['Y_O_hist'] = occupancy_state.copy()
    households_df['Y_S_hist'] = sales_state.copy()
    households_df['Y_R_hist'] = recovery_state.copy()

    # Initialize previous states for tracking recent transitions (state at t-1)
    # Starts with the current initial state.
    households_df['Y_O_prev'] = occupancy_state.copy()
    households_df['Y_S_prev'] = sales_state.copy()
    households_df['Y_R_prev'] = recovery_state.copy()

    # Initialize transition markers (0 = no transition in the last step, 1 = recent 0->1 transition)
    # Starts at 0 as there's no previous step.
    households_df['Y_O_transition'] = np.zeros(n_households, dtype=np.int8)
    households_df['Y_S_transition'] = np.zeros(n_households, dtype=np.int8)
    households_df['Y_R_transition'] = np.zeros(n_households, dtype=np.int8)

    # Print summary of initial states
    print(f"Initial states summary:")
    print(f"  Vacant households (Y_O=1): {np.sum(occupancy_state == 1)} / {n_households}")
    print(f"  For-sale properties (Y_S=1): {np.sum(sales_state == 1)} / {n_households}")
    print(f"  Repaired/undamaged (Y_R=1): {np.sum(recovery_state == 1)} / {n_households}")

    return households_df

# --- Network and Influence Functions ---

def create_network_matrices(households_df, network_df):
    """
    Create weighted adjacency matrices for bonding (W_B) and bridging (W_R) networks.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data (must include 'household_id').
    network_df : DataFrame
        DataFrame containing network connections ('home_1', 'home_2', 'type', 'avg_link').
        'type' should be 1 for bonding, 2 for bridging.

    Returns:
    --------
    W_B : ndarray
        Weighted adjacency matrix for bonding network (n_households x n_households).
    W_R : ndarray
        Weighted adjacency matrix for bridging network (n_households x n_households).
    id_to_idx : dict
        Dictionary mapping household ID to matrix index.
    idx_to_id : dict
        Dictionary mapping matrix index to household ID.
    """
    print("Creating network adjacency matrices...")
    # Get household IDs and create mappings
    household_ids = households_df['household_id'].values
    id_to_idx = {h_id: idx for idx, h_id in enumerate(household_ids)}
    idx_to_id = {idx: h_id for h_id, idx in id_to_idx.items()}

    n_households = len(household_ids)

    # Initialize adjacency matrices with zeros
    W_B = np.zeros((n_households, n_households))  # Bonding network
    W_R = np.zeros((n_households, n_households))  # Bridging network

    # Populate matrices from network_df
    skipped_connections = 0
    for _, row in network_df.iterrows():
        home_1 = row['home_1']
        home_2 = row['home_2']
        weight = row['avg_link']
        conn_type = row['type']

        # Get indices, skip if household not in our simulation set
        if home_1 in id_to_idx and home_2 in id_to_idx:
            i = id_to_idx[home_1]
            j = id_to_idx[home_2]

            # Add edge weight based on type (undirected)
            if conn_type == 1:  # Bonding
                W_B[i, j] = weight
                W_B[j, i] = weight
            elif conn_type == 2: # Bridging
                W_R[i, j] = weight
                W_R[j, i] = weight
            # else: ignore other types if any
        else:
            skipped_connections += 1

    if skipped_connections > 0:
         print(f"  Warning: Skipped {skipped_connections} connections involving households not in the main dataset.")
    print("Network matrices created.")
    return W_B, W_R, id_to_idx, idx_to_id

def calculate_raw_influences(households_df, W_B, W_R):
    """
    Calculate raw social influence m^s for each decision dimension s.
    - Bonding networks use historical states (Y_hist: max state seen so far).
    - Bridging networks use only recent transitions (Y_transition: 0->1 in last step).

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with current states and history.
    W_B : ndarray
        Weighted adjacency matrix for bonding network.
    W_R : ndarray
        Weighted adjacency matrix for bridging network.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added raw influence columns ('m_O', 'm_S', 'm_R').
    """
    n_households = len(households_df)

    # Get historical decision states (for bonding influence)
    Y_O_hist = households_df['Y_O_hist'].values
    Y_S_hist = households_df['Y_S_hist'].values
    Y_R_hist = households_df['Y_R_hist'].values

    # Get transition markers (for bridging influence - only recent 0->1 transitions)
    Y_O_transition = households_df['Y_O_transition'].values
    Y_S_transition = households_df['Y_S_transition'].values
    Y_R_transition = households_df['Y_R_transition'].values

    # Initialize raw influence vectors
    m_O = np.zeros(n_households)
    m_S = np.zeros(n_households)
    m_R = np.zeros(n_households)

    # Calculate combined raw influences using matrix operations for efficiency
    # Bonding influence = W_B * Y_hist (element-wise)
    bonding_influence_O = W_B @ Y_O_hist
    bonding_influence_S = W_B @ Y_S_hist
    bonding_influence_R = W_B @ Y_R_hist

    # Bridging influence = W_R * Y_transition (element-wise)
    bridging_influence_O = W_R @ Y_O_transition
    bridging_influence_S = W_R @ Y_S_transition
    bridging_influence_R = W_R @ Y_R_transition

    # Total weights for normalization (sum of weights for each household)
    total_weights_B = W_B.sum(axis=1)
    total_weights_R = W_R.sum(axis=1)
    total_weights = total_weights_B + total_weights_R

    # Combine influences and normalize
    # Avoid division by zero for isolated nodes (total_weight = 0)
    valid_weights_mask = total_weights > 0
    m_O[valid_weights_mask] = (bonding_influence_O[valid_weights_mask] + bridging_influence_O[valid_weights_mask]) / total_weights[valid_weights_mask]
    m_S[valid_weights_mask] = (bonding_influence_S[valid_weights_mask] + bridging_influence_S[valid_weights_mask]) / total_weights[valid_weights_mask]
    m_R[valid_weights_mask] = (bonding_influence_R[valid_weights_mask] + bridging_influence_R[valid_weights_mask]) / total_weights[valid_weights_mask]

    # Store raw influences in dataframe
    households_df['m_O'] = m_O
    households_df['m_S'] = m_S
    households_df['m_R'] = m_R

    return households_df


def calculate_adjusted_influences(households_df, omega_matrix):
    """
    Calculate correlation-adjusted influences I^s = m * Omega.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with raw influences ('m_O', 'm_S', 'm_R').
    omega_matrix : ndarray
        3x3 correlation matrix for cross-dimension dependencies.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added adjusted influence columns ('I_O', 'I_S', 'I_R').
    """
    # Get raw influences
    m_O = households_df['m_O'].values
    m_S = households_df['m_S'].values
    m_R = households_df['m_R'].values

    # Stack raw influences into a matrix: shape (n_households, 3)
    m = np.vstack([m_O, m_S, m_R]).T

    # Calculate adjusted influences I = m @ Omega
    I = m @ omega_matrix  # shape: (n_households, 3)

    # Unpack results and store in dataframe
    households_df['I_O'] = I[:, 0]
    households_df['I_S'] = I[:, 1]
    households_df['I_R'] = I[:, 2]

    return households_df

# --- Threshold Functions ---

def initialize_group_lambda(group_df, threshold_params):
    """
    Initialize group-level logit thresholds lambda (λ_g,0^s) for time t=0.
    These are drawn from the stationary distribution of the AR(1) process around the long-term mean.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group-level attributes (H_g, D_g). Index must be 'group_id'.
    threshold_params : dict
        Dictionary of threshold model parameters (beta coefficients, phi, sigma_epsilon, etc).

    Returns:
    --------
    group_df : DataFrame
        DataFrame with added lambda columns for t=0 (lambda_O_0, lambda_S_0, lambda_R_0)
        and long-term mean columns (mu_bar_O, mu_bar_S, mu_bar_R).
    """
    print("Initializing group-level lambda values at t=0...")

    # Extract beta coefficients from params
    beta_H_O = threshold_params['beta_H_O']
    beta_D_O = threshold_params['beta_D_O']
    beta_H_S = threshold_params['beta_H_S']
    beta_D_S = threshold_params['beta_D_S']
    beta_H_R = threshold_params['beta_H_R']
    beta_D_R = threshold_params['beta_D_R']

    # Extract AR(1) parameters for stationary distribution
    phi_O = threshold_params['phi_O']
    phi_S = threshold_params['phi_S']
    phi_R = threshold_params['phi_R']

    sigma_epsilon_O = threshold_params['sigma_epsilon_O']
    sigma_epsilon_S = threshold_params['sigma_epsilon_S']
    sigma_epsilon_R = threshold_params['sigma_epsilon_R']

    # Calculate stationary variance for each state
    # Formula: sigma_epsilon^2 / (1 - phi^2)
    stationary_var_O = sigma_epsilon_O**2 / (1 - phi_O**2)
    stationary_var_S = sigma_epsilon_S**2 / (1 - phi_S**2)
    stationary_var_R = sigma_epsilon_R**2 / (1 - phi_R**2)

    # Calculate standard deviations for initialization
    stationary_std_O = np.sqrt(stationary_var_O)
    stationary_std_S = np.sqrt(stationary_var_S)
    stationary_std_R = np.sqrt(stationary_var_R)

    # Calculate long-term means (μ̄_g^s) for each group using dot products
    group_df['mu_bar_O'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_O) + np.dot(row['D_g'], beta_D_O), axis=1)
    group_df['mu_bar_S'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_S) + np.dot(row['D_g'], beta_D_S), axis=1)
    group_df['mu_bar_R'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_R) + np.dot(row['D_g'], beta_D_R), axis=1)

    # Initialize lambda values at t=0 by drawing from normal distribution with
    # stationary variance of the AR(1) process
    group_df['lambda_O_0'] = np.random.normal(group_df['mu_bar_O'], stationary_std_O)
    group_df['lambda_S_0'] = np.random.normal(group_df['mu_bar_S'], stationary_std_S)
    group_df['lambda_R_0'] = np.random.normal(group_df['mu_bar_R'], stationary_std_R)

    print("  Group lambda values initialized.")
    return group_df

def update_group_lambda(group_df, t, threshold_params):
    """
    Update group-level logit thresholds lambda (λ_g,t^s) using AR(1) process:

    λ_g,t^s = (1-φ_s)μ̄_g^s + φ_s λ_g,t-1^s + ε_g,t^s

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group lambda values at time t-1.
    t : int
        Current time step.
    threshold_params : dict
        Dictionary of threshold model parameters including AR(1) parameters.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with updated lambda columns for time t.
    """
    # Extract AR(1) parameters
    phi_O = threshold_params['phi_O']  # Persistence parameter for occupancy state
    phi_S = threshold_params['phi_S']  # Persistence parameter for sales state
    phi_R = threshold_params['phi_R']  # Persistence parameter for recovery state

    sigma_epsilon_O = threshold_params['sigma_epsilon_O']  # Innovation standard deviation
    sigma_epsilon_S = threshold_params['sigma_epsilon_S']
    sigma_epsilon_R = threshold_params['sigma_epsilon_R']

    # Generate column names for current and previous time steps
    prev_lambda_O = f'lambda_O_{t-1}'
    prev_lambda_S = f'lambda_S_{t-1}'
    prev_lambda_R = f'lambda_R_{t-1}'

    curr_lambda_O = f'lambda_O_{t}'
    curr_lambda_S = f'lambda_S_{t}'
    curr_lambda_R = f'lambda_R_{t}'

    # Generate innovation terms ε_g,t^s ~ N(0, σ_ε,s²)
    epsilon_O = np.random.normal(0, sigma_epsilon_O, size=len(group_df))
    epsilon_S = np.random.normal(0, sigma_epsilon_S, size=len(group_df))
    epsilon_R = np.random.normal(0, sigma_epsilon_R, size=len(group_df))

    # Apply AR(1) update equation
    group_df[curr_lambda_O] = (1 - phi_O) * group_df['mu_bar_O'] + phi_O * group_df[prev_lambda_O] + epsilon_O
    group_df[curr_lambda_S] = (1 - phi_S) * group_df['mu_bar_S'] + phi_S * group_df[prev_lambda_S] + epsilon_S
    group_df[curr_lambda_R] = (1 - phi_R) * group_df['mu_bar_R'] + phi_R * group_df[prev_lambda_R] + epsilon_R

    return group_df

def assign_individual_thresholds_from_lambda(households_df, group_df, t, threshold_params):
    """
    Assign thresholds to individual households based on group-level lambda values at time t.

    λ_h,t^s ~ N(λ_g(h),t^s, σ_ind,s²)
    τ_h,t^s = σ(λ_h,t^s) = 1/(1 + exp(-λ_h,t^s))

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_df : DataFrame
        DataFrame containing group lambda values at time t, indexed by 'group_id'.
    t : int
        Current time step.
    threshold_params : dict
        Dictionary of threshold model parameters.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated lambda and threshold columns for time t.
    """
    # Extract individual heterogeneity standard deviations
    sigma_ind_O = threshold_params['sigma_ind_O']
    sigma_ind_S = threshold_params['sigma_ind_S']
    sigma_ind_R = threshold_params['sigma_ind_R']

    # Generate column names for current time step
    lambda_O_col = f'lambda_O_{t}'
    lambda_S_col = f'lambda_S_{t}'
    lambda_R_col = f'lambda_R_{t}'

    # Create temporary columns for group lambda values by merging
    temp_df = households_df.merge(
        group_df[[lambda_O_col, lambda_S_col, lambda_R_col]],
        left_on='group_id',
        right_index=True,
        how='left',
        suffixes=('', '_group')
    )

    # Generate standard normal deviates (eta) for each household
    num_households = len(households_df)
    eta_O = np.random.normal(0, 1, size=num_households)
    eta_S = np.random.normal(0, 1, size=num_households)
    eta_R = np.random.normal(0, 1, size=num_households)

    # Assign eta to the main dataframe for tracking/debugging
    households_df['eta_O'] = eta_O
    households_df['eta_S'] = eta_S
    households_df['eta_R'] = eta_R

    # Calculate household-specific lambdas (λ_h,t^s) using non-centered parameterization
    # λ_h,t^s = λ_g(h),t^s + σ_ind,s * η_h
    households_df['lambda_O'] = temp_df[lambda_O_col] + sigma_ind_O * eta_O
    households_df['lambda_S'] = temp_df[lambda_S_col] + sigma_ind_S * eta_S
    households_df['lambda_R'] = temp_df[lambda_R_col] + sigma_ind_R * eta_R

    # Transform lambdas to thresholds (tau) using the sigmoid function
    # τ_h,t^s = σ(λ_h,t^s) = 1/(1 + exp(-λ_h,t^s))
    households_df['tau_O'] = 1 / (1 + np.exp(-households_df['lambda_O']))
    households_df['tau_S'] = 1 / (1 + np.exp(-households_df['lambda_S']))
    households_df['tau_R'] = 1 / (1 + np.exp(-households_df['lambda_R']))

    # Store the group-level lambda values for analysis/visualization
    households_df['group_lambda_O'] = temp_df[lambda_O_col]
    households_df['group_lambda_S'] = temp_df[lambda_S_col]
    households_df['group_lambda_R'] = temp_df[lambda_R_col]

    return households_df

def visualize_lambda_evolution(group_df, time_steps, output_dir='.'):
    """
    Visualize the evolution of group-level lambda values over time.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group lambda values for all time steps.
    time_steps : list
        List of time steps to visualize.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing group lambda evolution over time...")
    if not time_steps:
        print("  No time steps to visualize.")
        return

    # Create figure with subplots for each decision state
    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)

    # Get group IDs to color-code the lines
    group_ids = sorted(group_df.index.unique())
    num_groups = len(group_ids)
    color_map = plt.cm.get_cmap('tab10', num_groups)

    # Decision states to plot
    states = ['O', 'S', 'R']
    titles = ['Occupancy', 'Sales', 'Recovery']

    # For each decision state
    for i, (state, title) in enumerate(zip(states, titles)):
        ax = axes[i]

        # For each group, plot lambda evolution
        for g_idx, g_id in enumerate(group_ids):
            # Extract lambda values for this group and state across all time steps
            lambda_values = []
            for t in time_steps:
                lambda_col = f'lambda_{state}_{t}'
                if lambda_col in group_df.columns:
                    lambda_values.append(group_df.loc[g_id, lambda_col])
                else:
                    lambda_values.append(np.nan)  # Handle missing time steps

            # Plot evolution line for this group
            color = color_map(g_idx)
            ax.plot(time_steps, lambda_values, '-o', label=f'Group {g_id}', color=color, linewidth=2, markersize=4)

            # Add horizontal line for long-term mean if available
            mu_bar_col = f'mu_bar_{state}'
            if mu_bar_col in group_df.columns:
                mu_bar = group_df.loc[g_id, mu_bar_col]
                ax.axhline(y=mu_bar, color=color, linestyle='--', alpha=0.5)

        # Calculate sigmoid curves for reference (to show τ = σ(λ) mapping)
        lambda_range = np.linspace(-5, 5, 100)
        tau_range = 1 / (1 + np.exp(-lambda_range))

        # Create a twin axis for the sigmoid reference
        ax_twin = ax.twinx()
        ax_twin.plot(lambda_range, tau_range, 'k-', alpha=0.2, linewidth=1)
        ax_twin.set_ylabel('Threshold τ (sigmoid scale)', fontsize=10)
        ax_twin.set_ylim(0, 1)
        ax_twin.tick_params(axis='y', labelsize=8)

        # Formatting for main axis
        ax.set_ylabel(f'λ for {title} State', fontsize=12)
        ax.set_title(f'Evolution of Group-Level λ for {title} State (λ_{state})', fontsize=14)
        ax.grid(True, linestyle=':', alpha=0.6)

        # Add legend only to the first subplot to avoid repetition
        if i == 0:
            ax.legend(fontsize=10, loc='upper right')

    # Set common x-axis label
    axes[-1].set_xlabel('Time Step', fontsize=14)

    # Ensure x-ticks cover all time steps
    tick_step = max(1, len(time_steps) // 15)  # Show ~15 ticks max
    axes[-1].set_xticks(time_steps[::tick_step])

    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(hspace=0.3)

    # Save figure
    filename = os.path.join(output_dir, 'lambda_evolution.png')
    plt.savefig(filename, dpi=300)
    plt.close(fig)  # Close the figure to free memory
    print(f"  Lambda evolution plot saved to {filename}")

def visualize_tau_evolution(group_df, time_steps, output_dir='.'):
    """
    Visualize the evolution of implied group-level tau values (sigmoid transformed lambda) over time.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group lambda values for all time steps.
    time_steps : list
        List of time steps to visualize.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing group-level threshold (tau) evolution over time...")
    if not time_steps:
        print("  No time steps to visualize.")
        return

    # Create figure with subplots for each decision state
    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)

    # Get group IDs to color-code the lines
    group_ids = sorted(group_df.index.unique())
    num_groups = len(group_ids)
    color_map = plt.cm.get_cmap('tab10', num_groups)

    # Decision states to plot
    states = ['O', 'S', 'R']
    titles = ['Occupancy', 'Sales', 'Recovery']

    # For each decision state
    for i, (state, title) in enumerate(zip(states, titles)):
        ax = axes[i]

        # For each group, convert lambda to tau and plot evolution
        for g_idx, g_id in enumerate(group_ids):
            # Extract lambda values and convert to tau values
            tau_values = []
            for t in time_steps:
                lambda_col = f'lambda_{state}_{t}'
                if lambda_col in group_df.columns:
                    lambda_val = group_df.loc[g_id, lambda_col]
                    tau_val = 1 / (1 + np.exp(-lambda_val))  # Sigmoid transformation
                    tau_values.append(tau_val)
                else:
                    tau_values.append(np.nan)  # Handle missing time steps

            # Plot tau evolution line for this group
            color = color_map(g_idx)
            ax.plot(time_steps, tau_values, '-o', label=f'Group {g_id}', color=color, linewidth=2, markersize=4)

            # Add horizontal line for sigmoid of long-term mean if available
            mu_bar_col = f'mu_bar_{state}'
            if mu_bar_col in group_df.columns:
                mu_bar = group_df.loc[g_id, mu_bar_col]
                tau_mu_bar = 1 / (1 + np.exp(-mu_bar))  # Sigmoid of long-term mean
                ax.axhline(y=tau_mu_bar, color=color, linestyle='--', alpha=0.5)

        # Formatting
        ax.set_ylabel(f'τ for {title} State', fontsize=12)
        ax.set_title(f'Evolution of Group-Level Threshold τ for {title} State (τ_{state})', fontsize=14)
        ax.grid(True, linestyle=':', alpha=0.6)
        ax.set_ylim(0, 1)  # Tau values are between 0 and 1

        # Add legend only to the first subplot to avoid repetition
        if i == 0:
            ax.legend(fontsize=10, loc='upper right')

    # Set common x-axis label
    axes[-1].set_xlabel('Time Step', fontsize=14)

    # Ensure x-ticks cover all time steps
    tick_step = max(1, len(time_steps) // 15)  # Show ~15 ticks max
    axes[-1].set_xticks(time_steps[::tick_step])

    # Add interpretation text to explain the meaning of tau values
    interpretation_text = (
        "τ near 0: Low threshold (easily transitions)\n"
        "τ near 1: High threshold (resistant to transition)"
    )
    axes[0].text(0.02, 0.02, interpretation_text, transform=axes[0].transAxes,
                fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(hspace=0.3)

    # Save figure
    filename = os.path.join(output_dir, 'tau_evolution.png')
    plt.savefig(filename, dpi=300)
    plt.close(fig)  # Close the figure to free memory
    print(f"  Tau evolution plot saved to {filename}")


# --- State Update Function ---

def update_states_with_group_thresholds(households_df, smoothing_factor):
    """
    Update household decision states (Y_O, Y_S, Y_R) based on individual thresholds (tau)
    and adjusted influences (I) using a logistic smoothed threshold model.
    Tracks state transitions explicitly for bridging influence calculation in the *next* step.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with current states (Y_*), influences (I_*),
        and thresholds (tau_*). Must also contain previous states (Y_*_prev) and
        historical states (Y_*_hist).
    smoothing_factor : float
        Steepness factor for the logistic transition probability curve. Higher values approximate a step function.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated state columns (Y_*), updated history (Y_*_hist),
        updated previous state markers (Y_*_prev becomes the state before this update),
        and updated transition markers (Y_*_transition for 0->1 changes in this step).
    """
    # Store current states to later become the 'previous' states for the next timestep
    Y_O_prev = households_df['Y_O'].values.copy()
    Y_S_prev = households_df['Y_S'].values.copy()
    Y_R_prev = households_df['Y_R'].values.copy()

    # Get current states, influences, and thresholds
    Y_O = households_df['Y_O'].values # Current state (before update)
    Y_S = households_df['Y_S'].values
    Y_R = households_df['Y_R'].values
    I_O = households_df['I_O'].values
    I_S = households_df['I_S'].values
    I_R = households_df['I_R'].values
    tau_O = households_df['tau_O'].values # Thresholds generated for this step
    tau_S = households_df['tau_S'].values
    tau_R = households_df['tau_R'].values

    # --- Calculate State Transition Probabilities ---
    # Probability of transitioning from 0 to 1 using logistic function
    # p = 1 / (1 + exp(-k * (Influence - Threshold)))
    p_O = 1 / (1 + np.exp(-smoothing_factor * (I_O - tau_O)))
    p_S = 1 / (1 + np.exp(-smoothing_factor * (I_S - tau_S)))
    p_R = 1 / (1 + np.exp(-smoothing_factor * (I_R - tau_R)))

    # Store probabilities for analysis (optional)
    households_df['p_O'] = p_O
    households_df['p_S'] = p_S
    households_df['p_R'] = p_R

    # --- Stochastic State Update (0 -> 1 transitions only) ---
    # Generate random numbers for stochastic check
    rand_O = np.random.rand(len(Y_O))
    rand_S = np.random.rand(len(Y_S))
    rand_R = np.random.rand(len(Y_R))

    # Update state to 1 if currently 0 AND random number is less than transition probability
    new_Y_O = np.where((Y_O == 0) & (rand_O < p_O), 1, Y_O)
    new_Y_S = np.where((Y_S == 0) & (rand_S < p_S), 1, Y_S)
    new_Y_R = np.where((Y_R == 0) & (rand_R < p_R), 1, Y_R)

    # --- Update Tracking Columns ---
    # 1. Transition Markers (for next step's bridging influence)
    # A transition occurred *in this step* if the state went from 0 (Y_O_prev) to 1 (new_Y_O)
    Y_O_transition = ((Y_O_prev == 0) & (new_Y_O == 1)).astype(np.int8)
    Y_S_transition = ((Y_S_prev == 0) & (new_Y_S == 1)).astype(np.int8)
    Y_R_transition = ((Y_R_prev == 0) & (new_Y_R == 1)).astype(np.int8)

    # 2. Historical States (for next step's bonding influence)
    # Update historical state to be the maximum state seen so far (current or previous history)
    Y_O_hist = np.maximum(new_Y_O, households_df['Y_O_hist'].values)
    Y_S_hist = np.maximum(new_Y_S, households_df['Y_S_hist'].values)
    Y_R_hist = np.maximum(new_Y_R, households_df['Y_R_hist'].values)

    # --- Store Updated Values in DataFrame ---
    # Store the *newly calculated* states
    households_df['Y_O'] = new_Y_O
    households_df['Y_S'] = new_Y_S
    households_df['Y_R'] = new_Y_R

    # Store the *state before this update* as the 'previous' state for the next cycle
    households_df['Y_O_prev'] = Y_O_prev
    households_df['Y_S_prev'] = Y_S_prev
    households_df['Y_R_prev'] = Y_R_prev

    # Store the *updated* historical max states
    households_df['Y_O_hist'] = Y_O_hist
    households_df['Y_S_hist'] = Y_S_hist
    households_df['Y_R_hist'] = Y_R_hist

    # Store the *transition markers calculated in this step*
    households_df['Y_O_transition'] = Y_O_transition
    households_df['Y_S_transition'] = Y_S_transition
    households_df['Y_R_transition'] = Y_R_transition

    return households_df


# --- Visualization Functions ---

def visualize_network(households_df, W_B, W_R, id_to_idx, time_step, output_dir='.'):
    """
    Visualize the network with nodes colored by group/state at a specific time step.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data for the current time step.
    W_B, W_R : ndarray
        Adjacency matrices for bonding and bridging networks.
    id_to_idx : dict
        Mapping from household ID to matrix index.
    time_step : int
        Current simulation time step.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print(f"  Generating network visualization for t={time_step}...")
    # Create a new graph
    G = nx.Graph()

    # Get necessary data from DataFrame
    household_ids = households_df['household_id'].values
    idx_to_id = {v: k for k, v in id_to_idx.items()} # Need index to ID mapping
    Y_O = households_df['Y_O'].values
    Y_S = households_df['Y_S'].values
    Y_R = households_df['Y_R'].values
    group_ids = households_df['group_id'].values
    lats = households_df['lat'].values
    lons = households_df['lon'].values

    # Add nodes with attributes
    for i, h_id in enumerate(household_ids):
        G.add_node(h_id,
                   pos=(lons[i], lats[i]), # Use lon, lat for x, y plotting
                   Y_O=Y_O[i],
                   Y_S=Y_S[i],
                   Y_R=Y_R[i],
                   group_id=group_ids[i])

    # Create edge lists for bonding and bridging networks
    bonding_edges = []
    bridging_edges = []
    n_households = len(household_ids)

    for i in range(n_households):
        for j in range(i + 1, n_households):  # Avoid self-loops and duplicates
            h_i = idx_to_id[i]
            h_j = idx_to_id[j]

            if W_B[i, j] > 0:
                bonding_edges.append((h_i, h_j, {'weight': W_B[i, j], 'type': 'bonding'}))
            if W_R[i, j] > 0:
                bridging_edges.append((h_i, h_j, {'weight': W_R[i, j], 'type': 'bridging'}))

    # Add edges to graph
    G.add_edges_from(bonding_edges)
    G.add_edges_from(bridging_edges)

    # Get node positions
    pos = nx.get_node_attributes(G, 'pos')

    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(20, 18)) # Increased size slightly
    axes = axes.flatten()

    # --- Plotting Setup ---
    n_groups = households_df['group_id'].nunique()
    # Ensure n_groups is at least 1 for cmap
    cmap_n_groups = max(1, n_groups)
    group_cmap = plt.cm.get_cmap('tab10', cmap_n_groups) # Colormap for groups
    state_cmap = LinearSegmentedColormap.from_list('StateColors', ['green', 'red']) # 0=green, 1=red

    node_size = 40 # Slightly larger nodes
    edge_alpha_B = 0.4
    edge_alpha_R = 0.25
    edge_width_B = 0.8
    edge_width_R = 0.6

    # --- Plot 1: Nodes colored by Group ID ---
    ax = axes[0]
    node_colors_group = [G.nodes[node]['group_id'] for node in G.nodes()]
    # Normalize colors if n_groups > 0
    norm_colors = np.array(node_colors_group) / max(1, n_groups -1) if n_groups > 1 else np.zeros(len(node_colors_group))

    nx.draw_networkx_nodes(G, pos, node_color=node_colors_group, cmap=group_cmap, vmin=0, vmax=max(0, n_groups-1),
                           node_size=node_size, alpha=0.9, ax=ax)
    nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                           alpha=edge_alpha_B, edge_color='blue', ax=ax)
    nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                           alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)
    ax.set_title(f'Network by Group Membership (t={time_step})', fontsize=16)
    ax.axis('off') # Turn off axis box

    # --- Plots 2-4: Nodes colored by Decision State (Y_O, Y_S, Y_R) ---
    states_info = [
        {'attr': 'Y_O', 'title': 'Occupancy State (Y_O=1 is Vacant)', 'ax_idx': 1},
        {'attr': 'Y_S', 'title': 'Sales State (Y_S=1 is For Sale)', 'ax_idx': 2},
        {'attr': 'Y_R', 'title': 'Recovery State (Y_R=1 is Repaired)', 'ax_idx': 3},
    ]

    for info in states_info:
        ax = axes[info['ax_idx']]
        attr = info['attr']
        node_colors_state = [G.nodes[node][attr] for node in G.nodes()]

        nx.draw_networkx_nodes(G, pos, node_color=node_colors_state, cmap=state_cmap, vmin=0, vmax=1,
                               node_size=node_size, alpha=0.9, ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                               alpha=edge_alpha_B, edge_color='blue', ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                               alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)

        # Add title with overall percentage
        state_pct = np.mean(node_colors_state) * 100 if node_colors_state else 0 # Handle empty list
        ax.set_title(f'{info["title"]} (t={time_step}) - {state_pct:.1f}%', fontsize=16)
        ax.axis('off') # Turn off axis box

        # Add group-level statistics in a text box
        group_stats = []
        unique_group_ids = sorted(households_df['group_id'].unique())
        for g_id in unique_group_ids:
            group_nodes = [node for node in G.nodes() if G.nodes[node]['group_id'] == g_id]
            if group_nodes: # Ensure group is not empty
                group_state_values = [G.nodes[node][attr] for node in group_nodes]
                group_pct = np.mean(group_state_values) * 100 if group_state_values else 0
                group_stats.append(f"Grp {g_id}: {group_pct:.1f}%")
            else:
                 group_stats.append(f"Grp {g_id}: N/A")

        stats_text = "\n".join(group_stats)
        ax.text(0.02, 0.02, stats_text, fontsize=10, transform=ax.transAxes,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

    # --- Legend ---
    legend_handles = []
    # Group colors
    unique_group_ids_legend = sorted(households_df['group_id'].unique())
    if unique_group_ids_legend:
        norm_factor = max(1, n_groups - 1) # Avoid division by zero if n_groups is 0 or 1
        for g_id in unique_group_ids_legend:
             legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',
                                             markerfacecolor=group_cmap(g_id / norm_factor), # Normalize index
                                             markersize=10, label=f'Group {g_id}'))
    # Edge types
    legend_handles.extend([
        plt.Line2D([0], [0], color='blue', lw=2, label='Bonding'),
        plt.Line2D([0], [0], color='green', lw=2, linestyle='--', label='Bridging')
    ])
    # State colors
    legend_handles.extend([
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='State=0'),
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='State=1')
    ])

    # Calculate dynamic ncol for legend based on number of items
    num_legend_items = len(legend_handles)
    legend_ncol = min(num_legend_items, 8) # Max 8 columns, adjust as needed

    fig.legend(handles=legend_handles, loc='lower center', ncol=legend_ncol, fontsize=12, bbox_to_anchor=(0.5, 0.01))

    plt.tight_layout(rect=[0, 0.05, 1, 0.96]) # Adjust rect to prevent title overlap and fit legend
    filename = os.path.join(output_dir, f'network_state_t{time_step:03d}.png') # Use padded number for sorting
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show() # Avoid showing plot in script execution
    plt.close(fig) # Close the figure to free memory
    print(f"    Visualization saved to {filename}")




def visualize_state_evolution(results, output_dir='.'):
    """
    Visualize the evolution of the proportion of households in each state (Y=1) over time.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing state evolution over time...")
    time_steps = sorted(results.keys())
    if not time_steps:
        print("  No results to visualize.")
        return

    # Calculate proportions for each state at each time step
    props = {'Y_O': [], 'Y_S': [], 'Y_R': []}
    for t in time_steps:
        df = results[t]
        if not df.empty:
            props['Y_O'].append(np.mean(df['Y_O']) * 100)
            props['Y_S'].append(np.mean(df['Y_S']) * 100)
            props['Y_R'].append(np.mean(df['Y_R']) * 100)
        else: # Handle empty dataframe case if needed
            props['Y_O'].append(np.nan) # Use NaN for missing data points
            props['Y_S'].append(np.nan)
            props['Y_R'].append(np.nan)


    # Create plot
    plt.figure(figsize=(12, 7))
    plt.plot(time_steps, props['Y_O'], 'r-o', linewidth=2, markersize=5, label='Vacant (Y_O=1)')
    plt.plot(time_steps, props['Y_S'], 'g-^', linewidth=2, markersize=5, label='For Sale (Y_S=1)')
    plt.plot(time_steps, props['Y_R'], 'b-s', linewidth=2, markersize=5, label='Repaired (Y_R=1)')

    # Formatting
    plt.xlabel('Time Step', fontsize=14)
    plt.ylabel('Proportion of Households (%)', fontsize=14)
    plt.title('Evolution of Household States Over Time', fontsize=16)
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.legend(fontsize=12, loc='best')
    # Ensure x-ticks cover the range, adjust frequency dynamically
    tick_step = max(1, len(time_steps) // 12) # Show ~12 ticks max
    plt.xticks(time_steps[::tick_step])
    plt.yticks(np.arange(0, 101, 10)) # Y-axis ticks every 10%
    plt.ylim(-2, 102) # Add slight padding to y-axis

    # Save figure
    plt.tight_layout()
    filename = os.path.join(output_dir, 'state_evolution.png')
    plt.savefig(filename, dpi=300)
    plt.show()
    plt.close()
    print(f"  State evolution plot saved to {filename}")


def homogenize_group_attributes(households_df, seed=None):
    """
    Make all households within the same group have identical attributes.
    Called after group assignment to ensure within-group homogeneity.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id' already assigned.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with homogenized household attributes (same values within groups).
    """
    print("Homogenizing attributes within groups...")

    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)

    # Get unique group IDs
    group_ids = sorted(households_df['group_id'].unique())

    # Attributes to homogenize
    attributes = ['income', 'resource_access', 'damage_structural', 'damage_utility']

    # For each group, set unified attribute values
    for g_id in group_ids:
        # Get indices of households in this group
        group_indices = households_df[households_df['group_id'] == g_id].index

        if len(group_indices) > 0:
            # Method 1: Randomly generate a single set of binary attributes for the entire group
            group_income = np.random.uniform(0,0.7)
            group_resource_access = np.random.uniform(0,0.3)
            group_damage_structural = np.random.uniform(0,0.5)
            group_damage_utility = np.random.uniform(0,0.3)

            # Apply the same values to all households in this group
            households_df.loc[group_indices, 'income'] = group_income
            households_df.loc[group_indices, 'resource_access'] = group_resource_access
            households_df.loc[group_indices, 'damage_structural'] = group_damage_structural
            households_df.loc[group_indices, 'damage_utility'] = group_damage_utility

            print(f"  Group {g_id} ({len(group_indices)} households) - Attributes set to: " +
                  f"income={group_income}, resource_access={group_resource_access}, " +
                  f"damage_structural={group_damage_structural}, damage_utility={group_damage_utility}")

    return households_df


def run_simulation_with_groups(households_df_initial, network_df, simulation_params):
    """
    Run the full agent-based simulation with group-level dynamics and time-varying thresholds.

    Parameters:
    -----------
    households_df_initial : DataFrame
        Initial DataFrame containing basic household attributes (ID, lat, lon, characteristics).
    network_df : DataFrame
        DataFrame containing network connections ('home_1', 'home_2', 'type', 'avg_link').
    simulation_params : dict
        Dictionary of simulation parameters (num_steps, seed, thresholds, omega, etc.).

    Returns:
    --------
    results : dict
        Dictionary mapping time step (int) to DataFrame with full household state at that step.
    group_df : DataFrame
        DataFrame containing the final group attributes and threshold parameters.
    """
    # --- Parameter Extraction ---
    num_steps = simulation_params.get('num_steps', 20)
    output_dir = simulation_params.get('output_dir', 'simulation_results')
    random_seed = simulation_params.get('random_seed', None)
    damage_threshold = simulation_params.get('damage_threshold', 0.5)
    visualization_interval = simulation_params.get('visualization_interval', 5)
    omega_matrix = simulation_params.get('omega_matrix')
    threshold_params = simulation_params.get('threshold_params')
    num_groups = simulation_params.get('num_groups', 5)
    smoothing_factor = simulation_params.get('smoothing_factor')

    print("="*30)
    print(" Starting Time-Varying Threshold Simulation Run ")
    print("="*30)
    print(f"Parameters: Steps={num_steps}, Groups={num_groups}, Seed={random_seed}, Output='{output_dir}'")

    # --- Setup ---
    # Set global random seed for reproducibility
    if random_seed is not None:
        np.random.seed(random_seed)

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Make a copy to avoid modifying the original input dataframe
    households_df = households_df_initial.copy()

    # --- Initialization Steps (Time t=0) ---
    print("\n--- Initialization (t=0) ---")
    # 1. Assign households to groups
    households_df, group_centroids = assign_households_to_groups(
        households_df, num_groups=num_groups, random_seed=random_seed)

    # 2. Homogenize attributes within groups if needed
    households_df = homogenize_group_attributes(households_df, seed=random_seed)

    # 3. Generate group-level attributes (means, feature vectors)
    group_df = generate_group_attributes(households_df, group_centroids)

    # 4. Initialize group-level lambda values at t=0
    group_df = initialize_group_lambda(group_df, threshold_params)

    # 5. Assign initial individual thresholds to households using lambda values at t=0
    households_df = assign_individual_thresholds_from_lambda(households_df, group_df, 0, threshold_params)

    # 6. Initialize household decision states (Y_O, Y_S, Y_R) and history/tracking columns
    households_df = initialize_decision_states(households_df, damage_threshold, seed=random_seed)

    # 7. Create static network matrices (W_B, W_R) and ID/index mappings
    W_B, W_R, id_to_idx, idx_to_id = create_network_matrices(households_df, network_df)

    # --- Store Initial State ---
    print("  Calculating initial influences for t=0...")
    households_df = calculate_raw_influences(households_df, W_B, W_R)
    households_df = calculate_adjusted_influences(households_df, omega_matrix)

    # Add placeholder columns for probabilities at t=0, filled with NaN
    households_df['p_O'] = np.nan
    households_df['p_S'] = np.nan
    households_df['p_R'] = np.nan

    results = {0: households_df.copy()}
    print("Initialization complete.")

    # --- Initial Visualizations ---
    print("\n--- Initial Visualizations ---")
    if visualization_interval > 0:
        visualize_network(households_df, W_B, W_R, id_to_idx, 0, output_dir)

    # --- Simulation Loop (Time t=1 to num_steps) ---
    print(f"\n--- Running Simulation Steps (1 to {num_steps}) ---")
    for t in range(1, num_steps + 1):
        print(f"\n--- Time Step {t} ---")

        # 1. Update group-level lambda values using AR(1) process
        print(f"  Updating group-level lambda values...")
        group_df = update_group_lambda(group_df, t, threshold_params)

        # 2. Calculate Raw Influences (m_*) based on states from t-1
        print(f"  Calculating raw influences...")
        households_df = calculate_raw_influences(households_df, W_B, W_R)

        # 3. Calculate Adjusted Influences (I_*) using Omega matrix
        print(f"  Calculating adjusted influences...")
        households_df = calculate_adjusted_influences(households_df, omega_matrix)

        # 4. Assign NEW Individual Thresholds using group lambda at time t
        print(f"  Assigning new individual thresholds from group lambda...")
        households_df = assign_individual_thresholds_from_lambda(households_df, group_df, t, threshold_params)

        # 5. Update Household States based on new influences and thresholds
        print(f"  Updating household states...")
        households_df = update_states_with_group_thresholds(households_df, smoothing_factor)

        # 6. Store results for this time step
        results[t] = households_df.copy()
        print(f"  State update complete for t={t}.")

        # Calculate and display summary statistics
        mean_O = np.nanmean(households_df['Y_O'])*100 if 'Y_O' in households_df.columns and not households_df['Y_O'].empty else np.nan
        mean_S = np.nanmean(households_df['Y_S'])*100 if 'Y_S' in households_df.columns and not households_df['Y_S'].empty else np.nan
        mean_R = np.nanmean(households_df['Y_R'])*100 if 'Y_R' in households_df.columns and not households_df['Y_R'].empty else np.nan
        print(f"    Vacant: {mean_O:.1f}%, "
              f"For Sale: {mean_S:.1f}%, "
              f"Repaired: {mean_R:.1f}%")

        # 7. Visualize network at specified intervals
        if visualization_interval > 0 and (t % visualization_interval == 0 or t == num_steps):
             visualize_network(households_df, W_B, W_R, id_to_idx, t, output_dir)

    print("\n--- Simulation Loop Completed ---")

    # --- Final Visualizations & Export ---
    print("\n--- Final Visualizations & Export ---")

    # 1. Visualize state evolution
    visualize_state_evolution(results, output_dir)

    # 3. Visualize lambda evolution (NEW)
    time_steps = sorted(results.keys())
    visualize_lambda_evolution(group_df, time_steps, output_dir)

    # 4. Visualize tau evolution (NEW)
    visualize_tau_evolution(group_df, time_steps, output_dir)

    # 5. Export results
    export_results_to_csv(results, output_dir)

    print("\nSimulation finished successfully!")
    print(f"Results and visualizations saved in '{output_dir}'")
    print("="*30)

    return results, group_df


# --- Export Function ---

def export_results_to_csv(results, output_dir):
    """
    Export simulation results to a CSV file, including states, thresholds, and influences.
    Handles potentially missing probability columns at t=0.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data for that step.
    output_dir : str
        Directory to save the output CSV file.
    """
    print("Exporting detailed simulation results to CSV...")
    all_data = []

    # Define base and optional columns
    base_cols = [
        'household_id', 'group_id',
        'Y_O', 'Y_S', 'Y_R',
        'tau_O', 'tau_S', 'tau_R',
        'I_O', 'I_S', 'I_R',
        'm_O', 'm_S', 'm_R'
    ]
    optional_cols = ['p_O', 'p_S', 'p_R']

    # Process each timestep's DataFrame
    for timestep, df in results.items():
        # Check which columns actually exist in this timestep's DataFrame
        cols_to_select = [col for col in base_cols if col in df.columns]
        existing_optional_cols = [col for col in optional_cols if col in df.columns]
        all_cols_this_step = cols_to_select + existing_optional_cols

        if not all_cols_this_step: # Skip if somehow df is empty or has no relevant columns
             print(f"Warning: No columns to select for timestep {timestep}. Skipping.")
             continue

        # Select only the existing columns
        df_export = df[all_cols_this_step].copy() # Create a copy

        # Add timestep column
        df_export['timestep'] = timestep

        # Rename columns for clarity in CSV
        rename_map = {
            'Y_O': 'occupancy_state',    # 1=vacant
            'Y_S': 'sales_state',        # 1=for_sale
            'Y_R': 'recovery_state',     # 1=repaired
            'tau_O': 'threshold_occupancy',
            'tau_S': 'threshold_sale',
            'tau_R': 'threshold_recovery',
            'I_O': 'influence_occupancy', # Adjusted influence
            'I_S': 'influence_sale',
            'I_R': 'influence_recovery',
            'm_O': 'raw_influence_O',     # Raw influence
            'm_S': 'raw_influence_S',
            'm_R': 'raw_influence_R',
            'p_O': 'prob_transition_O',   # Transition probability
            'p_S': 'prob_transition_S',
            'p_R': 'prob_transition_R'
        }
        # Only rename columns that exist in df_export
        actual_rename_map = {k: v for k, v in rename_map.items() if k in df_export.columns}
        df_export.rename(columns=actual_rename_map, inplace=True)

        all_data.append(df_export)

    # Concatenate all dataframes
    if not all_data:
        print("  No data to export.")
        return

    results_df = pd.concat(all_data, ignore_index=True, sort=False) # sort=False preserves columns

    # Define desired final column order (superset of all possible columns)
    final_column_order = [
        'household_id', 'timestep', 'group_id',
        'occupancy_state', 'sales_state', 'recovery_state',
        'threshold_occupancy', 'threshold_sale', 'threshold_recovery',
        'influence_occupancy', 'influence_sale', 'influence_recovery',
        'raw_influence_O', 'raw_influence_S', 'raw_influence_R',
        'prob_transition_O', 'prob_transition_S', 'prob_transition_R'
    ]
    # Filter order to only include columns actually present in the concatenated df
    existing_final_columns = [col for col in final_column_order if col in results_df.columns]
    results_df = results_df[existing_final_columns]


    # Sort for readability
    results_df = results_df.sort_values(by=['household_id', 'timestep'])

    # Save to CSV
    csv_path = os.path.join(output_dir, 'decision_states_simulation_details.csv')
    try:
        results_df.to_csv(csv_path, index=False, float_format='%.5f') # Format floats
        print(f"  Detailed results saved to {csv_path}")
    except Exception as e:
        print(f"  Error saving CSV: {e}")


# --- Main Execution ---

# Updated main function to include AR(1) parameters
def main():
    """Main function to set up parameters and run the simulation with time-varying thresholds."""

    # --- Configuration ---
    NETWORK_FILE = 'small_baltimore_household.csv'  # Ensure this file exists
    OUTPUT_DIR = 'simulation_results_dynamic_thresholds'
    RANDOM_SEED = 12
    NUM_STEPS = 24
    NUM_GROUPS = 5
    VISUALIZATION_INTERVAL = 4
    DAMAGE_THRESHOLD = 0.15
    SMOOTHING_FACTOR = 9

    # Correlation matrix (Omega)
    OMEGA_MATRIX = np.array([
        [1.0, 0.3, -0.3],  # Influence -> State O
        [0.2, 1.0, 0.1],   # Influence -> State S
        [-0.2, 0.1, 1.0]   # Influence -> State R
    ])

    # Beta coefficients for long-term means (same as in original)
    BETA_PARAMS = {
        # Beta coefficients for Occupancy (O) threshold mean
        'beta_H_O': np.array([-0.5, -0.3]),
        'beta_D_O': np.array([0.8, 0.6]),

        # Beta coefficients for Sales (S) threshold mean
        'beta_H_S': np.array([-0.1, 0.4]),
        'beta_D_S': np.array([0.5, -0.7]),

        # Beta coefficients for Recovery (R) threshold mean
        'beta_H_R': np.array([-0.1, 0.3]),
        'beta_D_R': np.array([0.2, 0.1]),
    }

    # NEW AR(1) parameters for time-varying thresholds
    AR1_PARAMS = {
        # Persistence parameters φ (phi) - controls how quickly lambda reverts to mean
        # Higher values = slower reversion (more persistence)
        'phi_O': 0.8,
        'phi_S': 0.5,
        'phi_R': 0.8,

        # Standard deviations for innovations ε (epsilon) - controls random shocks
        'sigma_epsilon_O': 0.4,
        'sigma_epsilon_S': 0.4,
        'sigma_epsilon_R': 0.4,

        # Standard deviations for individual household heterogeneity
        'sigma_ind_O': 0.4,
        'sigma_ind_S': 0.6,
        'sigma_ind_R': 0.4,
    }

    # Combine all threshold parameters
    THRESHOLD_PARAMS = {**BETA_PARAMS, **AR1_PARAMS}

    # --- Data Loading ---
    print("--- Loading Data ---")
    try:
        network_df = pd.read_csv(NETWORK_FILE)
        print(f"Network data loaded: {len(network_df)} connections from '{NETWORK_FILE}'")
    except FileNotFoundError:
        print(f"Error: Network file not found at '{NETWORK_FILE}'. Please ensure the file exists.")
        return
    except Exception as e:
        print(f"Error loading network file '{NETWORK_FILE}': {e}")
        return

    # --- Prepare Household Data ---
    print("\n--- Preparing Household Data ---")
    try:
        homes1 = network_df[['home_1', 'home_1_lat', 'home_1_lon']].rename(
            columns={'home_1': 'id', 'home_1_lat': 'lat', 'home_1_lon': 'lon'})
        homes2 = network_df[['home_2', 'home_2_lat', 'home_2_lon']].rename(
            columns={'home_2': 'id', 'home_2_lat': 'lat', 'home_2_lon': 'lon'})
        unique_homes = pd.concat([homes1, homes2]).drop_duplicates(subset=['id']).reset_index(drop=True)

        household_ids = unique_homes['id'].tolist()
        household_locs = {row['id']: (float(row['lat']), float(row['lon']))
                          for _, row in unique_homes.iterrows()
                          if pd.notna(row['lat']) and pd.notna(row['lon'])}

        # Filter household_ids to only include those with valid locations
        household_ids = [hid for hid in household_ids if hid in household_locs]
        print(f"Found {len(household_ids)} unique households with valid locations.")

        if not household_ids:
            print("Error: No unique households with valid lat/lon found in the network data.")
            return

        # Generate initial household attributes
        households_df_initial = generate_household_attributes(household_ids, household_locs, seed=RANDOM_SEED)

    except KeyError as e:
        print(f"Error preparing household data: Missing expected column in network file: {e}")
        return
    except Exception as e:
        print(f"Error preparing household data: {e}")
        return

    # --- Simulation Parameters Dictionary ---
    simulation_params = {
        'num_steps': NUM_STEPS,
        'output_dir': OUTPUT_DIR,
        'random_seed': RANDOM_SEED,
        'damage_threshold': DAMAGE_THRESHOLD,
        'visualization_interval': VISUALIZATION_INTERVAL,
        'num_groups': NUM_GROUPS,
        'omega_matrix': OMEGA_MATRIX,
        'threshold_params': THRESHOLD_PARAMS,
        'smoothing_factor': SMOOTHING_FACTOR
    }

    # --- Run Simulation ---
    try:
        results, final_group_df = run_simulation_with_groups(
            households_df_initial,
            network_df,
            simulation_params
        )
    except Exception as e:
        print(f"\n--- SIMULATION FAILED ---")
        print(f"Error during simulation run: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- Post-Simulation Analysis ---
    if results:
        final_step = max(results.keys())
        final_households_df = results[final_step]

        # Save final households data separately
        final_csv_path = os.path.join(OUTPUT_DIR, 'final_household_states.csv')
        try:
            final_households_df.to_csv(final_csv_path, index=False, float_format='%.5f')
            print(f"\nFinal household states (t={final_step}) saved to '{final_csv_path}'")
        except Exception as e:
            print(f"Error saving final household states CSV: {e}")

        # Print summary of final state
        print(f"\n--- Summary of Final State (t={final_step}) ---")
        n_final = len(final_households_df)
        if n_final > 0:
            print(f"Vacant (Y_O=1): {np.nansum(final_households_df['Y_O'])} / {n_final} ({np.nanmean(final_households_df['Y_O'])*100:.1f}%)")
            print(f"For Sale (Y_S=1): {np.nansum(final_households_df['Y_S'])} / {n_final} ({np.nanmean(final_households_df['Y_S'])*100:.1f}%)")
            print(f"Repaired (Y_R=1): {np.nansum(final_households_df['Y_R'])} / {n_final} ({np.nanmean(final_households_df['Y_R'])*100:.1f}%)")

            # Group-level statistics at the final step
            print("\nFinal Group-level Statistics:")
            for group_id in sorted(final_households_df['group_id'].unique()):
                group_data = final_households_df[final_households_df['group_id'] == group_id]
                n_group = len(group_data)
                if n_group > 0:
                    print(f"\nGroup {group_id} (n={n_group}):")
                    print(f"  Vacant:   {np.nanmean(group_data['Y_O'])*100:.1f}%")
                    print(f"  For Sale: {np.nanmean(group_data['Y_S'])*100:.1f}%")
                    print(f"  Repaired: {np.nanmean(group_data['Y_R'])*100:.1f}%")

                    # Show final lambda and tau values
                    lambda_O_col = f'lambda_O_{final_step}'
                    lambda_S_col = f'lambda_S_{final_step}'
                    lambda_R_col = f'lambda_R_{final_step}'

                    if all(col in final_group_df.columns for col in [lambda_O_col, lambda_S_col, lambda_R_col]):
                        lambda_O = final_group_df.loc[group_id, lambda_O_col]
                        lambda_S = final_group_df.loc[group_id, lambda_S_col]
                        lambda_R = final_group_df.loc[group_id, lambda_R_col]

                        tau_O = 1 / (1 + np.exp(-lambda_O))
                        tau_S = 1 / (1 + np.exp(-lambda_S))
                        tau_R = 1 / (1 + np.exp(-lambda_R))

                        print(f"  Final Group Lambda: λ_O={lambda_O:.3f}, λ_S={lambda_S:.3f}, λ_R={lambda_R:.3f}")
                        print(f"  Final Group Threshold: τ_O={tau_O:.3f}, τ_S={tau_S:.3f}, τ_R={tau_R:.3f}")
                        print(f"  Long-term Mean: μ̄_O={final_group_df.loc[group_id, 'mu_bar_O']:.3f}, " +
                              f"μ̄_S={final_group_df.loc[group_id, 'mu_bar_S']:.3f}, " +
                              f"μ̄_R={final_group_df.loc[group_id, 'mu_bar_R']:.3f}")
                else:
                    print(f"\nGroup {group_id}: No households in final data.")
        else:
            print("Final household data is empty.")
    else:
        print("Simulation did not produce results.")


if __name__ == "__main__":
    main()

"""#MCMC"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# import networkx as nx
import os
import time
import pickle
import json
import jax
import jax.numpy as jnp
from jax import random, lax
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, Predictive

numpyro.set_host_device_count(4)
rng_key = random.PRNGKey(24)
# --- Data Loading and Preparation (Modified) ---

def load_data(households_path, network_path, decisions_path):
    """
    Load the CSV data files.
    - households_path: Path to 'final_household_states.csv' (static attributes).
    - network_path: Path to network connections CSV.
    - decisions_path: Path to 'decision_states_simulation_details.csv' (time-series data).
    """
    print("Loading data...")
    if not os.path.exists(households_path):
        raise FileNotFoundError(f"Household attributes data not found at: {households_path}")
    if not os.path.exists(network_path):
        print(f"Warning: Network data not found at: {network_path}. Proceeding without network structure.")
        network_df = None
    else:
        network_df = pd.read_csv(network_path)
    if not os.path.exists(decisions_path):
        raise FileNotFoundError(f"Decision time-series data not found at: {decisions_path}")

    households_static_attributes_df = pd.read_csv(households_path)
    decisions_timeseries_df = pd.read_csv(decisions_path)

    print(f"Loaded {len(households_static_attributes_df)} households' static attributes.")
    print(f"Loaded {len(decisions_timeseries_df)} decision records.")
    if network_df is not None:
        print(f"Loaded {len(network_df)} network connections.")
    return households_static_attributes_df, network_df, decisions_timeseries_df


def prepare_data_for_inference(households_static_attributes_df, network_df, decisions_timeseries_df):
    """
    Prepare data for MCMC inference. Group lambdas are NOT loaded as observed data here,
    as they will be treated as latent variables in the model.
    """
    start_time = time.time()
    print("Preparing data for inference (group lambdas will be latent)...")

    all_household_ids_from_decisions = sorted(decisions_timeseries_df['household_id'].unique())
    n_households = len(all_household_ids_from_decisions)

    if 'household_id' not in households_static_attributes_df.columns:
        raise KeyError("'household_id' column missing from the households static attributes file.")
    h_attributes_df = households_static_attributes_df.set_index('household_id').reindex(all_household_ids_from_decisions).reset_index()

    if h_attributes_df['group_id'].isnull().any():
        missing_count = h_attributes_df['group_id'].isnull().sum()
        print(f"Warning: {missing_count} households from decision data are missing static attributes. Ensure data integrity.")

    timesteps = sorted(decisions_timeseries_df['timestep'].unique())
    n_timesteps = len(timesteps)
    print(f"Found {n_households} households and {n_timesteps} timesteps.")

    required_attr_cols = ['income', 'resource_access', 'damage_structural', 'damage_utility', 'group_id']
    for col in required_attr_cols:
        if col not in h_attributes_df.columns:
            raise KeyError(f"Required static attribute column '{col}' not found in households attribute data.")

    H_individual_np = np.column_stack([h_attributes_df['income'].values, h_attributes_df['resource_access'].values])
    D_individual_np = np.column_stack([h_attributes_df['damage_structural'].values, h_attributes_df['damage_utility'].values])

    group_ids_sim = h_attributes_df['group_id'].values
    unique_sim_group_ids_sorted = sorted(h_attributes_df['group_id'].dropna().unique()) # dropna before unique
    sim_gid_to_mcmc_gidx_map = {sim_gid: i for i, sim_gid in enumerate(unique_sim_group_ids_sorted)}
    n_groups = len(unique_sim_group_ids_sorted)
    if n_groups == 0:
        raise ValueError("No valid groups found after processing group_id. Check input data.")

    group_indices_for_households_np = np.array([sim_gid_to_mcmc_gidx_map.get(gid, -1) for gid in group_ids_sim], dtype=np.int32)
    if np.any(group_indices_for_households_np == -1):
        raise ValueError("Some households have group_ids not in unique_sim_group_ids_sorted. This indicates NaN or inconsistent group_ids.")


    Y_O_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)
    Y_S_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)
    Y_R_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)
    m_O_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)
    m_S_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)
    m_R_np = np.full((n_timesteps, n_households), np.nan, dtype=np.float32)

    state_cols_map = {'Y_O': 'occupancy_state', 'Y_S': 'sales_state', 'Y_R': 'recovery_state'}
    raw_influence_cols_map = {'m_O': 'raw_influence_O', 'm_S': 'raw_influence_S', 'm_R': 'raw_influence_R'}

    for col_set in [state_cols_map, raw_influence_cols_map]:
        for key, csv_col_name in col_set.items():
            if csv_col_name not in decisions_timeseries_df.columns:
                raise KeyError(f"Expected column '{csv_col_name}' for '{key}' not found in decisions_timeseries_df.")

    hid_to_hidx_map = {hid: i for i, hid in enumerate(all_household_ids_from_decisions)}

    for t_idx, t_val in enumerate(timesteps):
        t_data_df = decisions_timeseries_df[decisions_timeseries_df['timestep'] == t_val]
        for _, row in t_data_df.iterrows():
            h_id = row['household_id']
            if h_id in hid_to_hidx_map:
                h_idx = hid_to_hidx_map[h_id]
                Y_O_np[t_idx, h_idx] = row[state_cols_map['Y_O']]
                Y_S_np[t_idx, h_idx] = row[state_cols_map['Y_S']]
                Y_R_np[t_idx, h_idx] = row[state_cols_map['Y_R']]
                m_O_np[t_idx, h_idx] = row[raw_influence_cols_map['m_O']]
                m_S_np[t_idx, h_idx] = row[raw_influence_cols_map['m_S']]
                m_R_np[t_idx, h_idx] = row[raw_influence_cols_map['m_R']]

    data_dict = {
        'Y_O': jnp.nan_to_num(jnp.array(Y_O_np), nan=0.0),
        'Y_S': jnp.nan_to_num(jnp.array(Y_S_np), nan=0.0),
        'Y_R': jnp.nan_to_num(jnp.array(Y_R_np), nan=0.0),
        'm_O': jnp.nan_to_num(jnp.array(m_O_np), nan=0.0),
        'm_S': jnp.nan_to_num(jnp.array(m_S_np), nan=0.0),
        'm_R': jnp.nan_to_num(jnp.array(m_R_np), nan=0.0),
        'H_individual': jnp.nan_to_num(jnp.array(H_individual_np), nan=0.0),
        'D_individual': jnp.nan_to_num(jnp.array(D_individual_np), nan=0.0),
        'group_indices_for_households': jnp.array(group_indices_for_households_np, dtype=jnp.int32),
        'n_households': n_households,
        'n_timesteps': n_timesteps,
        'n_groups': n_groups,
    }

    print(f"Data preparation completed in {time.time() - start_time:.2f} seconds.")
    print(f"Shapes: Y_O={data_dict['Y_O'].shape}, m_O={data_dict['m_O'].shape}, H_individual={data_dict['H_individual'].shape}")
    print(f"Num groups: {n_groups}, group_indices_for_households shape: {data_dict['group_indices_for_households'].shape}")
    return data_dict

# --- Prior Configuration (remains the same) ---
def setup_prior_config():
    prior_config = {
        'omega': {'mean': 0.0, 'std': 0.3},
        'beta_H': {'mean': 0.0, 'std': 1.0},
        'beta_D': {'mean': 0.0, 'std': 1.0},
        'phi': {'mean': 0.5, 'std': 0.5},
        'smoothing_factor': {'mean': 5.0, 'std': 5.0}
    }
    print("Prior configuration set:")
    print(json.dumps(prior_config, indent=2))
    return prior_config

# --- calculate_adjusted_influences (remains the same) ---
def calculate_adjusted_influences(raw_influences, omega):
    m = jnp.stack([raw_influences['m_O'], raw_influences['m_S'], raw_influences['m_R']], axis=-1)
    I = jnp.matmul(m, omega)
    return I[:, 0], I[:, 1], I[:, 2]

# --- NumPyro Model (Modified for Latent Group Lambdas) ---
def model(data, prior_config, fixed_params=None):
    if fixed_params is None: fixed_params = {}

    n_households = data['n_households']
    n_timesteps = data['n_timesteps']
    n_groups = data['n_groups']

    Y_O_obs, Y_S_obs, Y_R_obs = data['Y_O'], data['Y_S'], data['Y_R']
    m_O_obs, m_S_obs, m_R_obs = data['m_O'], data['m_S'], data['m_R']
    H_individual, D_individual = data['H_individual'], data['D_individual']
    group_indices_hh_to_g = data['group_indices_for_households']

    dim_H, dim_D = H_individual.shape[1], D_individual.shape[1]

    def sample_or_fix(param_name, dist_func, is_vector=False, dim_size=None, **dist_args):
        if param_name in fixed_params:
            value = jnp.asarray(fixed_params[param_name])
            if is_vector and dim_size is not None and value.shape != (dim_size,):
                if value.size == dim_size: value = value.reshape((dim_size,))
                else: raise ValueError(f"Shape mismatch for fixed {param_name}.")
            return numpyro.deterministic(param_name, value)
        else:
            if is_vector and dim_size is not None:
                return numpyro.sample(param_name, dist_func(**dist_args).expand([dim_size]))
            return numpyro.sample(param_name, dist_func(**dist_args))

    # --- Omega, Smoothing Factor, Betas, AR(1) params, Sigma_ind (Sampling as before) ---
    omega_0_1 = sample_or_fix("omega_0_1", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_0_2 = sample_or_fix("omega_0_2", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_1_0 = sample_or_fix("omega_1_0", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_1_2 = sample_or_fix("omega_1_2", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_2_0 = sample_or_fix("omega_2_0", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_2_1 = sample_or_fix("omega_2_1", dist.Normal, loc=prior_config['omega']['mean'], scale=prior_config['omega']['std'])
    omega_matrix = jnp.eye(3)
    omega_matrix = omega_matrix.at[0, 1].set(omega_0_1); omega_matrix = omega_matrix.at[0, 2].set(omega_0_2)
    omega_matrix = omega_matrix.at[1, 0].set(omega_1_0); omega_matrix = omega_matrix.at[1, 2].set(omega_1_2)
    omega_matrix = omega_matrix.at[2, 0].set(omega_2_0); omega_matrix = omega_matrix.at[2, 1].set(omega_2_1)
    omega_full_det = numpyro.deterministic("omega_full", omega_matrix)

    smoothing_factor = sample_or_fix("smoothing_factor", dist.Normal, loc=prior_config['smoothing_factor']['mean'], scale=prior_config['smoothing_factor']['std'])

    beta_H_O = sample_or_fix("beta_H_O", dist.Normal, is_vector=True, dim_size=dim_H, loc=prior_config['beta_H']['mean'], scale=prior_config['beta_H']['std'])
    beta_D_O = sample_or_fix("beta_D_O", dist.Normal, is_vector=True, dim_size=dim_D, loc=prior_config['beta_D']['mean'], scale=prior_config['beta_D']['std'])
    beta_H_S = sample_or_fix("beta_H_S", dist.Normal, is_vector=True, dim_size=dim_H, loc=prior_config['beta_H']['mean'], scale=prior_config['beta_H']['std'])
    beta_D_S = sample_or_fix("beta_D_S", dist.Normal, is_vector=True, dim_size=dim_D, loc=prior_config['beta_D']['mean'], scale=prior_config['beta_D']['std'])
    beta_H_R = sample_or_fix("beta_H_R", dist.Normal, is_vector=True, dim_size=dim_H, loc=prior_config['beta_H']['mean'], scale=prior_config['beta_H']['std'])
    beta_D_R = sample_or_fix("beta_D_R", dist.Normal, is_vector=True, dim_size=dim_D, loc=prior_config['beta_D']['mean'], scale=prior_config['beta_D']['std'])

    phi_O = sample_or_fix("phi_O", dist.Normal, loc=prior_config['phi']['mean'], scale=prior_config['phi']['std'])
    phi_S = sample_or_fix("phi_S", dist.Normal, loc=prior_config['phi']['mean'], scale=prior_config['phi']['std'])
    phi_R = sample_or_fix("phi_R", dist.Normal, loc=prior_config['phi']['mean'], scale=prior_config['phi']['std'])

    sigma_epsilon_O =  numpyro.sample("sigma_epsilon_O", dist.InverseGamma(concentration=2.0, rate=1.0))
    sigma_epsilon_S =  numpyro.sample("sigma_epsilon_S", dist.InverseGamma(concentration=2.0, rate=1.0))
    sigma_epsilon_R =  numpyro.sample("sigma_epsilon_R", dist.InverseGamma(concentration=2.0, rate=1.0))

    # Remove separate sigma_0 parameters as they're not needed
    # Instead, calculate stationary variances based on AR(1) process parameters

    sigma_ind_O =  numpyro.sample("sigma_ind_O", dist.InverseGamma(concentration=2.0, rate=1.0))
    sigma_ind_S =  numpyro.sample("sigma_ind_S", dist.InverseGamma(concentration=2.0, rate=1.0))
    sigma_ind_R =  numpyro.sample("sigma_ind_R", dist.InverseGamma(concentration=2.0, rate=1.0))

    # --- Calculate Group-Level Features (H_g, D_g) ---
    _H_g = jnp.zeros((n_groups, dim_H))
    _D_g = jnp.zeros((n_groups, dim_D))
    for g_idx in range(n_groups):
        mask = (group_indices_hh_to_g == g_idx)
        count = jnp.maximum(jnp.sum(mask), 1.0)
        _H_g = _H_g.at[g_idx].set(jnp.sum(H_individual * mask[:, None], axis=0) / count)
        _D_g = _D_g.at[g_idx].set(jnp.sum(D_individual * mask[:, None], axis=0) / count)
    H_g_det, D_g_det = numpyro.deterministic("H_g", _H_g), numpyro.deterministic("D_g", _D_g)

    # --- Calculate Long-Term Means (mu_bar_g^s) ---
    mu_bar_O_g = numpyro.deterministic("mu_bar_O_g", jnp.dot(H_g_det, beta_H_O) + jnp.dot(D_g_det, beta_D_O))
    mu_bar_S_g = numpyro.deterministic("mu_bar_S_g", jnp.dot(H_g_det, beta_H_S) + jnp.dot(D_g_det, beta_D_S))
    mu_bar_R_g = numpyro.deterministic("mu_bar_R_g", jnp.dot(H_g_det, beta_H_R) + jnp.dot(D_g_det, beta_D_R))

    # --- Sample Latent Group Lambdas (lambda_g,t^s) using AR(1) ---
    # Initialize arrays for latent group lambdas
    lambda_g_O_latent = jnp.zeros((n_timesteps, n_groups))
    lambda_g_S_latent = jnp.zeros((n_timesteps, n_groups))
    lambda_g_R_latent = jnp.zeros((n_timesteps, n_groups))

    # Calculate stationary variances for initial state (t=0) based on AR(1) parameters
    # Formula: sigma_epsilon^2 / (1 - phi^2)
    stationary_var_O = sigma_epsilon_O**2 / (1 - phi_O**2)
    stationary_var_S = sigma_epsilon_S**2 / (1 - phi_S**2)
    stationary_var_R = sigma_epsilon_R**2 / (1 - phi_R**2)

    # Calculate stationary standard deviations
    stationary_std_O = jnp.sqrt(stationary_var_O)
    stationary_std_S = jnp.sqrt(stationary_var_S)
    stationary_std_R = jnp.sqrt(stationary_var_R)

    # Store these calculated values as deterministic nodes for monitoring
    numpyro.deterministic("stationary_std_O", stationary_std_O)
    numpyro.deterministic("stationary_std_S", stationary_std_S)
    numpyro.deterministic("stationary_std_R", stationary_std_R)

    # Time t=0 (Using stationary distribution of AR(1) process)
    with numpyro.plate("groups_plate_latent_t0", n_groups):
        lambda_g0_O = numpyro.sample("lambda_g0_O", dist.Normal(mu_bar_O_g, stationary_std_O))
        lambda_g0_S = numpyro.sample("lambda_g0_S", dist.Normal(mu_bar_S_g, stationary_std_S))
        lambda_g0_R = numpyro.sample("lambda_g0_R", dist.Normal(mu_bar_R_g, stationary_std_R))

    lambda_g_O_latent = lambda_g_O_latent.at[0, :].set(lambda_g0_O)
    lambda_g_S_latent = lambda_g_S_latent.at[0, :].set(lambda_g0_S)
    lambda_g_R_latent = lambda_g_R_latent.at[0, :].set(lambda_g0_R)

    for t in range(1, n_timesteps):
        mean_ar1_O_t = (1 - phi_O) * mu_bar_O_g + phi_O * lambda_g_O_latent[t-1, :]
        mean_ar1_S_t = (1 - phi_S) * mu_bar_S_g + phi_S * lambda_g_S_latent[t-1, :]
        mean_ar1_R_t = (1 - phi_R) * mu_bar_R_g + phi_R * lambda_g_R_latent[t-1, :]

        with numpyro.plate(f"groups_plate_latent_t{t}", n_groups):
            lambda_gt_O_curr = numpyro.sample(f"lambda_gt_O_{t}", dist.Normal(mean_ar1_O_t, sigma_epsilon_O))
            lambda_gt_S_curr = numpyro.sample(f"lambda_gt_S_{t}", dist.Normal(mean_ar1_S_t, sigma_epsilon_S))
            lambda_gt_R_curr = numpyro.sample(f"lambda_gt_R_{t}", dist.Normal(mean_ar1_R_t, sigma_epsilon_R))

        lambda_g_O_latent = lambda_g_O_latent.at[t, :].set(lambda_gt_O_curr)
        lambda_g_S_latent = lambda_g_S_latent.at[t, :].set(lambda_gt_S_curr)
        lambda_g_R_latent = lambda_g_R_latent.at[t, :].set(lambda_gt_R_curr)

    # Make them deterministic for easier access in posterior if needed, though they are already sampled.
    lambda_g_O_latent_det = numpyro.deterministic("lambda_g_O_latent", lambda_g_O_latent)
    lambda_g_S_latent_det = numpyro.deterministic("lambda_g_S_latent", lambda_g_S_latent)
    lambda_g_R_latent_det = numpyro.deterministic("lambda_g_R_latent", lambda_g_R_latent)

    # --- Household-Level Latent Logit-Thresholds (Z_h,t or lambda_h,t) ---
    # Mean is the *latent sampled* group lambda.
    lambda_g_mapped_to_hh_O = lambda_g_O_latent_det[:, group_indices_hh_to_g] # Shape (T, H)
    lambda_g_mapped_to_hh_S = lambda_g_S_latent_det[:, group_indices_hh_to_g]
    lambda_g_mapped_to_hh_R = lambda_g_R_latent_det[:, group_indices_hh_to_g]

    eta_O = numpyro.sample("eta_O", dist.Normal(0, 1).expand([n_timesteps, n_households]))
    eta_S = numpyro.sample("eta_S", dist.Normal(0, 1).expand([n_timesteps, n_households]))
    eta_R = numpyro.sample("eta_R", dist.Normal(0, 1).expand([n_timesteps, n_households]))

    Z_O_ht = numpyro.deterministic("Z_O_ht", lambda_g_mapped_to_hh_O + sigma_ind_O * eta_O)
    Z_S_ht = numpyro.deterministic("Z_S_ht", lambda_g_mapped_to_hh_S + sigma_ind_S * eta_S)
    Z_R_ht = numpyro.deterministic("Z_R_ht", lambda_g_mapped_to_hh_R + sigma_ind_R * eta_R)

    tau_O_ht = numpyro.deterministic("tau_O_ht", jax.nn.sigmoid(Z_O_ht))
    tau_S_ht = numpyro.deterministic("tau_S_ht", jax.nn.sigmoid(Z_S_ht))
    tau_R_ht = numpyro.deterministic("tau_R_ht", jax.nn.sigmoid(Z_R_ht))

    # --- Likelihood for Observed Household Decisions Y_h,t^s ---
    def scan_body_Y_likelihood(carry_previous_Y_states, t_idx_scan): # t_idx_scan is current time t
        Y_O_prev_obs, Y_S_prev_obs, Y_R_prev_obs = carry_previous_Y_states
        current_raw_influences = {
            'm_O': m_O_obs[t_idx_scan, :], 'm_S': m_S_obs[t_idx_scan, :], 'm_R': m_R_obs[t_idx_scan, :]
        }
        I_O_t, I_S_t, I_R_t = calculate_adjusted_influences(current_raw_influences, omega_full_det)

        p_trans_O_if_0 = jax.nn.sigmoid(smoothing_factor * (I_O_t - tau_O_ht[t_idx_scan, :]))
        p_trans_S_if_0 = jax.nn.sigmoid(smoothing_factor * (I_S_t - tau_S_ht[t_idx_scan, :]))
        p_trans_R_if_0 = jax.nn.sigmoid(smoothing_factor * (I_R_t - tau_R_ht[t_idx_scan, :]))

        prob_Y_O_is_1 = jnp.where(Y_O_prev_obs == 1, 1.0, p_trans_O_if_0)
        prob_Y_S_is_1 = jnp.where(Y_S_prev_obs == 1, 1.0, p_trans_S_if_0)
        prob_Y_R_is_1 = jnp.where(Y_R_prev_obs == 1, 1.0, p_trans_R_if_0)

        next_carry_Y_O = Y_O_obs[t_idx_scan, :] # Use observed Y for next carry
        next_carry_Y_S = Y_S_obs[t_idx_scan, :]
        next_carry_Y_R = Y_R_obs[t_idx_scan, :]
        return (next_carry_Y_O, next_carry_Y_S, next_carry_Y_R), (prob_Y_O_is_1, prob_Y_S_is_1, prob_Y_R_is_1)

    initial_carry_Y = (Y_O_obs[0, :], Y_S_obs[0, :], Y_R_obs[0, :])
    # Scan for t = 1 to n_timesteps-1 (indices for scan_body are t_idx_scan = 1, ..., n_timesteps-1)
    _, (probs_Y_O_is_1_all_t, probs_Y_S_is_1_all_t, probs_Y_R_is_1_all_t) = lax.scan(
        scan_body_Y_likelihood, initial_carry_Y, jnp.arange(1, n_timesteps)
    )

    eps = jnp.finfo(jnp.float32).eps
    with numpyro.plate("households_Y_lik_plate", n_households):
        for t_scan_output_idx in range(n_timesteps - 1): # Corresponds to t=1, ..., n_timesteps-1
            t_actual = t_scan_output_idx + 1
            current_probs_O = jnp.clip(probs_Y_O_is_1_all_t[t_scan_output_idx, :], eps, 1.0 - eps)
            current_probs_S = jnp.clip(probs_Y_S_is_1_all_t[t_scan_output_idx, :], eps, 1.0 - eps)
            current_probs_R = jnp.clip(probs_Y_R_is_1_all_t[t_scan_output_idx, :], eps, 1.0 - eps)
            numpyro.sample(f"Y_O_t{t_actual}_lik", dist.Bernoulli(probs=current_probs_O), obs=Y_O_obs[t_actual, :])
            numpyro.sample(f"Y_S_t{t_actual}_lik", dist.Bernoulli(probs=current_probs_S), obs=Y_S_obs[t_actual, :])
            numpyro.sample(f"Y_R_t{t_actual}_lik", dist.Bernoulli(probs=current_probs_R), obs=Y_R_obs[t_actual, :])

# --- run_mcmc_inference (remains the same) ---
def run_mcmc_inference(data, prior_config, fixed_params=None, num_samples=500, num_warmup=200, num_chains=2, rng_key_seed=24):
    print(f"Running MCMC inference: {num_samples} samples, {num_warmup} warmup, {num_chains} chains.")
    kernel = NUTS(model, target_accept_prob=0.92, max_tree_depth=10)
    mcmc_instance = MCMC(
        kernel, num_samples=num_samples, num_warmup=num_warmup, num_chains=num_chains,
        progress_bar=True, chain_method='parallel' if num_chains > 1 else 'sequential',
    )
    current_rng_key = random.PRNGKey(rng_key_seed)
    start_time = time.time()
    mcmc_instance.run(current_rng_key, data=data, prior_config=prior_config, fixed_params=fixed_params)
    inference_time = time.time() - start_time
    print(f"MCMC inference completed in {inference_time:.2f} sec ({inference_time/60:.2f} min).")
    mcmc_instance.print_summary()
    return mcmc_instance, mcmc_instance.get_samples()

# --- Main Execution ---
def main_inference_ar1_latent_group_lambda():
    SIM_OUTPUT_DIR = 'simulation_results_dynamic_thresholds'
    HOUSEHOLDS_STATIC_ATTR_FILE = os.path.join(SIM_OUTPUT_DIR, 'final_household_states.csv')
    NETWORK_FILE = 'small_baltimore_household.csv'
    DECISIONS_TIMESERIES_FILE = os.path.join(SIM_OUTPUT_DIR, 'decision_states_simulation_details.csv')
    MCMC_OUTPUT_DIR = 'mcmc_inference_ar1_latent_gl_results' # New output dir
    os.makedirs(MCMC_OUTPUT_DIR, exist_ok=True)
    print(f"MCMC Results will be saved in: {MCMC_OUTPUT_DIR}")

    total_start_time = time.time()
    try:
        prior_config = setup_prior_config()
        fixed_params = {}
        mcmc_params = {'num_samples': 10000, 'num_warmup': 500, 'num_chains': 2}


        hs_static_df, net_df, dec_ts_df = load_data(HOUSEHOLDS_STATIC_ATTR_FILE, NETWORK_FILE, DECISIONS_TIMESERIES_FILE)
        data_for_mcmc = prepare_data_for_inference(hs_static_df, net_df, dec_ts_df)

        if data_for_mcmc['n_households'] == 0 or data_for_mcmc['n_timesteps'] == 0 or data_for_mcmc['n_groups'] == 0:
             raise ValueError("Insufficient data (households, timesteps, or groups is zero).")

        mcmc_run_instance, posterior_samples = run_mcmc_inference(
            data_for_mcmc, prior_config, fixed_params,
            num_samples=mcmc_params['num_samples'], num_warmup=mcmc_params['num_warmup'],
            num_chains=mcmc_params['num_chains'], rng_key_seed=456
        )

        samples_filename = os.path.join(MCMC_OUTPUT_DIR, "posterior_samples_ar1_latent_gl.pkl")
        posterior_samples_np = {k: np.array(v_ps) for k, v_ps in posterior_samples.items()}
        with open(samples_filename, "wb") as f_ps: pickle.dump(posterior_samples_np, f_ps)
        print(f"Posterior samples saved to {samples_filename}")

        total_time = time.time() - total_start_time
        print(f"\n--- MCMC Pipeline Finished --- Total time: {total_time:.2f} sec ---")

    except Exception as e_gen:
        print(f"\n--- AN ERROR OCCURRED ---")
        print(f"Error Type: {type(e_gen).__name__}, Message: {e_gen}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main_inference_ar1_latent_group_lambda()

"""##Visualization of Inference Result"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import matplotlib as mpl
from matplotlib.gridspec import GridSpec

def plot_smoothing_factor_posterior(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,  # Higher DPI for publication quality
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=10,
    figsize=(8, 5),  # More suitable aspect ratio for papers
    show_plot=True  # Default to not showing plots for batch processing
):
    """
    Plot the posterior distribution of the smoothing factor parameter in publication quality.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure (600+ for publication)
        font_family: Font family for plot text (e.g., 'serif' for papers)
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        figsize: Figure dimensions (width, height) in inches
        show_plot: Whether to display the plot (set to False for headless environments)
    """
    print(f"Loading posterior samples from {samples_file}...")

    try:
        # --- Configure plotting style for publication ---
        plt.style.use('seaborn-v0_8-whitegrid')  # Clean style
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Extract the smoothing factor samples
        if 'smoothing_factor' in posterior_samples:
            smoothing_samples = posterior_samples['smoothing_factor']
            print(f"Found {len(smoothing_samples)} samples for smoothing_factor")
        else:
            print("Error: 'smoothing_factor' not found in posterior samples.")
            print(f"Available keys: {list(posterior_samples.keys())}")
            return

        # Create figure
        fig, ax = plt.subplots(figsize=figsize)

        # Plot the posterior distribution with more subtle colors
        sns.histplot(smoothing_samples, kde=True, color='#4878d0', alpha=0.6, ax=ax, bins=30,
                    label='Posterior', edgecolor='white', linewidth=0.8)

        # Plot the prior distribution for comparison
        prior_mean, prior_std = 5.0, 5.0
        x = np.linspace(max(0, prior_mean - 3*prior_std), prior_mean + 3*prior_std, 1000)
        prior_density = np.exp(-0.5 * ((x - prior_mean) / prior_std)**2) / (prior_std * np.sqrt(2 * np.pi))
        # Scale prior density to match histogram scale for better comparison
        scaling_factor = len(smoothing_samples) * (max(smoothing_samples) - min(smoothing_samples)) / 30
        ax.plot(x, prior_density * scaling_factor, color='#d65f5f', linestyle='--',
                linewidth=1.5, label=r'Prior: $\mathcal{N}(5, 5)$')

        # Calculate summary statistics
        mean_val = np.mean(smoothing_samples)
        median_val = np.median(smoothing_samples)
        std_val = np.std(smoothing_samples)
        credible_05 = np.percentile(smoothing_samples, 5)
        credible_95 = np.percentile(smoothing_samples, 95)

        # Add reference lines with improved aesthetics
        ax.axvline(mean_val, color='#4878d0', linestyle='--', linewidth=1.2,
                  label=f'Mean: {mean_val:.2f}')
        ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=1.2,
                  label=f'Median: {median_val:.2f}')

        # Add 90% credible interval with transparency for the area
        ax.axvspan(credible_05, credible_95, alpha=0.15, color='#4878d0')
        ax.axvline(credible_05, color='#4878d0', linestyle=':', linewidth=1.0)
        ax.axvline(credible_95, color='#4878d0', linestyle=':', linewidth=1.0,
                  label=f'90% CI: [{credible_05:.2f}, {credible_95:.2f}]')

        # Add summary statistics as text with cleaner box
        stats_text = (
            f"Mean: {mean_val:.3f}\n"
            f"Median: {median_val:.3f}\n"
            f"Std Dev: {std_val:.3f}\n"
            f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
        )
        ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize,
                verticalalignment='top', horizontalalignment='right',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9,
                          edgecolor='#cccccc', linewidth=0.5))

        # Formatting with LaTeX for mathematical symbols
        ax.set_xlabel(r'Smoothing Factor ($k$)', fontsize=label_fontsize)
        ax.set_ylabel('Density', fontsize=label_fontsize)
        ax.set_title('Posterior Distribution of Smoothing Factor', fontsize=title_fontsize)

        # Improve legend appearance
        ax.legend(fontsize=legend_fontsize, loc='upper left', framealpha=0.9,
                 edgecolor='#cccccc', fancybox=True)

        # Set tick parameters
        ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Save the plot with proper extension
        plot_path = os.path.join(output_dir, f'smoothing_factor_posterior.{output_format}')
        plt.tight_layout()
        plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

        return smoothing_samples, (mean_val, median_val, std_val, credible_05, credible_95)

    except Exception as e:
        print(f"Error plotting smoothing factor: {str(e)}")
        import traceback
        traceback.print_exc()


def plot_omega_posterior(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,  # Higher DPI for publication quality
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=9,
    show_plot=True  # Default to not showing plots for batch processing
):
    """
    Plot the posterior distribution of the omega correlation matrix in publication quality.
    Handles asymmetric correlation matrices.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plots
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure (600+ for publication)
        font_family: Font family for plot text (e.g., 'serif' for papers)
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        show_plot: Whether to display the plots (set to False for headless environments)
    """
    print(f"Loading posterior samples for omega matrix from {samples_file}...")

    try:
        # --- Configure plotting style for publication ---
        plt.style.use('seaborn-v0_8-whitegrid')  # Clean style
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Extract the omega_full samples
        if 'omega_full' in posterior_samples:
            omega_samples = posterior_samples['omega_full']
            print(f"Found omega samples with shape {omega_samples.shape}")
        else:
            print("Error: 'omega_full' not found in posterior samples.")
            print(f"Available keys: {list(posterior_samples.keys())}")
            return

        # Calculate the posterior mean omega matrix
        omega_mean = np.mean(omega_samples, axis=0)

        # Create directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # --- PLOT 1: Heatmap of posterior mean omega ---
        fig1, ax1 = plt.subplots(figsize=(7, 6))

        # Create the heatmap with improved aesthetics
        cmap = sns.diverging_palette(240, 10, as_cmap=True)  # Blue to red diverging colormap
        mask = np.eye(omega_mean.shape[0], dtype=bool)  # Mask for diagonal elements

        # Create the heatmap
        hm = sns.heatmap(omega_mean, annot=True, cmap=cmap, vmin=-1, vmax=1,
                    square=True, center=0, fmt='.2f', ax=ax1,
                    cbar_kws={"shrink": 0.8, "label": "Correlation"})

        # Add labels and title using LaTeX
        ax1.set_title(r'Posterior Mean of $\Omega$ Correlation Matrix', fontsize=title_fontsize)
        ax1.set_xticklabels(['O', 'S', 'R'], fontsize=tick_fontsize)
        ax1.set_yticklabels(['O', 'S', 'R'], fontsize=tick_fontsize)

        # Add axis labels to clarify the meaning
        ax1.set_xlabel('Target State', fontsize=label_fontsize)
        ax1.set_ylabel('Source Influence', fontsize=label_fontsize)

        # Add a caption explaining the matrix
        caption = (r"$\Omega$ Matrix: Cross-Influence Between Decision Dimensions" + "\n" +
                  r"$\omega_{ij}$ shows how influence in dimension $i$ affects state in dimension $j$")
        fig1.text(0.5, 0.01, caption, ha='center', fontsize=stats_fontsize,
                 bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.5',
                           edgecolor='#cccccc', linewidth=0.5))

        # Save the heatmap
        heatmap_path = os.path.join(output_dir, f'omega_posterior_mean_heatmap.{output_format}')
        fig1.tight_layout(rect=[0, 0.05, 1, 0.95])
        fig1.savefig(heatmap_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Heatmap saved to {heatmap_path}")

        if show_plot:
            plt.show()

        plt.close(fig1)

        # --- PLOT 2: Distribution of all off-diagonal elements (handles asymmetric case) ---
        # Define all off-diagonal elements to plot in row-major order
        element_indices = [(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]  # All off-diagonal: (O,S), (O,R), (S,O), (S,R), (R,O), (R,S)
        element_labels = [('O', 'S'), ('O', 'R'), ('S', 'O'), ('S', 'R'), ('R', 'O'), ('R', 'S')]

        # Create a 2x3 subplot arrangement
        fig2 = plt.figure(figsize=(15, 10))
        gs = GridSpec(2, 3, figure=fig2)
        axes = [fig2.add_subplot(gs[i//3, i%3]) for i in range(len(element_indices))]

        # Define a consistent color palette for the plots
        colors = ['#4878d0', '#ee854a', '#6acc64', '#d65f5f', '#956cb4', '#8c613c']

        for i, ((row, col), ax) in enumerate(zip(element_indices, axes)):
            # Extract samples for this element
            element_samples = omega_samples[:, row, col]

            # Create distribution plot
            sns.histplot(element_samples, kde=True, ax=ax, color=colors[i], alpha=0.6, bins=25,
                         edgecolor='white', linewidth=0.8)

            # Calculate statistics
            mean_val = np.mean(element_samples)
            median_val = np.median(element_samples)
            std_val = np.std(element_samples)
            credible_05 = np.percentile(element_samples, 5)
            credible_95 = np.percentile(element_samples, 95)

            # Add reference lines
            ax.axvline(mean_val, color=colors[i], linestyle='--', linewidth=1.2,
                      label=f'Mean: {mean_val:.2f}')
            ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=1.2,
                      label=f'Median: {median_val:.2f}')

            # Add 90% credible interval with shaded area
            ax.axvspan(credible_05, credible_95, alpha=0.15, color=colors[i])
            ax.axvline(credible_05, color=colors[i], linestyle=':', linewidth=1.0)
            ax.axvline(credible_95, color=colors[i], linestyle=':', linewidth=1.0,
                      label=f'90% CI')

            # Add summary statistics as text with improved box
            stats_text = (
                f"Mean: {mean_val:.3f}\n"
                f"Median: {median_val:.3f}\n"
                f"Std Dev: {std_val:.3f}\n"
                f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
            )
            ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize,
                    verticalalignment='top', horizontalalignment='right',
                    bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.9,
                              edgecolor='#cccccc', linewidth=0.5))

            # Formatting with LaTeX notation
            ax.set_xlabel(rf'$\omega({element_labels[i][0]},{element_labels[i][1]})$', fontsize=label_fontsize)
            ax.set_ylabel('Density', fontsize=label_fontsize)
            ax.set_title(rf'Posterior of $\omega({element_labels[i][0]},{element_labels[i][1]})$', fontsize=label_fontsize)

            # Improve legend appearance
            ax.legend(fontsize=legend_fontsize, loc='upper left', framealpha=0.9,
                     edgecolor='#cccccc', fancybox=True)

            # Set tick parameters
            ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

            # Set x-axis limits to valid correlation range
            ax.set_xlim(-1.1, 1.1)

        # Add a figure title
        fig2.suptitle('Posterior Distributions of Off-Diagonal Omega Matrix Elements',
                     fontsize=title_fontsize, y=0.98)

        # Save the distributions plot
        dist_path = os.path.join(output_dir, f'omega_posterior_distributions.{output_format}')
        fig2.tight_layout(rect=[0, 0, 1, 0.95])  # Make room for the suptitle
        fig2.savefig(dist_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Distributions plot saved to {dist_path}")

        if show_plot:
            plt.show()

        plt.close(fig2)

        # --- PLOT 3: Compare to target matrix if provided ---
        target_matrix = np.array([
            [1.0, 0.2, -0.2],  # Influence -> State O
            [0.2, 1.0, 0.1],   # Influence -> State S
            [-0.2, 0.1, 1.0]   # Influence -> State R
        ])

        fig3, ax3 = plt.subplots(figsize=(9, 6))

        # Create a plot to compare posterior mean with target
        # Define labels with LaTeX
        labels = [r'$\omega(O,O)$', r'$\omega(O,S)$', r'$\omega(O,R)$',
                  r'$\omega(S,O)$', r'$\omega(S,S)$', r'$\omega(S,R)$',
                  r'$\omega(R,O)$', r'$\omega(R,S)$', r'$\omega(R,R)$']
        x = np.arange(len(labels))
        width = 0.35

        # Flatten matrices for bar chart
        posterior_mean_flat = omega_mean.flatten()
        target_flat = target_matrix.flatten()

        # Create bars with improved colors
        ax3.bar(x - width/2, posterior_mean_flat, width, label='Posterior Mean',
                color='#4878d0', alpha=0.8, edgecolor='white', linewidth=0.5)
        ax3.bar(x + width/2, target_flat, width, label='Target Matrix',
                color='#6acc64', alpha=0.8, edgecolor='white', linewidth=0.5)

        # Add labels and formatting
        ax3.set_ylabel('Correlation Value', fontsize=label_fontsize)
        ax3.set_title('Comparison of Posterior Mean and Target Omega Matrix', fontsize=title_fontsize)
        ax3.set_xticks(x)
        ax3.set_xticklabels(labels, rotation=45, ha='right', fontsize=tick_fontsize)

        # Improve legend
        ax3.legend(fontsize=legend_fontsize, loc='best', framealpha=0.9,
                  edgecolor='#cccccc', fancybox=True)

        ax3.set_ylim(-1.1, 1.1)
        ax3.axhline(0, color='black', linestyle='-', alpha=0.2)

        # Add grid lines
        ax3.grid(True, linestyle=':', alpha=0.3)

        # Set tick parameters
        ax3.tick_params(axis='both', which='major', labelsize=tick_fontsize)

        # Add explanation caption
        caption = ("Comparison of posterior mean values to target correlation matrix.\n"
                  "Diagonal elements (1.0) represent self-correlation, off-diagonal elements represent cross-correlation.")
        fig3.text(0.5, 0.01, caption, ha='center', fontsize=stats_fontsize,
                 bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.4',
                          edgecolor='#cccccc', linewidth=0.5))

        # Save the plot
        compare_path = os.path.join(output_dir, f'omega_posterior_vs_target.{output_format}')
        fig3.tight_layout(rect=[0, 0.05, 1, 0.95])
        fig3.savefig(compare_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Comparison plot saved to {compare_path}")

        if show_plot:
            plt.show()

        plt.close(fig3)

        return omega_samples, omega_mean

    except Exception as e:
        print(f"Error plotting omega matrix: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Adjust these paths to your saved posterior samples
    samples_file = "/content/mcmc_inference_ar1_latent_gl_results/posterior_samples_ar1_latent_gl.pkl"
    output_dir = "/content/mcmc_inference_ar1_latent_gl_results/"

    # Output format for publication quality (pdf for vector graphics)
    output_format = "pdf"  # Use "pdf" for publications or "png" for quick previews

    # Run the plotting functions with publication settings
    plot_smoothing_factor_posterior(samples_file, output_dir, output_format=output_format)
    plot_omega_posterior(samples_file, output_dir, output_format=output_format)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import matplotlib as mpl
from matplotlib.gridspec import GridSpec

def plot_sigma_posteriors(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=9,
    figsize=(15, 10),
    show_plot=False
):
    """
    Plot the posterior distributions of all sigma parameters in publication quality.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure
        font_family: Font family for plot text
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        figsize: Figure dimensions (width, height) in inches
        show_plot: Whether to display the plot
    """
    print(f"Loading posterior samples from {samples_file} for sigma parameters...")

    try:
        # Configure plotting style
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Define sigma parameter groups
        sigma_groups = {
            'epsilon': ['sigma_epsilon_O', 'sigma_epsilon_S', 'sigma_epsilon_R'],
            'individual': ['sigma_ind_O', 'sigma_ind_S', 'sigma_ind_R']
        }

        # Create figure with 3 rows (one for each sigma group)
        fig, axes = plt.subplots(2, 3, figsize=figsize)

        # Define colors for each dimension (O, S, R)
        colors = ['#4878d0', '#ee854a', '#6acc64']

        # Iterate through sigma groups
        for row_idx, (group_name, param_names) in enumerate(sigma_groups.items()):
            for col_idx, param_name in enumerate(param_names):
                ax = axes[row_idx, col_idx]

                if param_name in posterior_samples:
                    samples = posterior_samples[param_name]
                    print(f"Found {len(samples)} samples for {param_name}")

                    # Create histogram with KDE
                    sns.histplot(samples, kde=True, color=colors[col_idx], alpha=0.6,
                                ax=ax, bins=25, edgecolor='white', linewidth=0.8)

                    # Calculate summary statistics
                    mean_val = np.mean(samples)
                    median_val = np.median(samples)
                    std_val = np.std(samples)
                    credible_05 = np.percentile(samples, 5)
                    credible_95 = np.percentile(samples, 95)

                    # Add reference lines
                    ax.axvline(mean_val, color=colors[col_idx], linestyle='--', linewidth=1.2,
                              label=f'Mean: {mean_val:.2f}')
                    ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=1.2,
                              label=f'Median: {median_val:.2f}')

                    # Add 90% credible interval
                    ax.axvspan(credible_05, credible_95, alpha=0.15, color=colors[col_idx])
                    ax.axvline(credible_05, color=colors[col_idx], linestyle=':', linewidth=1.0)
                    ax.axvline(credible_95, color=colors[col_idx], linestyle=':', linewidth=1.0,
                              label=f'90% CI')

                    # Add statistics textbox
                    stats_text = (
                        f"Mean: {mean_val:.3f}\n"
                        f"Median: {median_val:.3f}\n"
                        f"Std Dev: {std_val:.3f}\n"
                        f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
                    )
                    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize,
                            verticalalignment='top', horizontalalignment='right',
                            bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.9,
                                     edgecolor='#cccccc', linewidth=0.5))

                    # Set axis labels and title
                    dimension = param_name.split('_')[-1]
                    group_label = group_name
                    if group_name == 'epsilon':
                        group_label = r'\varepsilon'
                    elif group_name == 'individual':
                        group_label = 'ind'

                    ax.set_xlabel(rf'$\sigma_{{{group_label}}}^{{{dimension}}}$', fontsize=label_fontsize)
                    ax.set_ylabel('Density', fontsize=label_fontsize)
                    ax.set_title(f'{param_name}', fontsize=label_fontsize)

                    # Set tick parameters
                    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

                    # Add legend
                    ax.legend(fontsize=legend_fontsize, loc='upper right', framealpha=0.9,
                             edgecolor='#cccccc', fancybox=True)
                else:
                    print(f"Warning: {param_name} not found in posterior samples")
                    ax.text(0.5, 0.5, f"{param_name} not found in samples",
                           horizontalalignment='center', verticalalignment='center')

        # Add super title
        fig.suptitle('Posterior Distributions of Sigma Parameters', fontsize=title_fontsize, y=0.98)

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Save the plot
        plot_path = os.path.join(output_dir, f'sigma_parameters_posterior.{output_format}')
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

    except Exception as e:
        print(f"Error plotting sigma parameters: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_phi_posteriors(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=9,
    figsize=(12, 5),
    show_plot=False
):
    """
    Plot the posterior distributions of phi parameters in publication quality.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure
        font_family: Font family for plot text
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        figsize: Figure dimensions (width, height) in inches
        show_plot: Whether to display the plot
    """
    print(f"Loading posterior samples from {samples_file} for phi parameters...")

    try:
        # Configure plotting style
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Define phi parameters
        phi_params = ['phi_O', 'phi_S', 'phi_R']

        # Create figure with 1 row, 3 columns
        fig, axes = plt.subplots(1, 3, figsize=figsize)

        # Define colors for each dimension (O, S, R)
        colors = ['#4878d0', '#ee854a', '#6acc64']

        for idx, param_name in enumerate(phi_params):
            ax = axes[idx]

            if param_name in posterior_samples:
                samples = posterior_samples[param_name]
                print(f"Found {len(samples)} samples for {param_name}")

                # Create histogram with KDE
                sns.histplot(samples, kde=True, color=colors[idx], alpha=0.6,
                            ax=ax, bins=25, edgecolor='white', linewidth=0.8)

                # Calculate summary statistics
                mean_val = np.mean(samples)
                median_val = np.median(samples)
                std_val = np.std(samples)
                credible_05 = np.percentile(samples, 5)
                credible_95 = np.percentile(samples, 95)

                # Add reference lines
                ax.axvline(mean_val, color=colors[idx], linestyle='--', linewidth=1.2,
                          label=f'Mean: {mean_val:.2f}')
                ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=1.2,
                          label=f'Median: {median_val:.2f}')

                # Add 90% credible interval
                ax.axvspan(credible_05, credible_95, alpha=0.15, color=colors[idx])
                ax.axvline(credible_05, color=colors[idx], linestyle=':', linewidth=1.0)
                ax.axvline(credible_95, color=colors[idx], linestyle=':', linewidth=1.0,
                          label=f'90% CI')

                # Add statistics textbox
                stats_text = (
                    f"Mean: {mean_val:.3f}\n"
                    f"Median: {median_val:.3f}\n"
                    f"Std Dev: {std_val:.3f}\n"
                    f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
                )
                ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize,
                        verticalalignment='top', horizontalalignment='right',
                        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.9,
                                 edgecolor='#cccccc', linewidth=0.5))

                # Set axis labels and title
                dimension = param_name.split('_')[-1]
                ax.set_xlabel(rf'$\phi^{{{dimension}}}$', fontsize=label_fontsize)
                ax.set_ylabel('Density', fontsize=label_fontsize)
                ax.set_title(f'AR(1) Persistence for Dimension {dimension}', fontsize=label_fontsize)

                # Set tick parameters
                ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

                # Add legend
                ax.legend(fontsize=legend_fontsize, loc='upper left', framealpha=0.9,
                         edgecolor='#cccccc', fancybox=True)
            else:
                print(f"Warning: {param_name} not found in posterior samples")
                ax.text(0.5, 0.5, f"{param_name} not found in samples",
                       horizontalalignment='center', verticalalignment='center')

        # Add super title
        fig.suptitle('Posterior Distributions of Phi Parameters (AR(1) Persistence)', fontsize=title_fontsize, y=1.05)

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Save the plot
        plot_path = os.path.join(output_dir, f'phi_parameters_posterior.{output_format}')
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

    except Exception as e:
        print(f"Error plotting phi parameters: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_beta_posteriors(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=9,
    figsize=(15, 12),
    show_plot=False
):
    """
    Plot the posterior distributions of beta parameters in publication quality.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure
        font_family: Font family for plot text
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        figsize: Figure dimensions (width, height) in inches
        show_plot: Whether to display the plot
    """
    print(f"Loading posterior samples from {samples_file} for beta parameters...")

    try:
        # Configure plotting style
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Define beta parameter groups
        beta_groups = {
            'H': ['beta_H_O', 'beta_H_S', 'beta_H_R'],
            'D': ['beta_D_O', 'beta_D_S', 'beta_D_R']
        }

        # Define dimensions
        dimensions = ['O', 'S', 'R']

        # Define colors for each dimension (O, S, R)
        colors = ['#4878d0', '#ee854a', '#6acc64']

        # First figure: Heatmap of posterior means
        fig1, axes1 = plt.subplots(1, 2, figsize=(12, 6))

        # Second figure: Detailed distributions of each beta parameter component
        # We'll need to determine how many components each beta has
        sample_beta_H = posterior_samples.get(beta_groups['H'][0])
        sample_beta_D = posterior_samples.get(beta_groups['D'][0])

        if sample_beta_H is None or sample_beta_D is None:
            print("Warning: Beta parameters not found in samples")
            return

        dim_H = sample_beta_H.shape[1] if len(sample_beta_H.shape) > 1 else 1
        dim_D = sample_beta_D.shape[1] if len(sample_beta_D.shape) > 1 else 1

        # PLOT 1: Heatmaps of posterior means for beta_H and beta_D
        for group_idx, (group_name, param_names) in enumerate(beta_groups.items()):
            ax = axes1[group_idx]

            # Create mean matrix for this beta group
            if group_name == 'H':
                mean_matrix = np.zeros((dim_H, 3))  # 3 dimensions (O, S, R)
                for dim_idx, param_name in enumerate(param_names):
                    if param_name in posterior_samples:
                        samples = posterior_samples[param_name]
                        if len(samples.shape) > 1:
                            mean_vector = np.mean(samples, axis=0)
                            mean_matrix[:, dim_idx] = mean_vector
                        else:
                            mean_matrix[0, dim_idx] = np.mean(samples)

                # Create heatmap
                sns.heatmap(mean_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f',
                           ax=ax, cbar_kws={"shrink": 0.8, "label": "Coefficient Value"})

                # Set labels
                ax.set_xlabel('Decision Dimension', fontsize=label_fontsize)
                ax.set_ylabel('Household Attribute Index', fontsize=label_fontsize)
                ax.set_title(rf'Posterior Mean of $\beta_H$ Coefficients', fontsize=label_fontsize)
                ax.set_xticklabels(dimensions, fontsize=tick_fontsize)
                ax.set_yticklabels(range(dim_H), fontsize=tick_fontsize)

            elif group_name == 'D':
                mean_matrix = np.zeros((dim_D, 3))  # 3 dimensions (O, S, R)
                for dim_idx, param_name in enumerate(param_names):
                    if param_name in posterior_samples:
                        samples = posterior_samples[param_name]
                        if len(samples.shape) > 1:
                            mean_vector = np.mean(samples, axis=0)
                            mean_matrix[:, dim_idx] = mean_vector
                        else:
                            mean_matrix[0, dim_idx] = np.mean(samples)

                # Create heatmap
                sns.heatmap(mean_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f',
                           ax=ax, cbar_kws={"shrink": 0.8, "label": "Coefficient Value"})

                # Set labels
                ax.set_xlabel('Decision Dimension', fontsize=label_fontsize)
                ax.set_ylabel('Damage Attribute Index', fontsize=label_fontsize)
                ax.set_title(rf'Posterior Mean of $\beta_D$ Coefficients', fontsize=label_fontsize)
                ax.set_xticklabels(dimensions, fontsize=tick_fontsize)
                ax.set_yticklabels(range(dim_D), fontsize=tick_fontsize)

        fig1.suptitle('Posterior Means of Beta Coefficients', fontsize=title_fontsize, y=0.98)

        # Save the heatmap figure
        heatmap_path = os.path.join(output_dir, f'beta_parameters_heatmap.{output_format}')
        fig1.tight_layout(rect=[0, 0, 1, 0.95])
        fig1.savefig(heatmap_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Heatmap saved to {heatmap_path}")

        # PLOT 2: Detailed distribution plots for each beta coefficient
        # Create subplots grid for beta_H
        fig2 = plt.figure(figsize=figsize)
        gs = GridSpec(dim_H, 3, figure=fig2)  # dim_H rows, 3 columns (O, S, R)

        # Plot each beta_H coefficient
        for i in range(dim_H):
            for j, dim in enumerate(dimensions):
                param_name = f'beta_H_{dim}'
                ax = fig2.add_subplot(gs[i, j])

                if param_name in posterior_samples:
                    samples = posterior_samples[param_name]
                    if len(samples.shape) > 1:
                        coef_samples = samples[:, i]
                    else:
                        coef_samples = samples

                    # Create histogram with KDE
                    sns.histplot(coef_samples, kde=True, color=colors[j], alpha=0.6,
                                ax=ax, bins=25, edgecolor='white', linewidth=0.8)

                    # Calculate summary statistics
                    mean_val = np.mean(coef_samples)
                    median_val = np.median(coef_samples)
                    std_val = np.std(coef_samples)
                    credible_05 = np.percentile(coef_samples, 5)
                    credible_95 = np.percentile(coef_samples, 95)

                    # Add reference lines
                    ax.axvline(mean_val, color=colors[j], linestyle='--', linewidth=1.2,
                              label=f'Mean: {mean_val:.2f}')
                    ax.axvline(0, color='black', linestyle='-', alpha=0.5,
                              label='Zero')

                    # Add 90% credible interval
                    ax.axvspan(credible_05, credible_95, alpha=0.15, color=colors[j])
                    ax.axvline(credible_05, color=colors[j], linestyle=':', linewidth=1.0)
                    ax.axvline(credible_95, color=colors[j], linestyle=':', linewidth=1.0)

                    # Add compact statistics textbox
                    stats_text = (
                        f"Mean: {mean_val:.3f}\n"
                        f"95% CI: [{credible_05:.3f}, {credible_95:.3f}]"
                    )
                    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize-1,
                            verticalalignment='top', horizontalalignment='right',
                            bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9,
                                     edgecolor='#cccccc', linewidth=0.5))

                    # Set axis labels and title
                    ax.set_xlabel(rf'$\beta_H^{{{dim}}}[{i}]$', fontsize=label_fontsize-1)
                    if j == 0:  # Only add y-label for the first column
                        ax.set_ylabel('Density', fontsize=label_fontsize-1)

                    # Set title only for the top row
                    if i == 0:
                        ax.set_title(f'Dimension {dim}', fontsize=label_fontsize)

                    # Set tick parameters
                    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize-1)

                    # Add legend only for the first subplot
                    if i == 0 and j == 0:
                        ax.legend(fontsize=legend_fontsize-1, loc='upper left', framealpha=0.9,
                                 edgecolor='#cccccc', fancybox=True)

        fig2.suptitle(r'Posterior Distributions of $\beta_H$ Coefficients', fontsize=title_fontsize, y=0.98)

        # Save the beta_H distributions plot
        beta_H_path = os.path.join(output_dir, f'beta_H_parameters_posterior.{output_format}')
        fig2.tight_layout(rect=[0, 0, 1, 0.96])
        fig2.savefig(beta_H_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Beta_H distributions plot saved to {beta_H_path}")

        # PLOT 3: Detailed distribution plots for each beta_D coefficient
        # Create subplots grid for beta_D
        fig3 = plt.figure(figsize=figsize)
        gs = GridSpec(dim_D, 3, figure=fig3)  # dim_D rows, 3 columns (O, S, R)

        # Plot each beta_D coefficient
        for i in range(dim_D):
            for j, dim in enumerate(dimensions):
                param_name = f'beta_D_{dim}'
                ax = fig3.add_subplot(gs[i, j])

                if param_name in posterior_samples:
                    samples = posterior_samples[param_name]
                    if len(samples.shape) > 1:
                        coef_samples = samples[:, i]
                    else:
                        coef_samples = samples

                    # Create histogram with KDE
                    sns.histplot(coef_samples, kde=True, color=colors[j], alpha=0.6,
                                ax=ax, bins=25, edgecolor='white', linewidth=0.8)

                    # Calculate summary statistics
                    mean_val = np.mean(coef_samples)
                    median_val = np.median(coef_samples)
                    std_val = np.std(coef_samples)
                    credible_05 = np.percentile(coef_samples, 5)
                    credible_95 = np.percentile(coef_samples, 95)

                    # Add reference lines
                    ax.axvline(mean_val, color=colors[j], linestyle='--', linewidth=1.2,
                              label=f'Mean: {mean_val:.2f}')
                    ax.axvline(0, color='black', linestyle='-', alpha=0.5,
                              label='Zero')

                    # Add 90% credible interval
                    ax.axvspan(credible_05, credible_95, alpha=0.15, color=colors[j])
                    ax.axvline(credible_05, color=colors[j], linestyle=':', linewidth=1.0)
                    ax.axvline(credible_95, color=colors[j], linestyle=':', linewidth=1.0)

                    # Add compact statistics textbox
                    stats_text = (
                        f"Mean: {mean_val:.3f}\n"
                        f"95% CI: [{credible_05:.3f}, {credible_95:.3f}]"
                    )
                    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize-1,
                            verticalalignment='top', horizontalalignment='right',
                            bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9,
                                     edgecolor='#cccccc', linewidth=0.5))

                    # Set axis labels and title
                    ax.set_xlabel(rf'$\beta_D^{{{dim}}}[{i}]$', fontsize=label_fontsize-1)
                    if j == 0:  # Only add y-label for the first column
                        ax.set_ylabel('Density', fontsize=label_fontsize-1)

                    # Set title only for the top row
                    if i == 0:
                        ax.set_title(f'Dimension {dim}', fontsize=label_fontsize)

                    # Set tick parameters
                    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize-1)

                    # Add legend only for the first subplot
                    if i == 0 and j == 0:
                        ax.legend(fontsize=legend_fontsize-1, loc='upper left', framealpha=0.9,
                                 edgecolor='#cccccc', fancybox=True)

        fig3.suptitle(r'Posterior Distributions of $\beta_D$ Coefficients', fontsize=title_fontsize, y=0.98)

        # Save the beta_D distributions plot
        beta_D_path = os.path.join(output_dir, f'beta_D_parameters_posterior.{output_format}')
        fig3.tight_layout(rect=[0, 0, 1, 0.96])
        fig3.savefig(beta_D_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Beta_D distributions plot saved to {beta_D_path}")

        if show_plot:
            plt.show()

        plt.close(fig1)
        plt.close(fig2)
        plt.close(fig3)

    except Exception as e:
        print(f"Error plotting beta parameters: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_mubar_posteriors(
    samples_file,
    output_dir='.',
    output_format='png',
    fig_dpi=600,
    font_family='serif',
    title_fontsize=14,
    label_fontsize=12,
    tick_fontsize=10,
    legend_fontsize=10,
    stats_fontsize=9,
    figsize=(15, 10),
    show_plot=False
):
    """
    Plot the posterior distributions of long-term means (mu_bar) in publication quality.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (e.g., 'png', 'pdf', 'svg', 'eps')
        fig_dpi: Resolution for the saved figure
        font_family: Font family for plot text
        title_fontsize: Font size for the main figure title
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for axis tick labels
        legend_fontsize: Font size for the legend
        stats_fontsize: Font size for the statistics text box
        figsize: Figure dimensions (width, height) in inches
        show_plot: Whether to display the plot
    """
    print(f"Loading posterior samples from {samples_file} for long-term mean parameters...")

    try:
        # Configure plotting style
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.family'] = font_family
        plt.rcParams['axes.labelweight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.grid'] = True
        plt.rcParams['grid.alpha'] = 0.3
        plt.rcParams['grid.linestyle'] = ':'

        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Check for mu_bar parameters
        mu_bar_params = ['mu_bar_O_g', 'mu_bar_S_g', 'mu_bar_R_g']

        # Check if mu_bar parameters exist in the posterior samples
        if not any(param in posterior_samples for param in mu_bar_params):
            print("Warning: No mu_bar parameters found in posterior samples.")
            return

        # Determine number of groups from the first available mu_bar parameter
        for param in mu_bar_params:
            if param in posterior_samples:
                n_groups = posterior_samples[param].shape[1]
                break

        # Create figure with 1 row, 3 columns
        fig, axes = plt.subplots(1, 3, figsize=figsize)

        # Define colors for each dimension (O, S, R)
        colors = ['#4878d0', '#ee854a', '#6acc64']

        for idx, param_name in enumerate(mu_bar_params):
            ax = axes[idx]
            dim = param_name.split('_')[2]  # Extract dimension (O, S, R)

            if param_name in posterior_samples:
                samples = posterior_samples[param_name]
                print(f"Found samples for {param_name} with shape {samples.shape}")

                # Create a list to store summary statistics for each group
                group_stats = []

                # Plot distribution for each group
                for g in range(n_groups):
                    group_samples = samples[:, g]

                    # Calculate summary statistics
                    mean_val = np.mean(group_samples)
                    median_val = np.median(group_samples)
                    std_val = np.std(group_samples)
                    credible_05 = np.percentile(group_samples, 5)
                    credible_95 = np.percentile(group_samples, 95)

                    # Use a lighter color for individual group distributions
                    group_color = colors[idx]
                    alpha = 0.2 + 0.6 * (g / max(1, n_groups - 1))  # Vary alpha for visual distinction

                    # Create density plot
                    sns.kdeplot(group_samples, ax=ax, color=group_color, alpha=alpha,
                               label=f'Group {g}' if g < 5 else None,  # Only label first 5 for clarity
                               linewidth=1.5)

                    # Store summary statistics for this group
                    group_stats.append({
                        'group': g,
                        'mean': mean_val,
                        'median': median_val,
                        'std': std_val,
                        'ci_05': credible_05,
                        'ci_95': credible_95
                    })

                # Create a summary statistics text box
                if n_groups <= 5:  # If there are few groups, show all statistics
                    stats_text = "\n".join([
                        f"Group {g}: μ={stats['mean']:.2f}, 90% CI=[{stats['ci_05']:.2f}, {stats['ci_95']:.2f}]"
                        for g, stats in enumerate(group_stats)
                    ])
                else:  # Otherwise, just show a summary
                    all_means = [stats['mean'] for stats in group_stats]
                    min_mean = min(all_means)
                    max_mean = max(all_means)
                    stats_text = f"Groups: {n_groups}\nMean Range: [{min_mean:.2f}, {max_mean:.2f}]"

                ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=stats_fontsize,
                        verticalalignment='top', horizontalalignment='right',
                        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.9,
                                 edgecolor='#cccccc', linewidth=0.5))

                # Set axis labels and title
                ax.set_xlabel(rf'$\bar{{\mu}}^{{{dim}}}_g$', fontsize=label_fontsize)
                ax.set_ylabel('Density', fontsize=label_fontsize)
                ax.set_title(f'Long-term Mean for Dimension {dim}', fontsize=label_fontsize)

                # Set tick parameters
                ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

                # Add legend if there are few groups
                if n_groups <= 5:
                    ax.legend(fontsize=legend_fontsize, loc='upper left', framealpha=0.9,
                             edgecolor='#cccccc', fancybox=True)
            else:
                print(f"Warning: {param_name} not found in posterior samples")
                ax.text(0.5, 0.5, f"{param_name} not found in samples",
                       horizontalalignment='center', verticalalignment='center')

        # Add super title
        fig.suptitle('Posterior Distributions of Long-term Means', fontsize=title_fontsize, y=1.05)

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Save the plot
        plot_path = os.path.join(output_dir, f'mubar_parameters_posterior.{output_format}')
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"Plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

    except Exception as e:
        print(f"Error plotting mu_bar parameters: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_all_posteriors(
    samples_file,
    output_dir='.',
    output_format='pdf',
    show_plot=True):
    """
    Run all posterior plotting functions.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plots
        output_format: Format to save the figures (e.g., 'png', 'pdf', 'svg', 'eps')
        show_plot: Whether to display the plots
    """
    print(f"Plotting all posterior distributions from {samples_file}...")

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Run all plotting functions
    plot_sigma_posteriors(samples_file, output_dir, output_format=output_format, show_plot=show_plot)
    plot_phi_posteriors(samples_file, output_dir, output_format=output_format, show_plot=show_plot)
    plot_beta_posteriors(samples_file, output_dir, output_format=output_format, show_plot=show_plot)
    plot_mubar_posteriors(samples_file, output_dir, output_format=output_format, show_plot=show_plot)

    print("All posterior plotting completed successfully.")

if __name__ == "__main__":
    # Adjust these paths to your saved posterior samples
    samples_file = "/content/mcmc_inference_ar1_latent_gl_results/posterior_samples_ar1_latent_gl.pkl"
    output_dir = "/content/mcmc_inference_ar1_latent_gl_results/"

    # Output format for publication quality (pdf for vector graphics)
    output_format = "pdf"

    # Run all plotting functions
    plot_all_posteriors(samples_file, output_dir, output_format=output_format)
