# -*- coding: utf-8 -*-
"""CLIMA_WP2_Network_Dynamics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D9QgaDQlau38KTG7KJjlTckMbPewZy2S

## Libraries and Packages
"""

!pip install python-geohash
!pip install pygeohash
#!pip install pyro-ppl
!pip install numpyro

import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from google.colab  import drive
from datetime import datetime, timedelta
import folium
from folium.plugins import MarkerCluster
from folium.plugins import HeatMap
import joblib
import math, bisect, random
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
import pdb
from tqdm import tqdm
from itertools import combinations
import geohash
import networkx as nx
import scipy.stats as stats
import scipy.signal as signal
from scipy.stats import beta, norm
import matplotlib.pyplot as plt
import torch
import pickle
import json
import jax
import jax.numpy as jnp
from jax.experimental import sparse
from jax import random, lax
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, Predictive
from scipy import sparse
import os
import time
import warnings
import seaborn as sns
import os
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.cm as cm

"""##Synthesized Data Generation"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import os
from matplotlib.colors import LinearSegmentedColormap
from scipy.stats import norm
from sklearn.cluster import KMeans # Import KMeans here

# --- Data Preparation Functions ---

def assign_households_to_groups(households_df, num_groups=5, random_seed=None):
    """
    Assign households to Census Block Groups (CBGs) based on geographical proximity using K-means.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with lat/lon coordinates.
    num_groups : int, default=5
        Number of groups to create.
    random_seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added 'group_id' column.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.
    """
    print(f"Assigning households to {num_groups} groups...")

    # Set random seed if provided for KMeans reproducibility
    if random_seed is not None:
        kmeans_random_state = random_seed
    else:
        kmeans_random_state = None # Allow KMeans to use its default random state

    # Extract coordinates
    coords = households_df[['lat', 'lon']].values

    # Use K-means clustering
    # Note: n_init='auto' is recommended in newer scikit-learn versions
    kmeans = KMeans(n_clusters=num_groups, random_state=kmeans_random_state, n_init='auto')
    group_assignments = kmeans.fit_predict(coords)

    # Get cluster centroids
    centroids = kmeans.cluster_centers_

    # Add group_id to dataframe
    households_df['group_id'] = group_assignments

    # Create a dictionary mapping group_id to centroid
    group_centroids = {i: (centroids[i, 0], centroids[i, 1]) for i in range(num_groups)}

    # Print summary
    print(f"Group assignments complete. Households per group:")
    group_counts = households_df['group_id'].value_counts().sort_index()
    for group_id, count in group_counts.items():
        print(f"  Group {group_id}: {count} households")

    return households_df, group_centroids

def generate_group_attributes(households_df, group_centroids):
    """
    Generate group-level attributes by aggregating household characteristics.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with group-level attributes (means, counts, feature vectors).
    """
    print("Generating group-level attributes...")

    # Get unique group IDs
    group_ids = sorted(households_df['group_id'].unique()) # Sort for consistent order

    # Initialize lists for group attributes
    group_data = []

    # Generate group attributes by aggregating household attributes
    for g_id in group_ids:
        # Get households in this group
        group_households = households_df[households_df['group_id'] == g_id]

        # Calculate group-level attributes (aggregations)
        avg_income = group_households['income'].mean()
        avg_resource_access = group_households['resource_access'].mean()
        avg_damage_structural = group_households['damage_structural'].mean()
        avg_damage_utility = group_households['damage_utility'].mean()

        # Get centroid
        centroid_lat, centroid_lon = group_centroids[g_id]

        # Create group feature vectors (needed for the hierarchical threshold model)
        H_g = np.array([avg_income, avg_resource_access])  # Socioeconomic features
        D_g = np.array([avg_damage_structural, avg_damage_utility])  # Damage features

        # Store group data
        group_data.append({
            'group_id': g_id,
            'centroid_lat': centroid_lat,
            'centroid_lon': centroid_lon,
            'avg_income': avg_income,
            'avg_resource_access': avg_resource_access,
            'avg_damage_structural': avg_damage_structural,
            'avg_damage_utility': avg_damage_utility,
            'household_count': len(group_households),
            'H_g': H_g, # Store the numpy array directly
            'D_g': D_g  # Store the numpy array directly
        })

    # Create DataFrame
    group_df = pd.DataFrame(group_data).set_index('group_id') # Use group_id as index

    return group_df

def generate_household_attributes(household_ids, household_locs, seed=None):
    """
    Generate binary household attributes for simulation using specified probabilities.

    Parameters:
    -----------
    household_ids : list
        List of household IDs.
    household_locs : dict
        Dictionary mapping household ID to (lat, lon) location.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with household attributes (ID, lat, lon, income, etc.).
    """
    print("Generating binary household attributes...")

    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)

    n_households = len(household_ids)

    # Define probabilities for each binary attribute
    p_income = 0.2            # 20% chance of high income (1)
    p_resource_access = 0.30  # 30% chance of good resource access (1)
    p_damage_structural = 0.15# 15% chance of structural damage (1)
    p_damage_utility = 0.50   # 50% chance of utility damage (1)

    # Generate binary attributes directly using numpy's random choice
    incomes = np.random.choice([0, 1], size=n_households, p=[1 - p_income, p_income])
    resource_access = np.random.choice([0, 1], size=n_households, p=[1 - p_resource_access, p_resource_access])
    damage_structural = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_structural, p_damage_structural])
    damage_utility = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_utility, p_damage_utility])

    # Extract location data efficiently
    lats = np.array([household_locs[h_id][0] for h_id in household_ids])
    lons = np.array([household_locs[h_id][1] for h_id in household_ids])

    # Create DataFrame
    households_df = pd.DataFrame({
        'household_id': household_ids,
        'lat': lats,
        'lon': lons,
        'income': incomes,                  # 1=high, 0=low
        'resource_access': resource_access, # 1=good, 0=poor
        'damage_structural': damage_structural, # 1=damaged, 0=not damaged
        'damage_utility': damage_utility,   # 1=damaged, 0=not damaged
    })

    return households_df

def initialize_decision_states(households_df, damage_threshold, seed=None):
    """
    Initialize household decision states Y = [Y^O, Y^S, Y^R] and history/transition tracking.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data.
    damage_threshold : float
        Threshold for determining initial recovery state based on average damage.
    seed : int, optional
        Random seed for reproducibility of initial vacant/for-sale selection.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added decision state columns (Y_O, Y_S, Y_R) and tracking columns.
    """
    print("Initializing decision states...")
    n_households = len(households_df)

    # Set random seed if provided for initial state randomization
    if seed is not None:
        np.random.seed(seed)

    # Initialize all states to 0
    occupancy_state = np.zeros(n_households, dtype=np.int8) # Y_O: 0=occupied, 1=vacant
    sales_state = np.zeros(n_households, dtype=np.int8)     # Y_S: 0=not for sale, 1=for sale

    # Randomly select a small number of initial vacant/for-sale properties
    num_vacant = np.random.randint(10, 25) # e.g., 1 or 2, or up to 2%
    num_for_sale = np.random.randint(15, 20)# e.g., 1, or up to 1%

    if n_households > 0: # Avoid errors with empty dataframes
        vacant_indices = np.random.choice(n_households, num_vacant, replace=False)
        # Ensure for_sale indices don't overlap with vacant ones initially if desired (optional)
        available_for_sale_indices = np.setdiff1d(np.arange(n_households), vacant_indices)
        if len(available_for_sale_indices) >= num_for_sale:
             for_sale_indices = np.random.choice(available_for_sale_indices, num_for_sale, replace=False)
        else: # Fallback if too few non-vacant houses left
             for_sale_indices = np.random.choice(n_households, num_for_sale, replace=False)


        # Set selected indices to 1
        occupancy_state[vacant_indices] = 1
        sales_state[for_sale_indices] = 1

    # Initialize recovery state based on damage levels
    # Y_R: 1=repaired/undamaged, 0=needs repair
    # If average damage is *below* threshold, it's considered 'repaired' or okay initially.
    avg_damage = (households_df['damage_structural'] + households_df['damage_utility']) / 2
    recovery_state = (avg_damage < damage_threshold).astype(np.int8)

    # Add to the dataframe
    households_df['Y_O'] = occupancy_state
    households_df['Y_S'] = sales_state
    households_df['Y_R'] = recovery_state

    # Initialize historical state tracking for bonding influence (max state seen so far)
    # Starts with the current initial state.
    households_df['Y_O_hist'] = occupancy_state.copy()
    households_df['Y_S_hist'] = sales_state.copy()
    households_df['Y_R_hist'] = recovery_state.copy()

    # Initialize previous states for tracking recent transitions (state at t-1)
    # Starts with the current initial state.
    households_df['Y_O_prev'] = occupancy_state.copy()
    households_df['Y_S_prev'] = sales_state.copy()
    households_df['Y_R_prev'] = recovery_state.copy()

    # Initialize transition markers (0 = no transition in the last step, 1 = recent 0->1 transition)
    # Starts at 0 as there's no previous step.
    households_df['Y_O_transition'] = np.zeros(n_households, dtype=np.int8)
    households_df['Y_S_transition'] = np.zeros(n_households, dtype=np.int8)
    households_df['Y_R_transition'] = np.zeros(n_households, dtype=np.int8)

    # Print summary of initial states
    print(f"Initial states summary:")
    print(f"  Vacant households (Y_O=1): {np.sum(occupancy_state == 1)} / {n_households}")
    print(f"  For-sale properties (Y_S=1): {np.sum(sales_state == 1)} / {n_households}")
    print(f"  Repaired/undamaged (Y_R=1): {np.sum(recovery_state == 1)} / {n_households}")

    return households_df

# --- Network and Influence Functions ---

def create_network_matrices(households_df, network_df):
    """
    Create weighted adjacency matrices for bonding (W_B) and bridging (W_R) networks.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data (must include 'household_id').
    network_df : DataFrame
        DataFrame containing network connections ('home_1', 'home_2', 'type', 'avg_link').
        'type' should be 1 for bonding, 2 for bridging.

    Returns:
    --------
    W_B : ndarray
        Weighted adjacency matrix for bonding network (n_households x n_households).
    W_R : ndarray
        Weighted adjacency matrix for bridging network (n_households x n_households).
    id_to_idx : dict
        Dictionary mapping household ID to matrix index.
    idx_to_id : dict
        Dictionary mapping matrix index to household ID.
    """
    print("Creating network adjacency matrices...")
    # Get household IDs and create mappings
    household_ids = households_df['household_id'].values
    id_to_idx = {h_id: idx for idx, h_id in enumerate(household_ids)}
    idx_to_id = {idx: h_id for h_id, idx in id_to_idx.items()}

    n_households = len(household_ids)

    # Initialize adjacency matrices with zeros
    W_B = np.zeros((n_households, n_households))  # Bonding network
    W_R = np.zeros((n_households, n_households))  # Bridging network

    # Populate matrices from network_df
    skipped_connections = 0
    for _, row in network_df.iterrows():
        home_1 = row['home_1']
        home_2 = row['home_2']
        weight = row['avg_link']
        conn_type = row['type']

        # Get indices, skip if household not in our simulation set
        if home_1 in id_to_idx and home_2 in id_to_idx:
            i = id_to_idx[home_1]
            j = id_to_idx[home_2]

            # Add edge weight based on type (undirected)
            if conn_type == 1:  # Bonding
                W_B[i, j] = weight
                W_B[j, i] = weight
            elif conn_type == 2: # Bridging
                W_R[i, j] = weight
                W_R[j, i] = weight
            # else: ignore other types if any
        else:
            skipped_connections += 1

    if skipped_connections > 0:
         print(f"  Warning: Skipped {skipped_connections} connections involving households not in the main dataset.")
    print("Network matrices created.")
    return W_B, W_R, id_to_idx, idx_to_id

def calculate_raw_influences(households_df, W_B, W_R):
    """
    Calculate raw social influence m^s for each decision dimension s.
    - Bonding networks use historical states (Y_hist: max state seen so far).
    - Bridging networks use only recent transitions (Y_transition: 0->1 in last step).

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with current states and history.
    W_B : ndarray
        Weighted adjacency matrix for bonding network.
    W_R : ndarray
        Weighted adjacency matrix for bridging network.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added raw influence columns ('m_O', 'm_S', 'm_R').
    """
    n_households = len(households_df)

    # Get historical decision states (for bonding influence)
    Y_O_hist = households_df['Y_O_hist'].values
    Y_S_hist = households_df['Y_S_hist'].values
    Y_R_hist = households_df['Y_R_hist'].values

    # Get transition markers (for bridging influence - only recent 0->1 transitions)
    Y_O_transition = households_df['Y_O_transition'].values
    Y_S_transition = households_df['Y_S_transition'].values
    Y_R_transition = households_df['Y_R_transition'].values

    # Initialize raw influence vectors
    m_O = np.zeros(n_households)
    m_S = np.zeros(n_households)
    m_R = np.zeros(n_households)

    # Calculate combined raw influences using matrix operations for efficiency
    # Bonding influence = W_B * Y_hist (element-wise)
    bonding_influence_O = W_B @ Y_O_hist
    bonding_influence_S = W_B @ Y_S_hist
    bonding_influence_R = W_B @ Y_R_hist

    # Bridging influence = W_R * Y_transition (element-wise)
    bridging_influence_O = W_R @ Y_O_transition
    bridging_influence_S = W_R @ Y_S_transition
    bridging_influence_R = W_R @ Y_R_transition

    # Total weights for normalization (sum of weights for each household)
    total_weights_B = W_B.sum(axis=1)
    total_weights_R = W_R.sum(axis=1)
    total_weights = total_weights_B + total_weights_R

    # Combine influences and normalize
    # Avoid division by zero for isolated nodes (total_weight = 0)
    valid_weights_mask = total_weights > 0
    m_O[valid_weights_mask] = (bonding_influence_O[valid_weights_mask] + bridging_influence_O[valid_weights_mask]) / total_weights[valid_weights_mask]
    m_S[valid_weights_mask] = (bonding_influence_S[valid_weights_mask] + bridging_influence_S[valid_weights_mask]) / total_weights[valid_weights_mask]
    m_R[valid_weights_mask] = (bonding_influence_R[valid_weights_mask] + bridging_influence_R[valid_weights_mask]) / total_weights[valid_weights_mask]

    # Store raw influences in dataframe
    households_df['m_O'] = m_O
    households_df['m_S'] = m_S
    households_df['m_R'] = m_R

    return households_df


def calculate_adjusted_influences(households_df, omega_matrix):
    """
    Calculate correlation-adjusted influences I^s = m * Omega.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with raw influences ('m_O', 'm_S', 'm_R').
    omega_matrix : ndarray
        3x3 correlation matrix for cross-dimension dependencies.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added adjusted influence columns ('I_O', 'I_S', 'I_R').
    """
    # Get raw influences
    m_O = households_df['m_O'].values
    m_S = households_df['m_S'].values
    m_R = households_df['m_R'].values

    # Stack raw influences into a matrix: shape (n_households, 3)
    m = np.vstack([m_O, m_S, m_R]).T

    # Calculate adjusted influences I = m @ Omega
    I = m @ omega_matrix  # shape: (n_households, 3)

    # Unpack results and store in dataframe
    households_df['I_O'] = I[:, 0]
    households_df['I_S'] = I[:, 1]
    households_df['I_R'] = I[:, 2]

    return households_df

# --- Threshold Functions ---

def generate_group_thresholds(group_df, threshold_params):
    """
    Generate group-level threshold *distribution parameters* (mean mu, std sigma)
    based on group attributes and hierarchical model parameters.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group-level attributes (H_g, D_g). Index must be 'group_id'.
    threshold_params : dict
        Dictionary of threshold model parameters (beta coefficients, sigmas).

    Returns:
    --------
    group_df : DataFrame
        DataFrame with added threshold distribution parameter columns (mu_O, mu_S, mu_R, sigma_O, etc.).
    """
    print("Generating group-level threshold distribution parameters...")

    # Extract beta coefficients from params
    beta_H_O = threshold_params['beta_H_O'] # Shape (2,)
    beta_D_O = threshold_params['beta_D_O'] # Shape (2,)
    beta_H_S = threshold_params['beta_H_S'] # Shape (2,)
    beta_D_S = threshold_params['beta_D_S'] # Shape (2,)
    beta_H_R = threshold_params['beta_H_R'] # Shape (2,)
    beta_D_R = threshold_params['beta_D_R'] # Shape (2,)

    # Extract standard deviation parameters (scalar for each dimension)
    sigma_O = threshold_params['sigma_O']
    sigma_S = threshold_params['sigma_S']
    sigma_R = threshold_params['sigma_R']

    # Calculate mean parameters (mu) for each group using dot products
    # group_df['H_g'] and group_df['D_g'] contain the feature vectors
    group_df['mu_O'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_O) + np.dot(row['D_g'], beta_D_O), axis=1)
    group_df['mu_S'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_S) + np.dot(row['D_g'], beta_D_S), axis=1)
    group_df['mu_R'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_R) + np.dot(row['D_g'], beta_D_R), axis=1)

    # Assign constant standard deviations to all groups
    group_df['sigma_O'] = sigma_O
    group_df['sigma_S'] = sigma_S
    group_df['sigma_R'] = sigma_R

    print("Group threshold parameters generated.")
    return group_df

def assign_individual_thresholds(households_df, group_df):
    """
    Assign thresholds (tau) to individual households by drawing from their group's distribution.
    This function is called AT EACH TIMESTEP to generate new thresholds.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_df : DataFrame
        DataFrame containing group threshold distribution parameters (mu_*, sigma_*), indexed by 'group_id'.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated threshold columns ('tau_O', 'tau_S', 'tau_R') and
        intermediate latent variables ('Z_O', 'Z_S', 'Z_R') for this timestep.
    """
    # Create temporary columns for group parameters by merging
    # Suffixes avoid collision if households_df already had mu/sigma columns from previous step
    temp_df = households_df.merge(
        group_df[['mu_O', 'mu_S', 'mu_R', 'sigma_O', 'sigma_S', 'sigma_R']],
        left_on='group_id',
        right_index=True, # Merge group_df on its index ('group_id')
        how='left',
        suffixes=('', '_group_params') # Add suffix to avoid potential name collisions if mu/sigma already exist
    )
    # temp_df index is now aligned with households_df index ('household_id')

    # --- Non-Centered Parameterization ---
    # 1. Generate standard normal deviates (eta) for each household
    num_households = len(households_df)
    eta_O = np.random.normal(0, 1, size=num_households)
    eta_S = np.random.normal(0, 1, size=num_households)
    eta_R = np.random.normal(0, 1, size=num_households)

    # Assign eta to the main dataframe, aligning by index
    households_df['eta_O'] = eta_O
    households_df['eta_S'] = eta_S
    households_df['eta_R'] = eta_R

    # 2. Calculate household-specific latent variables (Z) using NCP: Z = mu + sigma * eta
    # Use the mapped group parameters (mu, sigma) from temp_df, ensuring alignment
    # Accessing via temp_df which has the household_id index ensures correct mapping
    households_df['Z_O'] = temp_df['mu_O'] + temp_df['sigma_O'] * households_df['eta_O']
    households_df['Z_S'] = temp_df['mu_S'] + temp_df['sigma_S'] * households_df['eta_S']
    households_df['Z_R'] = temp_df['mu_R'] + temp_df['sigma_R'] * households_df['eta_R']
    # --- End of NCP ---

    # Transform latent variables (Z) to thresholds (tau) using the logistic function
    households_df['tau_O'] = 1 / (1 + np.exp(-households_df['Z_O']))
    households_df['tau_S'] = 1 / (1 + np.exp(-households_df['Z_S']))
    households_df['tau_R'] = 1 / (1 + np.exp(-households_df['Z_R']))

    # Keep the group mean columns (mu_*) for potential analysis/visualization if needed
    # Ensure we are taking the correctly mapped means from temp_df
    households_df['mu_O'] = temp_df['mu_O']
    households_df['mu_S'] = temp_df['mu_S']
    households_df['mu_R'] = temp_df['mu_R']

    # Optionally keep sigma columns as well
    # households_df['sigma_O'] = temp_df['sigma_O']
    # households_df['sigma_S'] = temp_df['sigma_S']
    # households_df['sigma_R'] = temp_df['sigma_R']

    # Reset index if it was originally a column
    # households_df = households_df.reset_index() # Uncomment if needed

    return households_df

# --- State Update Function ---

def update_states_with_group_thresholds(households_df, smoothing_factor):
    """
    Update household decision states (Y_O, Y_S, Y_R) based on individual thresholds (tau)
    and adjusted influences (I) using a logistic smoothed threshold model.
    Tracks state transitions explicitly for bridging influence calculation in the *next* step.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with current states (Y_*), influences (I_*),
        and thresholds (tau_*). Must also contain previous states (Y_*_prev) and
        historical states (Y_*_hist).
    smoothing_factor : float
        Steepness factor for the logistic transition probability curve. Higher values approximate a step function.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated state columns (Y_*), updated history (Y_*_hist),
        updated previous state markers (Y_*_prev becomes the state before this update),
        and updated transition markers (Y_*_transition for 0->1 changes in this step).
    """
    # Store current states to later become the 'previous' states for the next timestep
    Y_O_prev = households_df['Y_O'].values.copy()
    Y_S_prev = households_df['Y_S'].values.copy()
    Y_R_prev = households_df['Y_R'].values.copy()

    # Get current states, influences, and thresholds
    Y_O = households_df['Y_O'].values # Current state (before update)
    Y_S = households_df['Y_S'].values
    Y_R = households_df['Y_R'].values
    I_O = households_df['I_O'].values
    I_S = households_df['I_S'].values
    I_R = households_df['I_R'].values
    tau_O = households_df['tau_O'].values # Thresholds generated for this step
    tau_S = households_df['tau_S'].values
    tau_R = households_df['tau_R'].values

    # --- Calculate State Transition Probabilities ---
    # Probability of transitioning from 0 to 1 using logistic function
    # p = 1 / (1 + exp(-k * (Influence - Threshold)))
    p_O = 1 / (1 + np.exp(-smoothing_factor * (I_O - tau_O)))
    p_S = 1 / (1 + np.exp(-smoothing_factor * (I_S - tau_S)))
    p_R = 1 / (1 + np.exp(-smoothing_factor * (I_R - tau_R)))

    # Store probabilities for analysis (optional)
    households_df['p_O'] = p_O
    households_df['p_S'] = p_S
    households_df['p_R'] = p_R

    # --- Stochastic State Update (0 -> 1 transitions only) ---
    # Generate random numbers for stochastic check
    rand_O = np.random.rand(len(Y_O))
    rand_S = np.random.rand(len(Y_S))
    rand_R = np.random.rand(len(Y_R))

    # Update state to 1 if currently 0 AND random number is less than transition probability
    new_Y_O = np.where((Y_O == 0) & (rand_O < p_O), 1, Y_O)
    new_Y_S = np.where((Y_S == 0) & (rand_S < p_S), 1, Y_S)
    new_Y_R = np.where((Y_R == 0) & (rand_R < p_R), 1, Y_R)

    # --- Update Tracking Columns ---
    # 1. Transition Markers (for next step's bridging influence)
    # A transition occurred *in this step* if the state went from 0 (Y_O_prev) to 1 (new_Y_O)
    Y_O_transition = ((Y_O_prev == 0) & (new_Y_O == 1)).astype(np.int8)
    Y_S_transition = ((Y_S_prev == 0) & (new_Y_S == 1)).astype(np.int8)
    Y_R_transition = ((Y_R_prev == 0) & (new_Y_R == 1)).astype(np.int8)

    # 2. Historical States (for next step's bonding influence)
    # Update historical state to be the maximum state seen so far (current or previous history)
    Y_O_hist = np.maximum(new_Y_O, households_df['Y_O_hist'].values)
    Y_S_hist = np.maximum(new_Y_S, households_df['Y_S_hist'].values)
    Y_R_hist = np.maximum(new_Y_R, households_df['Y_R_hist'].values)

    # --- Store Updated Values in DataFrame ---
    # Store the *newly calculated* states
    households_df['Y_O'] = new_Y_O
    households_df['Y_S'] = new_Y_S
    households_df['Y_R'] = new_Y_R

    # Store the *state before this update* as the 'previous' state for the next cycle
    households_df['Y_O_prev'] = Y_O_prev
    households_df['Y_S_prev'] = Y_S_prev
    households_df['Y_R_prev'] = Y_R_prev

    # Store the *updated* historical max states
    households_df['Y_O_hist'] = Y_O_hist
    households_df['Y_S_hist'] = Y_S_hist
    households_df['Y_R_hist'] = Y_R_hist

    # Store the *transition markers calculated in this step*
    households_df['Y_O_transition'] = Y_O_transition
    households_df['Y_S_transition'] = Y_S_transition
    households_df['Y_R_transition'] = Y_R_transition

    return households_df


# --- Visualization Functions ---

def visualize_network(households_df, W_B, W_R, id_to_idx, time_step, output_dir='.'):
    """
    Visualize the network with nodes colored by group/state at a specific time step.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data for the current time step.
    W_B, W_R : ndarray
        Adjacency matrices for bonding and bridging networks.
    id_to_idx : dict
        Mapping from household ID to matrix index.
    time_step : int
        Current simulation time step.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print(f"  Generating network visualization for t={time_step}...")
    # Create a new graph
    G = nx.Graph()

    # Get necessary data from DataFrame
    household_ids = households_df['household_id'].values
    idx_to_id = {v: k for k, v in id_to_idx.items()} # Need index to ID mapping
    Y_O = households_df['Y_O'].values
    Y_S = households_df['Y_S'].values
    Y_R = households_df['Y_R'].values
    group_ids = households_df['group_id'].values
    lats = households_df['lat'].values
    lons = households_df['lon'].values

    # Add nodes with attributes
    for i, h_id in enumerate(household_ids):
        G.add_node(h_id,
                   pos=(lons[i], lats[i]), # Use lon, lat for x, y plotting
                   Y_O=Y_O[i],
                   Y_S=Y_S[i],
                   Y_R=Y_R[i],
                   group_id=group_ids[i])

    # Create edge lists for bonding and bridging networks
    bonding_edges = []
    bridging_edges = []
    n_households = len(household_ids)

    for i in range(n_households):
        for j in range(i + 1, n_households):  # Avoid self-loops and duplicates
            h_i = idx_to_id[i]
            h_j = idx_to_id[j]

            if W_B[i, j] > 0:
                bonding_edges.append((h_i, h_j, {'weight': W_B[i, j], 'type': 'bonding'}))
            if W_R[i, j] > 0:
                bridging_edges.append((h_i, h_j, {'weight': W_R[i, j], 'type': 'bridging'}))

    # Add edges to graph
    G.add_edges_from(bonding_edges)
    G.add_edges_from(bridging_edges)

    # Get node positions
    pos = nx.get_node_attributes(G, 'pos')

    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(20, 18)) # Increased size slightly
    axes = axes.flatten()

    # --- Plotting Setup ---
    n_groups = households_df['group_id'].nunique()
    # Ensure n_groups is at least 1 for cmap
    cmap_n_groups = max(1, n_groups)
    group_cmap = plt.cm.get_cmap('tab10', cmap_n_groups) # Colormap for groups
    state_cmap = LinearSegmentedColormap.from_list('StateColors', ['green', 'red']) # 0=green, 1=red

    node_size = 40 # Slightly larger nodes
    edge_alpha_B = 0.4
    edge_alpha_R = 0.25
    edge_width_B = 0.8
    edge_width_R = 0.6

    # --- Plot 1: Nodes colored by Group ID ---
    ax = axes[0]
    node_colors_group = [G.nodes[node]['group_id'] for node in G.nodes()]
    # Normalize colors if n_groups > 0
    norm_colors = np.array(node_colors_group) / max(1, n_groups -1) if n_groups > 1 else np.zeros(len(node_colors_group))

    nx.draw_networkx_nodes(G, pos, node_color=node_colors_group, cmap=group_cmap, vmin=0, vmax=max(0, n_groups-1),
                           node_size=node_size, alpha=0.9, ax=ax)
    nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                           alpha=edge_alpha_B, edge_color='blue', ax=ax)
    nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                           alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)
    ax.set_title(f'Network by Group Membership (t={time_step})', fontsize=16)
    ax.axis('off') # Turn off axis box

    # --- Plots 2-4: Nodes colored by Decision State (Y_O, Y_S, Y_R) ---
    states_info = [
        {'attr': 'Y_O', 'title': 'Occupancy State (Y_O=1 is Vacant)', 'ax_idx': 1},
        {'attr': 'Y_S', 'title': 'Sales State (Y_S=1 is For Sale)', 'ax_idx': 2},
        {'attr': 'Y_R', 'title': 'Recovery State (Y_R=1 is Repaired)', 'ax_idx': 3},
    ]

    for info in states_info:
        ax = axes[info['ax_idx']]
        attr = info['attr']
        node_colors_state = [G.nodes[node][attr] for node in G.nodes()]

        nx.draw_networkx_nodes(G, pos, node_color=node_colors_state, cmap=state_cmap, vmin=0, vmax=1,
                               node_size=node_size, alpha=0.9, ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                               alpha=edge_alpha_B, edge_color='blue', ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                               alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)

        # Add title with overall percentage
        state_pct = np.mean(node_colors_state) * 100 if node_colors_state else 0 # Handle empty list
        ax.set_title(f'{info["title"]} (t={time_step}) - {state_pct:.1f}%', fontsize=16)
        ax.axis('off') # Turn off axis box

        # Add group-level statistics in a text box
        group_stats = []
        unique_group_ids = sorted(households_df['group_id'].unique())
        for g_id in unique_group_ids:
            group_nodes = [node for node in G.nodes() if G.nodes[node]['group_id'] == g_id]
            if group_nodes: # Ensure group is not empty
                group_state_values = [G.nodes[node][attr] for node in group_nodes]
                group_pct = np.mean(group_state_values) * 100 if group_state_values else 0
                group_stats.append(f"Grp {g_id}: {group_pct:.1f}%")
            else:
                 group_stats.append(f"Grp {g_id}: N/A")

        stats_text = "\n".join(group_stats)
        ax.text(0.02, 0.02, stats_text, fontsize=10, transform=ax.transAxes,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

    # --- Legend ---
    legend_handles = []
    # Group colors
    unique_group_ids_legend = sorted(households_df['group_id'].unique())
    if unique_group_ids_legend:
        norm_factor = max(1, n_groups - 1) # Avoid division by zero if n_groups is 0 or 1
        for g_id in unique_group_ids_legend:
             legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',
                                             markerfacecolor=group_cmap(g_id / norm_factor), # Normalize index
                                             markersize=10, label=f'Group {g_id}'))
    # Edge types
    legend_handles.extend([
        plt.Line2D([0], [0], color='blue', lw=2, label='Bonding'),
        plt.Line2D([0], [0], color='green', lw=2, linestyle='--', label='Bridging')
    ])
    # State colors
    legend_handles.extend([
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='State=0'),
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='State=1')
    ])

    # Calculate dynamic ncol for legend based on number of items
    num_legend_items = len(legend_handles)
    legend_ncol = min(num_legend_items, 8) # Max 8 columns, adjust as needed

    fig.legend(handles=legend_handles, loc='lower center', ncol=legend_ncol, fontsize=12, bbox_to_anchor=(0.5, 0.01))

    plt.tight_layout(rect=[0, 0.05, 1, 0.96]) # Adjust rect to prevent title overlap and fit legend
    filename = os.path.join(output_dir, f'network_state_t{time_step:03d}.png') # Use padded number for sorting
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show() # Avoid showing plot in script execution
    plt.close(fig) # Close the figure to free memory
    print(f"    Visualization saved to {filename}")




def visualize_state_evolution(results, output_dir='.'):
    """
    Visualize the evolution of the proportion of households in each state (Y=1) over time.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing state evolution over time...")
    time_steps = sorted(results.keys())
    if not time_steps:
        print("  No results to visualize.")
        return

    # Calculate proportions for each state at each time step
    props = {'Y_O': [], 'Y_S': [], 'Y_R': []}
    for t in time_steps:
        df = results[t]
        if not df.empty:
            props['Y_O'].append(np.mean(df['Y_O']) * 100)
            props['Y_S'].append(np.mean(df['Y_S']) * 100)
            props['Y_R'].append(np.mean(df['Y_R']) * 100)
        else: # Handle empty dataframe case if needed
            props['Y_O'].append(np.nan) # Use NaN for missing data points
            props['Y_S'].append(np.nan)
            props['Y_R'].append(np.nan)


    # Create plot
    plt.figure(figsize=(12, 7))
    plt.plot(time_steps, props['Y_O'], 'r-o', linewidth=2, markersize=5, label='Vacant (Y_O=1)')
    plt.plot(time_steps, props['Y_S'], 'g-^', linewidth=2, markersize=5, label='For Sale (Y_S=1)')
    plt.plot(time_steps, props['Y_R'], 'b-s', linewidth=2, markersize=5, label='Repaired (Y_R=1)')

    # Formatting
    plt.xlabel('Time Step', fontsize=14)
    plt.ylabel('Proportion of Households (%)', fontsize=14)
    plt.title('Evolution of Household States Over Time', fontsize=16)
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.legend(fontsize=12, loc='best')
    # Ensure x-ticks cover the range, adjust frequency dynamically
    tick_step = max(1, len(time_steps) // 12) # Show ~12 ticks max
    plt.xticks(time_steps[::tick_step])
    plt.yticks(np.arange(0, 101, 10)) # Y-axis ticks every 10%
    plt.ylim(-2, 102) # Add slight padding to y-axis

    # Save figure
    plt.tight_layout()
    filename = os.path.join(output_dir, 'state_evolution.png')
    plt.savefig(filename, dpi=300)
    plt.show()
    plt.close()
    print(f"  State evolution plot saved to {filename}")


# --- Simulation Runner ---
def visualize_all_thresholds_by_group(results, group_df, output_dir='.'):
    """
    Visualize the distributions of thresholds (tau) for each group across ALL time steps.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data.
    group_df : DataFrame
        DataFrame containing group data with distribution parameters (mu_*, sigma_*).
    output_dir : str, default='.'
        Directory to save the visualization PNG files.
    """
    print("Visualizing threshold distributions by group across all time steps...")
    states = ['Occupancy (O)', 'Sales (S)', 'Recovery (R)']
    threshold_cols = ['tau_O', 'tau_S', 'tau_R']

    # Get all time steps
    time_steps = sorted(results.keys())
    if not time_steps:
        print("  No results to visualize.")
        return

    # Get all group IDs
    n_groups = len(group_df.index)
    if n_groups == 0:
        print("  No groups found, skipping threshold visualization.")
        return

    group_colors = plt.cm.tab10(np.linspace(0, 1, n_groups))

    # For each state (O, S, R)
    for state_idx, (state, threshold_col) in enumerate(zip(states, threshold_cols)):
        fig, axes = plt.subplots(1, n_groups, figsize=(n_groups * 4, 5), sharey=True)
        if n_groups == 1: axes = np.array([axes])  # Ensure axes is iterable

        fig.suptitle(f'All Time Steps {state} Threshold (τ) Distribution by Group', fontsize=16, y=1.02)

        # For each group
        for g_idx, g_id in enumerate(sorted(group_df.index)):
            ax = axes[g_idx]

            # Collect threshold values for this group across all time steps
            all_thresholds = []
            for t in time_steps:
                df_t = results[t]
                group_households = df_t[df_t['group_id'] == g_id]
                if threshold_col in group_households.columns and not group_households.empty:
                    all_thresholds.extend(group_households[threshold_col].tolist())

            if all_thresholds:
                # Convert to numpy array for calculations
                all_thresholds = np.array(all_thresholds)

                # Plot KDE of thresholds from all time steps
                sns.kdeplot(all_thresholds, ax=ax, color=group_colors[g_idx % len(group_colors)],
                           alpha=0.6, label='All time steps')

                # Calculate statistics
                group_mean = np.mean(all_thresholds)
                group_std = np.std(all_thresholds)

                # Add reference lines
                ax.axvline(group_mean, color='red', linestyle='--', linewidth=1.5,
                          label=f'Mean={group_mean:.2f}')

                # Add sample size information
                stats_text = f"Mean: {group_mean:.3f}\nStd: {group_std:.3f}"
                ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=9,
                        verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            else:
                ax.text(0.5, 0.5, "No thresholds data", ha='center', va='center')

            # Formatting
            ax.set_xlabel('Threshold Value (τ)')
            if g_idx == 0: ax.set_ylabel('Density')
            ax.set_title(f'Group {g_id}')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 6)
            ax.grid(False)
            ax.tick_params(axis='x', labelsize=9)
            ax.tick_params(axis='y', labelsize=9)
            ax.legend(fontsize=8, loc='best')

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        filename = os.path.join(output_dir, f'all_timesteps_threshold_{threshold_col}_distribution.png')
        plt.savefig(filename, dpi=300)
        plt.show()
        plt.close(fig)
        print(f"  Saved: {filename}")

    print("All-timesteps threshold visualization complete.")

def homogenize_group_attributes(households_df, seed=None):
    """
    Make all households within the same group have identical attributes.
    Called after group assignment to ensure within-group homogeneity.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id' already assigned.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with homogenized household attributes (same values within groups).
    """
    print("Homogenizing attributes within groups...")

    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)

    # Get unique group IDs
    group_ids = sorted(households_df['group_id'].unique())

    # Attributes to homogenize
    attributes = ['income', 'resource_access', 'damage_structural', 'damage_utility']

    # For each group, set unified attribute values
    for g_id in group_ids:
        # Get indices of households in this group
        group_indices = households_df[households_df['group_id'] == g_id].index

        if len(group_indices) > 0:
            # Method 1: Randomly generate a single set of binary attributes for the entire group
            group_income = np.random.choice([0, 1])
            group_resource_access = np.random.choice([0, 1])
            group_damage_structural = np.random.choice([0, 1])
            group_damage_utility = np.random.choice([0, 1])

            # Apply the same values to all households in this group
            households_df.loc[group_indices, 'income'] = group_income
            households_df.loc[group_indices, 'resource_access'] = group_resource_access
            households_df.loc[group_indices, 'damage_structural'] = group_damage_structural
            households_df.loc[group_indices, 'damage_utility'] = group_damage_utility

            print(f"  Group {g_id} ({len(group_indices)} households) - Attributes set to: " +
                  f"income={group_income}, resource_access={group_resource_access}, " +
                  f"damage_structural={group_damage_structural}, damage_utility={group_damage_utility}")

    return households_df


def run_simulation_with_groups(households_df_initial, network_df, simulation_params):
    """
    Run the full agent-based simulation with group-level dynamics and threshold regeneration.

    Parameters:
    -----------
    households_df_initial : DataFrame
        Initial DataFrame containing basic household attributes (ID, lat, lon, characteristics).
    network_df : DataFrame
        DataFrame containing network connections ('home_1', 'home_2', 'type', 'avg_link').
    simulation_params : dict
        Dictionary of simulation parameters (num_steps, seed, thresholds, omega, etc.).

    Returns:
    --------
    results : dict
        Dictionary mapping time step (int) to DataFrame with full household state at that step.
    group_df : DataFrame
        DataFrame containing the final group attributes and threshold parameters.
    """
    # --- Parameter Extraction ---
    num_steps = simulation_params.get('num_steps', 20)
    output_dir = simulation_params.get('output_dir', 'simulation_results')
    random_seed = simulation_params.get('random_seed', None) # Use None if not provided
    damage_threshold = simulation_params.get('damage_threshold', 0.5)
    visualization_interval = simulation_params.get('visualization_interval', 5)
    omega_matrix = simulation_params.get('omega_matrix')
    threshold_params = simulation_params.get('threshold_params')
    num_groups = simulation_params.get('num_groups', 5)
    smoothing_factor = simulation_params.get('smoothing_factor')

    print("="*30)
    print(" Starting Simulation Run ")
    print("="*30)
    print(f"Parameters: Steps={num_steps}, Groups={num_groups}, Seed={random_seed}, Output='{output_dir}'")

    # --- Setup ---
    # Set global random seed for reproducibility *across runs*
    if random_seed is not None:
        np.random.seed(random_seed)

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Make a copy to avoid modifying the original input dataframe
    households_df = households_df_initial.copy()
    # Ensure household_id is set as index early on if needed, or manage carefully
    # households_df.set_index('household_id', inplace=True, drop=False) # Keep id as column too

    # --- Initialization Steps (Time t=0) ---
    print("\n--- Initialization (t=0) ---")
    # 1. Assign households to groups
    households_df, group_centroids = assign_households_to_groups(
        households_df, num_groups=num_groups, random_seed=random_seed)

    households_df = homogenize_group_attributes(households_df, seed=random_seed)

    # 2. Generate group-level attributes (means, feature vectors)
    group_df = generate_group_attributes(households_df, group_centroids)

    # 3. Generate group-level *threshold distribution parameters* (mu, sigma)
    # These parameters are fixed based on initial group attributes.
    group_df = generate_group_thresholds(group_df, threshold_params)

    # 4. Assign *initial* individual thresholds to households (for t=0 state and visualization)
    # This uses the group distributions generated above.
    households_df = assign_individual_thresholds(households_df, group_df)

    # 5. Initialize household decision states (Y_O, Y_S, Y_R) and history/tracking columns
    households_df = initialize_decision_states(households_df, damage_threshold, seed=random_seed)

    # 6. Create static network matrices (W_B, W_R) and ID/index mappings
    W_B, W_R, id_to_idx, idx_to_id = create_network_matrices(households_df, network_df)

    # --- Store Initial State ---
    # Initialize results dictionary with the state at t=0
    # Calculate initial influences for completeness in t=0 results
    print("  Calculating initial influences for t=0...")
    households_df = calculate_raw_influences(households_df, W_B, W_R)
    households_df = calculate_adjusted_influences(households_df, omega_matrix)
    # Add placeholder columns for probabilities at t=0, filled with NaN or 0
    households_df['p_O'] = np.nan
    households_df['p_S'] = np.nan
    households_df['p_R'] = np.nan
    results = {0: households_df.copy()}
    print("Initialization complete.")

    # --- Initial Visualizations ---
    print("\n--- Initial Visualizations ---")
    if visualization_interval > 0: # Check if visualization is enabled
        visualize_network(households_df, W_B, W_R, id_to_idx, 0, output_dir)

    # --- Simulation Loop (Time t=1 to num_steps) ---
    print(f"\n--- Running Simulation Steps (1 to {num_steps}) ---")
    for t in range(1, num_steps + 1):
        print(f"\n--- Time Step {t} ---")

        # 1. Calculate Raw Influences (m_*) based on states from t-1
        # Uses Y_*_hist and Y_*_transition from the *previous* step (stored in current df)
        print(f"  Calculating raw influences...")
        households_df = calculate_raw_influences(households_df, W_B, W_R)

        # 2. Calculate Adjusted Influences (I_*) using Omega matrix
        print(f"  Calculating adjusted influences...")
        households_df = calculate_adjusted_influences(households_df, omega_matrix)

        # 3. Assign NEW Individual Thresholds (tau_*) for this timestep
        # Draws new random values based on the fixed group distributions (mu_g, sigma_g)
        print(f"  Assigning new individual thresholds...")
        households_df = assign_individual_thresholds(households_df, group_df)

        # 4. Update Household States (Y_*) based on I_* and tau_* for this step
        # This calculates new Y_*, updates Y_*_prev, Y_*_hist, Y_*_transition
        # It also calculates and stores p_O, p_S, p_R for this step.
        print(f"  Updating household states...")
        households_df = update_states_with_group_thresholds(households_df, smoothing_factor)

        # 5. Store results for this time step
        results[t] = households_df.copy()
        print(f"  State update complete for t={t}.")
        # Safely calculate means, handling potential NaN or empty data
        mean_O = np.nanmean(households_df['Y_O'])*100 if 'Y_O' in households_df.columns and not households_df['Y_O'].empty else np.nan
        mean_S = np.nanmean(households_df['Y_S'])*100 if 'Y_S' in households_df.columns and not households_df['Y_S'].empty else np.nan
        mean_R = np.nanmean(households_df['Y_R'])*100 if 'Y_R' in households_df.columns and not households_df['Y_R'].empty else np.nan
        print(f"    Vacant: {mean_O:.1f}%, "
              f"For Sale: {mean_S:.1f}%, "
              f"Repaired: {mean_R:.1f}%")


        # 6. Visualize network at specified intervals
        if visualization_interval > 0 and (t % visualization_interval == 0 or t == num_steps):
             visualize_network(households_df, W_B, W_R, id_to_idx, t, output_dir)

    print("\n--- Simulation Loop Completed ---")

    # --- Final Visualizations & Export ---
    print("\n--- Final Visualizations & Export ---")
    visualize_state_evolution(results, output_dir)
    visualize_all_thresholds_by_group(results, group_df, output_dir) # Add this line to call the new function
    export_results_to_csv(results, output_dir) # Export includes influences now

    print("\nSimulation finished successfully!")
    print(f"Results and visualizations saved in '{output_dir}'")
    print("="*30)

    return results, group_df


# --- Export Function ---

def export_results_to_csv(results, output_dir):
    """
    Export simulation results to a CSV file, including states, thresholds, and influences.
    Handles potentially missing probability columns at t=0.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data for that step.
    output_dir : str
        Directory to save the output CSV file.
    """
    print("Exporting detailed simulation results to CSV...")
    all_data = []

    # Define base and optional columns
    base_cols = [
        'household_id', 'group_id',
        'Y_O', 'Y_S', 'Y_R',
        'tau_O', 'tau_S', 'tau_R',
        'I_O', 'I_S', 'I_R',
        'm_O', 'm_S', 'm_R'
    ]
    optional_cols = ['p_O', 'p_S', 'p_R']

    # Process each timestep's DataFrame
    for timestep, df in results.items():
        # Check which columns actually exist in this timestep's DataFrame
        cols_to_select = [col for col in base_cols if col in df.columns]
        existing_optional_cols = [col for col in optional_cols if col in df.columns]
        all_cols_this_step = cols_to_select + existing_optional_cols

        if not all_cols_this_step: # Skip if somehow df is empty or has no relevant columns
             print(f"Warning: No columns to select for timestep {timestep}. Skipping.")
             continue

        # Select only the existing columns
        df_export = df[all_cols_this_step].copy() # Create a copy

        # Add timestep column
        df_export['timestep'] = timestep

        # Rename columns for clarity in CSV
        rename_map = {
            'Y_O': 'occupancy_state',    # 1=vacant
            'Y_S': 'sales_state',        # 1=for_sale
            'Y_R': 'recovery_state',     # 1=repaired
            'tau_O': 'threshold_occupancy',
            'tau_S': 'threshold_sale',
            'tau_R': 'threshold_recovery',
            'I_O': 'influence_occupancy', # Adjusted influence
            'I_S': 'influence_sale',
            'I_R': 'influence_recovery',
            'm_O': 'raw_influence_O',     # Raw influence
            'm_S': 'raw_influence_S',
            'm_R': 'raw_influence_R',
            'p_O': 'prob_transition_O',   # Transition probability
            'p_S': 'prob_transition_S',
            'p_R': 'prob_transition_R'
        }
        # Only rename columns that exist in df_export
        actual_rename_map = {k: v for k, v in rename_map.items() if k in df_export.columns}
        df_export.rename(columns=actual_rename_map, inplace=True)

        all_data.append(df_export)

    # Concatenate all dataframes
    if not all_data:
        print("  No data to export.")
        return

    results_df = pd.concat(all_data, ignore_index=True, sort=False) # sort=False preserves columns

    # Define desired final column order (superset of all possible columns)
    final_column_order = [
        'household_id', 'timestep', 'group_id',
        'occupancy_state', 'sales_state', 'recovery_state',
        'threshold_occupancy', 'threshold_sale', 'threshold_recovery',
        'influence_occupancy', 'influence_sale', 'influence_recovery',
        'raw_influence_O', 'raw_influence_S', 'raw_influence_R',
        'prob_transition_O', 'prob_transition_S', 'prob_transition_R'
    ]
    # Filter order to only include columns actually present in the concatenated df
    existing_final_columns = [col for col in final_column_order if col in results_df.columns]
    results_df = results_df[existing_final_columns]


    # Sort for readability
    results_df = results_df.sort_values(by=['household_id', 'timestep'])

    # Save to CSV
    csv_path = os.path.join(output_dir, 'decision_states_simulation_details.csv')
    try:
        results_df.to_csv(csv_path, index=False, float_format='%.5f') # Format floats
        print(f"  Detailed results saved to {csv_path}")
    except Exception as e:
        print(f"  Error saving CSV: {e}")


# --- Main Execution ---

def main():
    """Main function to set up parameters and run the simulation."""

    # --- Configuration ---
    NETWORK_FILE = 'small_baltimore_household.csv' # Ensure this file exists
    OUTPUT_DIR = 'simulation_results_dynamic_thresholds'
    RANDOM_SEED = 12
    NUM_STEPS = 24
    NUM_GROUPS = 5
    VISUALIZATION_INTERVAL = 4 # Set to 0 to disable intermediate network plots
    DAMAGE_THRESHOLD = 0.02 # Initial threshold for Y_R=1 (lower means needs less damage to be 'ok')
    SMOOTHING_FACTOR = 9 # Steepness of logistic transition (higher is sharper)

    # Correlation matrix (Omega) - Identity matrix means no cross-influence adjustment
    OMEGA_MATRIX = np.array([
        [1.0, 0.2, -0.2],  # Influence -> State O
        [0.2, 1.0, 0.1],  # Influence -> State S
        [-0.2, 0.1, 1.0]   # Influence -> State R
    ])
    # Example: Positive correlation between Sale and Occupancy influence
    # OMEGA_MATRIX = np.array([
    #     [1.0, 0.2, 0.0], # Influence O affects State O (1.0) and State S (0.2)
    #     [0.1, 1.0, 0.0], # Influence S affects State O (0.1) and State S (1.0)
    #     [0.0, 0.0, 1.0]
    # ])


    # Threshold model parameters (Beta coefficients and Sigmas)
    THRESHOLD_PARAMS = {
        # Beta coefficients link group features (H_g, D_g) to the *mean* (mu) of the latent variable Z
        # H_g = [avg_income, avg_resource_access]
        # D_g = [avg_damage_structural, avg_damage_utility]

        # Coefficients for Occupancy (O) threshold mean (mu_O)
        'beta_H_O': np.array([-0.5, -0.3]), # Higher income/access -> lower mu_O -> lower tau_O -> easier to become vacant?
        'beta_D_O': np.array([0.8, 0.6]),   # Higher damage -> higher mu_O -> higher tau_O -> harder to become vacant?

        # Coefficients for Sales (S) threshold mean (mu_S)
        'beta_H_S': np.array([-0.2, 0.1]),  # Higher income -> lower mu_S; Higher access -> higher mu_S
        'beta_D_S': np.array([0.5, 0.7]),   # Higher damage -> higher mu_S -> higher tau_S -> harder to go for sale?

        # Coefficients for Recovery (R) threshold mean (mu_R)
        'beta_H_R': np.array([0.6, 0.4]),   # Higher income/access -> higher mu_R -> higher tau_R -> harder to recover? (Maybe signs should be flipped?)
        'beta_D_R': np.array([-0.8, -1.0]), # Higher damage -> lower mu_R -> lower tau_R -> easier to recover? (Makes sense - more incentive)

        # Standard deviations (sigma) of the latent variable Z distribution for each group
        # Controls the variability of thresholds *within* a group
        'sigma_O': 0.5, # Larger sigma = more household variability around group mean
        'sigma_S': 0.4,
        'sigma_R': 0.6,
    }

    # --- Data Loading ---
    print("--- Loading Data ---")
    try:
        network_df = pd.read_csv(NETWORK_FILE)
        print(f"Network data loaded: {len(network_df)} connections from '{NETWORK_FILE}'")
    except FileNotFoundError:
        print(f"Error: Network file not found at '{NETWORK_FILE}'. Please ensure the file exists.")
        return # Exit if data not found
    except Exception as e:
        print(f"Error loading network file '{NETWORK_FILE}': {e}")
        return

    # --- Prepare Household Data ---
    print("\n--- Preparing Household Data ---")
    # Extract unique households and their locations
    print("Extracting unique households and locations...")
    try:
        homes1 = network_df[['home_1', 'home_1_lat', 'home_1_lon']].rename(
            columns={'home_1': 'id', 'home_1_lat': 'lat', 'home_1_lon': 'lon'})
        homes2 = network_df[['home_2', 'home_2_lat', 'home_2_lon']].rename(
            columns={'home_2': 'id', 'home_2_lat': 'lat', 'home_2_lon': 'lon'})
        unique_homes = pd.concat([homes1, homes2]).drop_duplicates(subset=['id']).reset_index(drop=True)

        household_ids = unique_homes['id'].tolist()
        # Ensure locations are valid floats
        household_locs = {row['id']: (float(row['lat']), float(row['lon']))
                          for _, row in unique_homes.iterrows()
                          if pd.notna(row['lat']) and pd.notna(row['lon'])}
        # Filter household_ids to only include those with valid locations
        household_ids = [hid for hid in household_ids if hid in household_locs]

        print(f"Found {len(household_ids)} unique households with valid locations.")

        if not household_ids:
            print("Error: No unique households with valid lat/lon found in the network data.")
            return

        # Generate initial household attributes (income, damage, etc.)
        households_df_initial = generate_household_attributes(household_ids, household_locs, seed=RANDOM_SEED)

    except KeyError as e:
        print(f"Error preparing household data: Missing expected column in network file: {e}")
        return
    except Exception as e:
        print(f"Error preparing household data: {e}")
        return


    # --- Simulation Parameters Dictionary ---
    simulation_params = {
        'num_steps': NUM_STEPS,
        'output_dir': OUTPUT_DIR,
        'random_seed': RANDOM_SEED,
        'damage_threshold': DAMAGE_THRESHOLD,
        'visualization_interval': VISUALIZATION_INTERVAL,
        'num_groups': NUM_GROUPS,
        'omega_matrix': OMEGA_MATRIX,
        'threshold_params': THRESHOLD_PARAMS,
        'smoothing_factor': SMOOTHING_FACTOR
    }

    # --- Run Simulation ---
    try:
        results, final_group_df = run_simulation_with_groups(
            households_df_initial,
            network_df,
            simulation_params
        )
    except Exception as e:
        print(f"\n--- SIMULATION FAILED ---")
        print(f"Error during simulation run: {e}")
        import traceback
        traceback.print_exc() # Print detailed traceback
        return # Stop execution

    # --- Post-Simulation Analysis (Optional) ---
    if results:
        final_step = max(results.keys())
        final_households_df = results[final_step]

        # Save final households data separately
        final_csv_path = os.path.join(OUTPUT_DIR, 'final_household_states.csv')
        try:
            final_households_df.to_csv(final_csv_path, index=False, float_format='%.5f')
            print(f"\nFinal household states (t={final_step}) saved to '{final_csv_path}'")
        except Exception as e:
            print(f"Error saving final household states CSV: {e}")


        # Print summary of final state
        print("\n--- Summary of Final State (t={final_step}) ---")
        n_final = len(final_households_df)
        if n_final > 0:
            # Use np.nansum and np.nanmean for robustness if states can be NaN (shouldn't happen here, but good practice)
            print(f"Vacant (Y_O=1): {np.nansum(final_households_df['Y_O'])} / {n_final} ({np.nanmean(final_households_df['Y_O'])*100:.1f}%)")
            print(f"For Sale (Y_S=1): {np.nansum(final_households_df['Y_S'])} / {n_final} ({np.nanmean(final_households_df['Y_S'])*100:.1f}%)")
            print(f"Repaired (Y_R=1): {np.nansum(final_households_df['Y_R'])} / {n_final} ({np.nanmean(final_households_df['Y_R'])*100:.1f}%)")

            # Group-level statistics at the final step
            print("\nFinal Group-level Statistics:")
            for group_id in sorted(final_households_df['group_id'].unique()):
                group_data = final_households_df[final_households_df['group_id'] == group_id]
                n_group = len(group_data)
                if n_group > 0:
                    print(f"\nGroup {group_id} (n={n_group}):")
                    print(f"  Vacant:   {np.nanmean(group_data['Y_O'])*100:.1f}%")
                    print(f"  For Sale: {np.nanmean(group_data['Y_S'])*100:.1f}%")
                    print(f"  Repaired: {np.nanmean(group_data['Y_R'])*100:.1f}%")
                    # Show final avg thresholds for the group (note: these were regenerated each step)
                    print(f"  Avg Final Thresholds: τ_O={np.nanmean(group_data['tau_O']):.3f}, τ_S={np.nanmean(group_data['tau_S']):.3f}, τ_R={np.nanmean(group_data['tau_R']):.3f}")
                    print(f"  Std Final Thresholds: τ_O={np.nanstd(group_data['tau_O']):.3f}, τ_S={np.nanstd(group_data['tau_S']):.3f}, τ_R={np.nanstd(group_data['tau_R']):.3f}")
                    # Show theoretical group means for comparison (ensure final_group_df is valid)
                else:
                     print(f"\nGroup {group_id}: No households in final data.")

        else:
            print("Final household data is empty.")
    else:
        print("Simulation did not produce results.")


if __name__ == "__main__":
    main()

"""#MCMC"""

# Set random seed for reproducibility4
numpyro.set_host_device_count(4)  # Utilize multiple devices if available - uncomment if needed
rng_key = random.PRNGKey(24)

# --- load_data function remains the same ---
def load_data(households_path, network_path, decisions_path):
    """
    Load the CSV data files and prepare for inference
    """
    print("Loading data...")

    if not os.path.exists(households_path):
        raise FileNotFoundError(f"Household data not found at: {households_path}")
    if not os.path.exists(network_path):
        print(f"Warning: Network data not found at: {network_path}. Proceeding without network structure.")
        network_df = None # Handle absence gracefully
    else:
        network_df = pd.read_csv(network_path)
    if not os.path.exists(decisions_path):
        raise FileNotFoundError(f"Decision data not found at: {decisions_path}")

    households_df = pd.read_csv(households_path)
    decisions_df = pd.read_csv(decisions_path)

    print(f"Loaded {len(households_df)} households and {len(decisions_df)} decision records.")
    if network_df is not None:
        print(f"Loaded {len(network_df)} network connections.")

    return households_df, network_df, decisions_df

# --- prepare_data_for_inference function remains the same ---
def prepare_data_for_inference(households_df, network_df, decisions_df):
    """
    Prepare data for MCMC inference with JAX/NumPyro, including historical states
    """
    start_time = time.time()
    print("Preparing data for inference...")

    # Get household IDs and create a mapping from ID to index
    household_ids = households_df['household_id'].unique() # Use unique IDs
    households_df = households_df.set_index('household_id').loc[household_ids].reset_index() # Ensure order matches unique IDs
    id_to_idx = {h_id: idx for idx, h_id in enumerate(household_ids)}
    n_households = len(household_ids)

    # Get unique timesteps and sort them
    timesteps = sorted(decisions_df['timestep'].unique())
    n_timesteps = len(timesteps)
    print(f"Found {n_households} households and {n_timesteps} timesteps.")

    # Get household attributes (ensure alignment with household_ids order)
    # IMPORTANT DIFFERENCE: MCMC uses individual features, Simulation uses group averages for threshold distribution
    income = households_df['income'].values
    resource_access = households_df['resource_access'].values
    damage_struct = households_df['damage_structural'].values
    damage_utility = households_df['damage_utility'].values

    # Create feature vectors with 2 dimensions based on individual data
    H = np.column_stack([income, resource_access])
    D = np.column_stack([damage_struct, damage_utility])

    # Get group ids if available (used for visualization, not model structure here)
    group_ids = None
    if 'group_id' in households_df.columns:
        group_ids = households_df['group_id'].values
        print(f"Found group IDs: {np.unique(group_ids)}")

    # Prepare the time series data
    # Initialize arrays to hold the state data across time
    Y_O = np.zeros((n_timesteps, n_households), dtype=np.float32)
    Y_S = np.zeros((n_timesteps, n_households), dtype=np.float32)
    Y_R = np.zeros((n_timesteps, n_households), dtype=np.float32)

    # Initialize arrays for raw influences
    m_O = np.zeros((n_timesteps, n_households), dtype=np.float32)
    m_S = np.zeros((n_timesteps, n_households), dtype=np.float32)
    m_R = np.zeros((n_timesteps, n_households), dtype=np.float32)

    # Find the appropriate columns for raw influences in decisions_df
    # Match the column names exported by the simulation
    raw_influence_cols = ['raw_influence_O', 'raw_influence_S', 'raw_influence_R']
    if not all(col in decisions_df.columns for col in raw_influence_cols):
         raise ValueError(f"Required raw influence columns ({raw_influence_cols}) not found in decisions_df")
    print(f"Found raw influences with columns: {raw_influence_cols}")

    # Map column names to standardized names (use simulation output names)
    state_cols = {
        'Y_O': 'occupancy_state',
        'Y_S': 'sales_state',
        'Y_R': 'recovery_state'
    }
    # Verify state columns exist
    for state, col in state_cols.items():
        if col not in decisions_df.columns:
            raise ValueError(f"Could not find required state column '{col}' for {state} in decisions_df.")
        print(f"Found state column for {state}: {col}")


    # Process by timestep for better memory efficiency
    for t_idx, t in enumerate(timesteps):
        # Get data for this timestep
        t_data = decisions_df[decisions_df['timestep'] == t].copy() # Use .copy() to avoid SettingWithCopyWarning

        # Ensure all households are present for the timestep, filling missing ones if necessary
        # (Crucial if the decisions_df doesn't have a row for every household at every timestep)
        t_data_map = t_data.set_index('household_id')
        # Create a temporary DataFrame with all household IDs for this timestep
        temp_df = pd.DataFrame(index=household_ids)

        # Map values, defaulting potentially missing ones (e.g., to 0 or NaN depending on logic)
        # Here, we assume if a household isn't in t_data, its state and influence are 0 for that timestep.
        # Adjust this assumption if needed based on how the simulation generates the data.
        for state, col in state_cols.items():
             target_array = {'Y_O': Y_O, 'Y_S': Y_S, 'Y_R': Y_R}[state]
             # Default missing to 0 (assuming state=0 if not recorded)
             target_array[t_idx, :] = temp_df.index.map(lambda hid: t_data_map.get(col, {}).get(hid, 0)).values
             # More robust mapping:
             mapped_values = temp_df.index.map(t_data_map.get(col, np.nan))
             target_array[t_idx, :] = np.nan_to_num(mapped_values, nan=0.0).astype(np.float32) # Replace NaN with 0

        # Map influences, defaulting missing to 0
        m_O[t_idx, :] = np.nan_to_num(temp_df.index.map(t_data_map.get(raw_influence_cols[0], np.nan)), nan=0.0).astype(np.float32)
        m_S[t_idx, :] = np.nan_to_num(temp_df.index.map(t_data_map.get(raw_influence_cols[1], np.nan)), nan=0.0).astype(np.float32)
        m_R[t_idx, :] = np.nan_to_num(temp_df.index.map(t_data_map.get(raw_influence_cols[2], np.nan)), nan=0.0).astype(np.float32)


    # Convert all data to JAX arrays
    Y_O_tensor = jnp.array(Y_O)
    Y_S_tensor = jnp.array(Y_S)
    Y_R_tensor = jnp.array(Y_R)

    m_O_tensor = jnp.array(m_O)
    m_S_tensor = jnp.array(m_S)
    m_R_tensor = jnp.array(m_R)

    H_tensor = jnp.array(H)
    D_tensor = jnp.array(D)

    # Return as dictionary
    data = {
        'Y_O': Y_O_tensor,
        'Y_S': Y_S_tensor,
        'Y_R': Y_R_tensor,
        'H': H_tensor, # Individual household features
        'D': D_tensor, # Individual household features
        'n_households': n_households,
        'n_timesteps': n_timesteps,
        'household_ids': household_ids, # Pass numpy array
        'timesteps': timesteps, # Pass sorted list/array
        'group_ids': jnp.array(group_ids) if group_ids is not None else None, # Pass JAX array or None (for visualization)
        'm_O': m_O_tensor,
        'm_S': m_S_tensor,
        'm_R': m_R_tensor,
        # NOTE: Smoothing factor is passed via fixed_params now
    }

    print(f"Data preparation completed in {time.time() - start_time:.2f} seconds")
    # Add checks for tensor shapes
    print(f"Shapes: Y_O={data['Y_O'].shape}, m_O={data['m_O'].shape}, H={data['H'].shape}, D={data['D'].shape}")
    return data


# --- calculate_adjusted_influences function remains the same ---
def calculate_adjusted_influences(raw_influences, omega):
    """
    Vectorized calculation of adjusted influences
    Adapted for JAX
    """
    # Stack raw influences into a single matrix for efficient multiplication
    # Ensure raw_influences values are correctly shaped (n_households,)
    m = jnp.stack([
        raw_influences['m_O'], # Shape: (n_households,)
        raw_influences['m_S'], # Shape: (n_households,)
        raw_influences['m_R']  # Shape: (n_households,)
    ], axis=-1)  # shape: (n_households, 3)

    # Calculate I = m @ omega^T (matrix multiplication)
    # omega should be (3, 3)
    # Use omega directly as simulation uses I = m @ omega
    I = jnp.matmul(m, omega)  # shape: (n_households, 3)

    # Return unpacked results
    return I[:, 0], I[:, 1], I[:, 2] # Each shape: (n_households,)


def model(data, prior_config, fixed_params=None):
    """
    NumPyro model implementation fully aligned with simulation logic:
    - Uses GROUP-LEVEL features to determine threshold distribution parameters
    - Latent variable Z ~ Normal(mu_g, sigma^2) where mu_g is group-specific
    - Threshold tau = sigmoid(Z)
    - Transition probability p = sigmoid(k * (I - tau))

    This version is compatible with JAX compilation by avoiding operations
    like jnp.unique() that require concrete values at compile time.

    Args:
        data: Dictionary containing model data (Y_*, m_*, H, D, group_ids, etc.)
        prior_config: Dictionary containing prior distribution configurations
        fixed_params: Dictionary containing parameters to fix (e.g., smoothing_factor, omega elements)
    """
    # Initialize fixed_params if None
    if fixed_params is None:
        fixed_params = {}

    n_households = data['n_households']
    n_timesteps = data['n_timesteps']

    # Extract observed data
    Y_O_obs = data['Y_O']  # Shape: (n_timesteps, n_households)
    Y_S_obs = data['Y_S']  # Shape: (n_timesteps, n_households)
    Y_R_obs = data['Y_R']  # Shape: (n_timesteps, n_households)

    # Raw influences from data
    m_O_obs = data['m_O']  # Shape: (n_timesteps, n_households)
    m_S_obs = data['m_S']  # Shape: (n_timesteps, n_households)
    m_R_obs = data['m_R']  # Shape: (n_timesteps, n_households)

    # Household features (Individual level)
    H = data['H']  # Shape: (n_households, n_features_H) e.g., (n_households, 2)
    D = data['D']  # Shape: (n_households, n_features_D) e.g., (n_households, 2)

    # Group IDs for each household - CRITICAL for hierarchical model
    group_ids = data['group_ids']  # Shape: (n_households,)

    # IMPORTANT: Instead of using jnp.unique(), we'll get the number of groups from data
    # This assumes that group IDs are integers from 0 to n_groups-1
    n_groups = data.get('n_groups', 5)  # Default to 5 groups if not provided

    # --- Sample or Fix Parameters ---
    def sample_or_fix(name, distribution, fixed_value=None):
        fixed_value_to_use = fixed_value if fixed_value is not None else fixed_params.get(name)
        if fixed_value_to_use is not None:
            value = jnp.asarray(fixed_value_to_use)
            # Handle scalar vs array parameters appropriately
            if isinstance(distribution, (dist.HalfCauchy, dist.Normal)) and value.ndim != 0:
                print(f"Warning: Fixed value for scalar parameter {name} has shape {value.shape}. Using value[0].")
                value = value.item()  # Convert 0-dim array to scalar
            return numpyro.deterministic(name, value)
        else:
            return numpyro.sample(name, distribution)

    # # Prior for omega matrix (correlation between decision dimensions)
    omega_chol = numpyro.sample(
        "omega_chol",
        dist.LKJCholesky(3, concentration=4)
    )

    # Convert Cholesky factor to correlation matrix
    omega_full = numpyro.deterministic(
        "omega_full",
        jnp.matmul(omega_chol, jnp.transpose(omega_chol))
    )
    # Smoothing factor parameter (get from fixed_params or use default)
    smoothing_factor = numpyro.sample("smoothing_factor", dist.Normal(5, 5))

    # Helper for vector parameters
    def sample_or_fix_vector(name, distribution, expected_dim, fixed_value=None):
        fixed_value_to_use = fixed_value if fixed_value is not None else fixed_params.get(name)
        if fixed_value_to_use is not None:
            value = jnp.asarray(fixed_value_to_use)
            if value.shape != (expected_dim,):
                print(f"Warning: Fixed value for {name} has shape {value.shape}, expected ({expected_dim},). Reshaping or check config.")
                # Attempt to reshape if possible, or raise error, or use default
                if value.size == expected_dim:
                    value = value.reshape((expected_dim,))
                else:
                    # Fallback or error - using zeros as a placeholder, but this should be fixed in config
                    print(f"Error: Size mismatch for fixed {name}. Using zeros. Check fixed_params.")
                    value = jnp.zeros((expected_dim,))
            return numpyro.deterministic(name, value)
        else:
            # Ensure the distribution has the correct event_shape
            if distribution.event_shape != (expected_dim,):
                print(f"Warning: Prior distribution for {name} has event_shape {distribution.event_shape}, expected ({expected_dim},). Adjusting prior.")
                # Recreate distribution with correct shape if possible
                if isinstance(distribution, dist.MultivariateNormal):
                    loc = distribution.loc
                    scale_tril = distribution.scale_tril
                    if loc.size == 1:
                        loc = loc * jnp.ones(expected_dim)
                    if scale_tril.shape == (1,1):
                        scale_tril = scale_tril.item() * jnp.eye(expected_dim)
                    elif scale_tril.shape == (expected_dim, expected_dim):
                        pass  # Already correct
                    else:  # Cannot easily fix scale_tril, use default prior
                        print(f"Error: Cannot adjust prior scale_tril for {name}. Using default prior.")
                        loc = prior_config[name.split('_')[1] + '_' + name.split('_')[0]]['mean'] * jnp.ones(expected_dim)
                        scale_tril = jnp.diag(prior_config[name.split('_')[1] + '_' + name.split('_')[0]]['std'] * jnp.ones(expected_dim))
                    distribution = dist.MultivariateNormal(loc=loc, scale_tril=scale_tril)
                else:
                    # For other distributions, might need specific handling
                    print(f"Error: Cannot automatically adjust prior shape for {name}. Check prior_config.")
                    # Fallback to a default prior shape
                    distribution = dist.Normal(0, 1).expand([expected_dim])
            return numpyro.sample(name, distribution)

    # Extract configurations and dimensions
    beta_H_config = prior_config['beta_H']
    beta_D_config = prior_config['beta_D']
    sigma_config = prior_config['sigma']
    dim_H = H.shape[1]  # Number of household features
    dim_D = D.shape[1]  # Number of damage features

    # Create means and scale matrices for MultivariateNormal priors
    beta_H_mean = beta_H_config['mean'] * jnp.ones(dim_H)
    beta_D_mean = beta_D_config['mean'] * jnp.ones(dim_D)
    # Ensure std is treated as scalar to create diagonal matrix
    beta_H_std = beta_H_config['std'] if isinstance(beta_H_config['std'], (int, float)) else beta_H_config['std'][0]
    beta_D_std = beta_D_config['std'] if isinstance(beta_D_config['std'], (int, float)) else beta_D_config['std'][0]
    beta_H_scale = jnp.diag(beta_H_std * jnp.ones(dim_H))
    beta_D_scale = jnp.diag(beta_D_std * jnp.ones(dim_D))

    # --- Sample Beta and Sigma parameters for O, S, R ---
    # Occupancy Parameters
    beta_H_O = sample_or_fix_vector("beta_H_O", dist.MultivariateNormal(loc=beta_H_mean, scale_tril=beta_H_scale), dim_H)
    beta_D_O = sample_or_fix_vector("beta_D_O", dist.MultivariateNormal(loc=beta_D_mean, scale_tril=beta_D_scale), dim_D)
    sigma_O = numpyro.sample("sigma_O", dist.InverseGamma(concentration=2.0, rate=1.0))

    # Sales Parameters
    beta_H_S = sample_or_fix_vector("beta_H_S", dist.MultivariateNormal(loc=beta_H_mean, scale_tril=beta_H_scale), dim_H)
    beta_D_S = sample_or_fix_vector("beta_D_S", dist.MultivariateNormal(loc=beta_D_mean, scale_tril=beta_D_scale), dim_D)
    sigma_S = numpyro.sample("sigma_S", dist.InverseGamma(concentration=2.0, rate=1.0))

    # Recovery Parameters
    beta_H_R = sample_or_fix_vector("beta_H_R", dist.MultivariateNormal(loc=beta_H_mean, scale_tril=beta_H_scale), dim_H)
    beta_D_R = sample_or_fix_vector("beta_D_R", dist.MultivariateNormal(loc=beta_D_mean, scale_tril=beta_D_scale), dim_D)
    sigma_R = numpyro.sample("sigma_R", dist.InverseGamma(concentration=2.0, rate=1.0 ))

    # --- GROUP-LEVEL HIERARCHICAL STRUCTURE ---
    # 1. Calculate group-level features (H_g, D_g) - averages within each group

    # Initialize arrays to hold group-level features
    H_g = jnp.zeros((n_groups, dim_H))
    D_g = jnp.zeros((n_groups, dim_D))

    # Define a function to calculate group features for one group
    def calculate_group_features(g_idx, carry):
        H_g, D_g = carry
        # Create a mask for households in this group
        group_mask = (group_ids == g_idx)  # Assumes group_ids are 0-indexed integers
        # Count households in this group
        n_in_group = jnp.sum(group_mask)
        # Safe mean calculation to handle empty groups
        safe_mean = lambda x: jnp.sum(x * group_mask[:, None], axis=0) / jnp.maximum(n_in_group, 1)
        # Calculate group average features
        H_g = H_g.at[g_idx].set(safe_mean(H))
        D_g = D_g.at[g_idx].set(safe_mean(D))
        return (H_g, D_g)

    # Use lax.fori_loop for efficient iteration over groups (JAX-compatible)
    H_g, D_g = lax.fori_loop(0, n_groups, calculate_group_features, (H_g, D_g))

    # Store group features as deterministic nodes for inspection
    H_g = numpyro.deterministic("H_g", H_g)
    D_g = numpyro.deterministic("D_g", D_g)

    # 2. Calculate group-level threshold distribution parameters (mu_g)
    # Shape: (n_groups,)
    mu_O_g = numpyro.deterministic("mu_O_g", jnp.dot(H_g, beta_H_O) + jnp.dot(D_g, beta_D_O))
    mu_S_g = numpyro.deterministic("mu_S_g", jnp.dot(H_g, beta_H_S) + jnp.dot(D_g, beta_D_S))
    mu_R_g = numpyro.deterministic("mu_R_g", jnp.dot(H_g, beta_H_R) + jnp.dot(D_g, beta_D_R))

    # 3. Map group-level parameters to individual households
    # Use the group_ids directly as indices into the group-level parameters
    # This assumes group_ids are integers from 0 to n_groups-1
    mu_O = mu_O_g[group_ids]
    mu_S = mu_S_g[group_ids]
    mu_R = mu_R_g[group_ids]

    # Store household-level means as deterministic nodes
    mu_O = numpyro.deterministic("mu_O", mu_O)
    mu_S = numpyro.deterministic("mu_S", mu_S)
    mu_R = numpyro.deterministic("mu_R", mu_R)

    # --- NON-CENTERED PARAMETERIZATION for latent threshold variables ---
    # Sample standard normal errors for each household and timestep
    epsilon_O = numpyro.sample("epsilon_O", dist.Normal(0, 1).expand([n_timesteps, n_households]))
    epsilon_S = numpyro.sample("epsilon_S", dist.Normal(0, 1).expand([n_timesteps, n_households]))
    epsilon_R = numpyro.sample("epsilon_R", dist.Normal(0, 1).expand([n_timesteps, n_households]))

    # Construct Z variables (latent variable) using non-centered parameterization
    # Z varies by household and timestep
    # Add timestep dimension to mu for broadcasting: mu[None, :] -> (1, n_households)
    Z_O = numpyro.deterministic("Z_O", mu_O[None, :] + sigma_O * epsilon_O)
    Z_S = numpyro.deterministic("Z_S", mu_S[None, :] + sigma_S * epsilon_S)
    Z_R = numpyro.deterministic("Z_R", mu_R[None, :] + sigma_R * epsilon_R)

    # Transform latent variables to thresholds using sigmoid
    tau_O_actual = numpyro.deterministic("tau_O_actual", jax.nn.sigmoid(Z_O))
    tau_S_actual = numpyro.deterministic("tau_S_actual", jax.nn.sigmoid(Z_S))
    tau_R_actual = numpyro.deterministic("tau_R_actual", jax.nn.sigmoid(Z_R))

    # --- Calculate Transition Probabilities and Likelihoods ---
    # Define function to calculate adjusted influences
    # Define function for calculating transition probabilities at each timestep
    def time_step_transition_probs(prev_states, t_idx):
        # prev_states contains (Y_O_{t-1}, Y_S_{t-1}, Y_R_{t-1})
        Y_O_prev, Y_S_prev, Y_R_prev = prev_states

        # Use provided raw influences from data for the current timestep
        raw_influences = {
            'm_O': m_O_obs[t_idx],
            'm_S': m_S_obs[t_idx],
            'm_R': m_R_obs[t_idx]
        }

        # Calculate adjusted influences I_t using the omega matrix
        I_O_t, I_S_t, I_R_t = calculate_adjusted_influences(raw_influences, omega_full)

        # Calculate transition probabilities using the logistic function
        # p = sigmoid(k * (I - tau))
        p_O_raw = jax.nn.sigmoid(smoothing_factor * (I_O_t - tau_O_actual[t_idx]))
        p_S_raw = jax.nn.sigmoid(smoothing_factor * (I_S_t - tau_S_actual[t_idx]))
        p_R_raw = jax.nn.sigmoid(smoothing_factor * (I_R_t - tau_R_actual[t_idx]))

        # Actual transition probability depends on previous state
        # If Y_{t-1}=1, the state remains 1 (probability 1)
        # If Y_{t-1}=0, the probability of transitioning to 1 is p_raw
        trans_prob_O = jnp.where(Y_O_prev == 0, p_O_raw, 1.0)
        trans_prob_S = jnp.where(Y_S_prev == 0, p_S_raw, 1.0)
        trans_prob_R = jnp.where(Y_R_prev == 0, p_R_raw, 1.0)

        # For the scan's carry state, use observed states to prevent drift
        Y_O_next = Y_O_obs[t_idx]
        Y_S_next = Y_S_obs[t_idx]
        Y_R_next = Y_R_obs[t_idx]

        # Return next carry and probabilities
        return (Y_O_next, Y_S_next, Y_R_next), (trans_prob_O, trans_prob_S, trans_prob_R)

    # Initial state at t=0 (observed)
    init_carry = (Y_O_obs[0], Y_S_obs[0], Y_R_obs[0])

    # Use lax.scan to compute transition probabilities for t = 1 to n_timesteps-1
    _, (trans_probs_O_all, trans_probs_S_all, trans_probs_R_all) = lax.scan(
        time_step_transition_probs,
        init_carry,
        jnp.arange(1, n_timesteps)
    )

    # --- Likelihood Calculation using Bernoulli ---
    # Observe data Y_t for t > 0 using calculated transition probabilities
    with numpyro.plate("households", n_households):
        # Loop over timesteps t=1 to n_timesteps-1
        for t in range(1, n_timesteps):
            # Clip probabilities to avoid numerical issues
            eps = jnp.finfo(jnp.float32).eps
            probs_O_t = jnp.clip(trans_probs_O_all[t-1], eps, 1.0 - eps)
            probs_S_t = jnp.clip(trans_probs_S_all[t-1], eps, 1.0 - eps)
            probs_R_t = jnp.clip(trans_probs_R_all[t-1], eps, 1.0 - eps)

            # Compare calculated probabilities with observed states
            numpyro.sample(
                f"Y_O_{t}",
                dist.Bernoulli(probs=probs_O_t),
                obs=Y_O_obs[t]
            )
            numpyro.sample(
                f"Y_S_{t}",
                dist.Bernoulli(probs=probs_S_t),
                obs=Y_S_obs[t]
            )
            numpyro.sample(
                f"Y_R_{t}",
                dist.Bernoulli(probs=probs_R_t),
                obs=Y_R_obs[t]
            )
# --- run_mcmc_inference function remains largely the same ---
def run_mcmc_inference(data, prior_config, fixed_params=None, num_samples=500, num_warmup=200, num_chains=1):
    """
    Run MCMC inference with NumPyro

    Args:
        data: Dictionary containing model data
        prior_config: Dictionary containing prior distribution configurations
        fixed_params: Dictionary containing parameters to fix and their values (incl. smoothing_factor)
        num_samples: Number of posterior samples to draw
        num_warmup: Number of warmup steps
        num_chains: Number of MCMC chains
    """
    print(f"Running MCMC inference with {num_samples} samples, {num_warmup} warmup steps, and {num_chains} chains...")

    # Define NUTS kernel with optimized settings
    nuts_kernel = NUTS(
        model,
        target_accept_prob=0.8,
        max_tree_depth=10 # Default is 10, adjust if needed based on diagnostics
    )

    # Run MCMC
    mcmc = MCMC(
        nuts_kernel,
        num_samples=num_samples,
        num_warmup=num_warmup,
        num_chains=num_chains,
        progress_bar=True,  # Show progress bar
        chain_method='parallel'
    )

    # Run inference with timing
    start_time = time.time()
    # Use a different key for each run if needed, or manage state carefully
    run_rng_key, _ = random.split(rng_key)
    mcmc.run(run_rng_key, data=data, prior_config=prior_config, fixed_params=fixed_params)
    inference_time = time.time() - start_time
    print(f"MCMC inference completed in {inference_time:.2f} seconds ({inference_time/60:.2f} minutes)")

    # Get posterior samples
    posterior_samples = mcmc.get_samples()

    # Print summary - can be verbose, use with caution for many parameters
    print("\nMCMC Summary (Top level parameters):")
    # Filter parameters to display - avoid large arrays like epsilon, Z, tau_actual
    params_to_summarize = [
        k for k, v in posterior_samples.items()
        if not k.startswith('epsilon_') and not k.startswith('Z_') and not k.startswith('tau_')
    ]
    # Explicitly add key params if needed and they exist
    key_params = ['omega_full', 'sigma_O', 'sigma_S', 'sigma_R',
                  'beta_H_O', 'beta_D_O', 'beta_H_S', 'beta_D_S', 'beta_H_R', 'beta_D_R']
    for kp in key_params:
        if kp in posterior_samples and kp not in params_to_summarize:
            params_to_summarize.append(kp)

    return mcmc, posterior_samples

# --- UPDATED visualize_group_thresholds function ---
def visualize_group_thresholds(posterior_samples, data, output_dir='.'):
    """
    Visualize the distributions of the inferred thresholds (tau_actual = sigmoid(Z))
    for each group based on posterior samples.

    Args:
        posterior_samples: Dictionary of posterior samples from MCMC.
        data: Dictionary of data used for MCMC (must include 'group_ids').
        output_dir: Directory to save plots.
    """
    print("Visualizing threshold distributions by group across all MCMC samples...")

    # Check if group_ids are available
    if data['group_ids'] is None:
        print("  No group IDs provided, skipping threshold visualization.")
        return

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Define states and corresponding threshold columns
    states = ['Occupancy (O)', 'Sales (S)', 'Recovery (R)']
    threshold_cols = ['tau_O_actual', 'tau_S_actual', 'tau_R_actual']

    # Get unique group IDs
    group_ids = np.unique(np.array(data['group_ids']))
    n_groups = len(group_ids)

    if n_groups == 0:
        print("  No groups found, skipping threshold visualization.")
        return

    # Create a color map for groups
    group_colors = plt.cm.tab10(np.linspace(0, 1, n_groups))

    # For each state (O, S, R)
    for state_idx, (state, threshold_col) in enumerate(zip(states, threshold_cols)):
        # Check if the threshold samples are available
        if threshold_col not in posterior_samples:
            print(f"  No samples found for {threshold_col}, checking for latent variables...")

            # Try to get latent variables and convert to thresholds
            latent_col = f"Z_{threshold_col[4]}"  # Extract decision type letter (O, S, R)
            if latent_col in posterior_samples:
                print(f"  Found latent variables {latent_col}, converting to thresholds...")
                # Convert Z to tau using sigmoid
                tau_samples = 1.0 / (1.0 + np.exp(-posterior_samples[latent_col]))
            else:
                print(f"  No data found for {threshold_col} or {latent_col}, skipping...")
                continue
        else:
            tau_samples = posterior_samples[threshold_col]

        # Create figure with subplots (one per group)
        fig, axes = plt.subplots(1, n_groups, figsize=(n_groups * 4, 5), sharey=True)
        if n_groups == 1:
            axes = np.array([axes])  # Ensure axes is iterable

        fig.suptitle(f'All MCMC Samples {state} Threshold (τ) Distribution by Group', fontsize=16, y=1.02)

        # For each group
        for g_idx, g_id in enumerate(sorted(group_ids)):
            ax = axes[g_idx]

            # Get household indices for this group
            group_mask = np.array(data['group_ids']) == g_id
            group_indices = np.where(group_mask)[0]

            if len(group_indices) == 0:
                ax.text(0.5, 0.5, "No households in group", ha='center', va='center')
                continue

            # Collect threshold values for this group across all MCMC samples and timesteps
            # Handle different possible shapes of tau_samples
            if tau_samples.ndim >= 3:  # [posterior_sample, timestep, household]
                # Extract thresholds for households in this group
                group_thresholds = tau_samples[:, :, group_indices].reshape(-1)
            elif tau_samples.ndim == 2:  # [timestep, household]
                group_thresholds = tau_samples[:, group_indices].reshape(-1)
            else:
                # Unexpected shape - try to adapt
                group_thresholds = np.array([])
                print(f"  Warning: Unexpected shape {tau_samples.shape} for {threshold_col}")
                for idx in range(tau_samples.shape[0]):
                    try:
                        if idx < tau_samples.shape[0] and group_indices[0] < tau_samples[idx].shape[0]:
                            group_thresholds = np.append(group_thresholds, tau_samples[idx][group_indices])
                    except:
                        continue

            if len(group_thresholds) > 0:
                # Plot KDE of thresholds from all MCMC samples
                sns.kdeplot(group_thresholds, ax=ax, color=group_colors[g_idx % len(group_colors)],
                           alpha=0.6, label='All MCMC samples')

                # Calculate statistics
                group_mean = np.mean(group_thresholds)
                group_std = np.std(group_thresholds)

                # Add reference lines
                ax.axvline(group_mean, color='red', linestyle='--', linewidth=1.5,
                          label=f'Mean={group_mean:.2f}')

                # Add sample size information
                stats_text = f"Mean: {group_mean:.3f}\nStd: {group_std:.3f}"
                ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=9,
                        verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

                ax.text(0.05, 0.05, f"n={len(group_thresholds)}", transform=ax.transAxes,
                       fontsize=8, verticalalignment='bottom')
            else:
                ax.text(0.5, 0.5, "No threshold data", ha='center', va='center')

            # Formatting
            ax.set_xlabel('Threshold Value (τ)')
            if g_idx == 0:
                ax.set_ylabel('Density')
            ax.set_title(f'Group {g_id}')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 6)
            ax.grid(False)
            ax.tick_params(axis='x', labelsize=9)
            ax.tick_params(axis='y', labelsize=9)
            ax.legend(fontsize=8, loc='best')

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        filename = os.path.join(output_dir, f'mcmc_samples_threshold_{threshold_col}_distribution.png')
        plt.savefig(filename, dpi=300)
        plt.show()
        plt.close(fig)
        print(f"  Saved: {filename}")

    print("Threshold visualization complete.")





# --- setup_prior_config function remains the same ---
def setup_prior_config():
    """
    Set up configurations for prior distributions using non-centered parameterization
    for the latent Gaussian threshold model
    """
    prior_config = {
        # Configuration for omega (correlation) priors - Off-diagonal elements
        'omega': {
            'mean': 0.0,    # Mean for Normal distribution
            'std': 0.3      # Standard deviation for Normal distribution (Weakly informative)
        },

        # Configurations for beta coefficients (effects of features on latent Z mean)
        # Assumes MultivariateNormal with diagonal covariance (independent priors for each feature effect)
        'beta_H': {
            'mean': 0.1,    # Mean for Normal distribution (Centered around zero effect a priori)
            'std': 1      # Standard deviation for Normal distribution
        },
        'beta_D': {
            'mean': 0.1,    # Mean for Normal distribution
            'std': 1      # Standard deviation for Normal distribution
        },

        # Configuration for sigma (scale of unexplained heterogeneity in latent Z)
        'sigma': {
            'scale': 0.5    # Scale parameter for Half-Cauchy distribution (Weakly informative, allows larger scales)
        },

    }
    print("Prior configuration set:")
    print(json.dumps(prior_config, indent=2))
    return prior_config

# --- main function updated to include smoothing_factor ---
def main():
    """Main function to run the inference pipeline"""
    # --- Configuration ---
    # Paths matching the simulation output
    HOUSEHOLDS_FILE = '/content/simulation_results_dynamic_thresholds/final_household_states.csv'
    NETWORK_FILE = 'small_baltimore_household.csv' # Assuming it's in the same directory or adjust path
    DECISIONS_FILE = '/content/simulation_results_dynamic_thresholds/decision_states_simulation_details.csv'
    OUTPUT_DIR = 'mcmc_inference_results_aligned' # Use a new dir name

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Results will be saved in: {OUTPUT_DIR}")

    # Log start time
    total_start_time = time.time()

    try:
        # --- Configuration ---
        print("\n--- Setting up Configurations ---")
        prior_config = setup_prior_config()

        # Define fixed parameters - MUST include smoothing_factor now
        fixed_params = {
             # Example: Fix entire omega matrix (ensure it's a 3x3 numpy array)
            #  'omega_matrix': np.array([
            #      [1.0, 0.0, 0.0],
            #      [0.0, 1.0, 0.0],
            #      [0.0, 0.0, 1.0]
            #  ]),


        }
        print("\nFixed Parameters:")
        # Print fixed params carefully, especially arrays
        for k, v in fixed_params.items():
            if isinstance(v, np.ndarray):
                 print(f"  {k}: array shape {v.shape}") # Avoid printing large arrays
            else:
                 print(f"  {k}: {v}")


        mcmc_params = {
             'num_samples': 1000, # Increase for better posterior estimates
             'num_warmup': 500,  # Increase for better adaptation
             'num_chains': 2     # Recommended: 2 or 4 for diagnostics (R_hat)
        }
        print("\nMCMC Parameters:")
        print(mcmc_params)


        # --- Load and Prepare Data ---
        print("\n--- Loading and Preparing Data ---")
        data_start_time = time.time()
        households_df, network_df, decisions_df = load_data(HOUSEHOLDS_FILE, NETWORK_FILE, DECISIONS_FILE)
        data = prepare_data_for_inference(households_df, network_df, decisions_df)
        print(f"Data loading and preparation completed in {time.time() - data_start_time:.2f} seconds")

        # Simple data validation
        if data['n_households'] == 0 or data['n_timesteps'] == 0:
             raise ValueError("No households or timesteps found in the data.")
        if data['Y_O'].shape != (data['n_timesteps'], data['n_households']):
             raise ValueError(f"Shape mismatch found in prepared data Y_O: {data['Y_O'].shape}, expected {(data['n_timesteps'], data['n_households'])}")
        if data['H'].shape[1] != prior_config['beta_H']['mean'].shape[0] if isinstance(prior_config['beta_H']['mean'], np.ndarray) else data['H'].shape[1] != 1:
             # Adjust check based on prior config structure if needed
             print(f"Warning: Number of features in H ({data['H'].shape[1]}) might not match prior dimension.")
        if data['D'].shape[1] != prior_config['beta_D']['mean'].shape[0] if isinstance(prior_config['beta_D']['mean'], np.ndarray) else data['D'].shape[1] != 1:
             print(f"Warning: Number of features in D ({data['D'].shape[1]}) might not match prior dimension.")


        # --- Run MCMC Inference ---
        print("\n--- Running MCMC Inference ---")
        mcmc_start_time = time.time()
        if mcmc_params['num_chains'] > 1 and jax.device_count() >= mcmc_params['num_chains']:
             numpyro.set_host_device_count(mcmc_params['num_chains'])
             print(f"Utilizing {mcmc_params['num_chains']} chains in parallel on {jax.device_count()} available devices.")
        elif mcmc_params['num_chains'] > 1:
             print(f"Warning: Requested {mcmc_params['num_chains']} chains, but only {jax.device_count()} devices available. Running fewer chains or sequentially.")
             # Adjust num_chains or let MCMC handle it (might run sequentially)
             # mcmc_params['num_chains'] = jax.device_count()


        mcmc, posterior_samples = run_mcmc_inference(
            data,
            prior_config,
            fixed_params=fixed_params, # Pass fixed params here
            num_samples=mcmc_params['num_samples'],
            num_warmup=mcmc_params['num_warmup'],
            num_chains=mcmc_params['num_chains']
        )
        print(f"MCMC inference completed in {time.time() - mcmc_start_time:.2f} seconds")

        # --- Save Posterior Samples ---
        samples_filename = os.path.join(OUTPUT_DIR, "posterior_samples.pkl")
        print(f"\n--- Saving Posterior Samples to {samples_filename} ---")
        try:
            # Convert JAX arrays in samples to NumPy before pickling for better compatibility
            posterior_samples_np = {k: np.array(v) for k, v in posterior_samples.items()}
            with open(samples_filename, "wb") as f:
                pickle.dump(posterior_samples_np, f)
            print("Posterior samples saved successfully.")
        except Exception as e:
            print(f"Error saving posterior samples: {e}")


        # --- Visualize Posterior Distributions ---
        print("\n--- Visualizing Results ---")
        viz_start_time = time.time()
        # Visualize group thresholds (tau_actual) using the NumPy version of samples
        visualize_group_thresholds(posterior_samples_np, data, OUTPUT_DIR)
        # Add other visualizations if needed (e.g., trace plots using arviz, pair plots for key params)
        print(f"Visualization completed in {time.time() - viz_start_time:.2f} seconds")

        total_time = time.time() - total_start_time
        print(f"\n--- Pipeline Finished ---")
        print(f"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")

        return posterior_samples_np # Return numpy samples

    except FileNotFoundError as e:
        print(f"\n--- ERROR: Data file not found ---")
        print(e)
        print("Please ensure the following files exist at the specified paths:")
        print(f" - {HOUSEHOLDS_FILE}")
        print(f" - {NETWORK_FILE}")
        print(f" - {DECISIONS_FILE}")
    except ValueError as e:
        print(f"\n--- ERROR: Value error during processing ---")
        print(e)
        import traceback
        traceback.print_exc()
    except Exception as e:
        print(f"\n--- An unexpected error occurred in main process ---")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("talk")

def plot_smoothing_factor_posterior(samples_file, output_dir='.', show_plot=True):
    """
    Plot the posterior distribution of the smoothing factor parameter.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        show_plot: Whether to display the plot (set to False for headless environments)
    """
    print(f"Loading posterior samples from {samples_file}...")

    try:
        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Extract the smoothing factor samples
        if 'smoothing_factor' in posterior_samples:
            smoothing_samples = posterior_samples['smoothing_factor']
            print(f"Found {len(smoothing_samples)} samples for smoothing_factor")
        else:
            print("Error: 'smoothing_factor' not found in posterior samples.")
            print(f"Available keys: {list(posterior_samples.keys())}")
            return

        # Create figure
        fig, ax = plt.subplots(figsize=(10, 6))

        # Plot the posterior distribution
        sns.histplot(smoothing_samples, kde=True, color='yellow', alpha=0.6, ax=ax, bins=30,
                    label='Posterior')

        # Plot the prior distribution for comparison
        prior_mean, prior_std = 5.0, 5.0
        x = np.linspace(max(0, prior_mean - 3*prior_std), prior_mean + 3*prior_std, 1000)
        prior_density = np.exp(-0.5 * ((x - prior_mean) / prior_std)**2) / (prior_std * np.sqrt(2 * np.pi))
        # Scale prior density to match histogram scale for better comparison
        scaling_factor = len(smoothing_samples) * (max(smoothing_samples) - min(smoothing_samples)) / 30
        ax.plot(x, prior_density * scaling_factor, 'r--', linewidth=2, label='Prior: N(5, 5)')

        # Calculate summary statistics
        mean_val = np.mean(smoothing_samples)
        median_val = np.median(smoothing_samples)
        std_val = np.std(smoothing_samples)
        credible_05 = np.percentile(smoothing_samples, 5)
        credible_95 = np.percentile(smoothing_samples, 95)

        # Add reference lines
        ax.axvline(mean_val, color='blue', linestyle='--', linewidth=1.5,
                  label=f'Mean: {mean_val:.2f}')
        ax.axvline(median_val, color='green', linestyle='-.', linewidth=1.5,
                  label=f'Median: {median_val:.2f}')

        # Add 90% credible interval
        ax.axvline(credible_05, color='darkblue', linestyle=':', linewidth=1.5)
        ax.axvline(credible_95, color='darkblue', linestyle=':', linewidth=1.5,
                  label=f'90% Credible Interval: [{credible_05:.2f}, {credible_95:.2f}]')

        # Add summary statistics as text
        stats_text = (
            f"Mean: {mean_val:.3f}\n"
            f"Median: {median_val:.3f}\n"
            f"Std Dev: {std_val:.3f}\n"
            f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
        )
        ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=12,
                verticalalignment='top', horizontalalignment='right',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))

        # Formatting
        ax.set_xlabel('Smoothing Factor (k)', fontsize=14)
        ax.set_ylabel('Density', fontsize=14)
        ax.set_title('Posterior Distribution of Smoothing Factor', fontsize=16)
        ax.legend(fontsize=12, loc='upper left')

        # Save the plot
        os.makedirs(output_dir, exist_ok=True)
        plot_path = os.path.join(output_dir, 'smoothing_factor_posterior.png')
        plt.tight_layout()
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

        return smoothing_samples, (mean_val, median_val, std_val, credible_05, credible_95)

    except Exception as e:
        print(f"Error plotting smoothing factor: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_omega_posterior(samples_file, output_dir='.', show_plot=True):
    """
    Plot the posterior distribution of the omega correlation matrix.

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plots
        show_plot: Whether to display the plots (set to False for headless environments)
    """
    print(f"Loading posterior samples for omega matrix from {samples_file}...")

    try:
        # Load the posterior samples
        with open(samples_file, 'rb') as f:
            posterior_samples = pickle.load(f)

        # Extract the omega_full samples
        if 'omega_full' in posterior_samples:
            omega_samples = posterior_samples['omega_full']
            print(f"Found omega samples with shape {omega_samples.shape}")
        else:
            print("Error: 'omega_full' not found in posterior samples.")
            print(f"Available keys: {list(posterior_samples.keys())}")
            return

        # Calculate the posterior mean omega matrix
        omega_mean = np.mean(omega_samples, axis=0)

        # Create directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # --- PLOT 1: Heatmap of posterior mean omega ---
        fig1, ax1 = plt.subplots(figsize=(8, 6))

        # Create the heatmap
        hm = sns.heatmap(omega_mean, annot=True, cmap='coolwarm', vmin=-1, vmax=1,
                    square=True, center=0, fmt='.2f', ax=ax1)

        # Add labels and title
        ax1.set_title('Posterior Mean of Omega Correlation Matrix', fontsize=14)
        ax1.set_xticklabels(['O', 'S', 'R'], fontsize=12)
        ax1.set_yticklabels(['O', 'S', 'R'], fontsize=12)

        # Add a caption explaining the matrix
        caption = "Omega Matrix: Cross-Influence Between Decision Dimensions\n" + \
                  "Elements show how influence in row dimension affects state in column dimension"
        fig1.text(0.5, 0.01, caption, ha='center', fontsize=10,
                 bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))

        # Save the heatmap
        heatmap_path = os.path.join(output_dir, 'omega_posterior_mean_heatmap.png')
        fig1.tight_layout(rect=[0, 0.05, 1, 0.95])
        fig1.savefig(heatmap_path, dpi=300, bbox_inches='tight')
        print(f"Heatmap saved to {heatmap_path}")

        if show_plot:
            plt.show()

        plt.close(fig1)

        # --- PLOT 2: Distribution of off-diagonal elements ---
        fig2, axes = plt.subplots(1, 3, figsize=(18, 6))

        # Define the off-diagonal elements to plot
        element_indices = [(0, 1), (0, 2), (1, 2)]  # (O,S), (O,R), (S,R)
        element_labels = [('O', 'S'), ('O', 'R'), ('S', 'R')]

        for i, ((row, col), ax) in enumerate(zip(element_indices, axes)):
            # Extract samples for this element
            element_samples = omega_samples[:, row, col]

            # Create distribution plot
            sns.histplot(element_samples, kde=True, ax=ax, color='teal', alpha=0.6, bins=25)

            # Calculate statistics
            mean_val = np.mean(element_samples)
            median_val = np.median(element_samples)
            std_val = np.std(element_samples)
            credible_05 = np.percentile(element_samples, 5)
            credible_95 = np.percentile(element_samples, 95)

            # Add reference lines
            ax.axvline(mean_val, color='blue', linestyle='--', linewidth=1.5,
                      label=f'Mean: {mean_val:.2f}')
            ax.axvline(median_val, color='green', linestyle='-.', linewidth=1.5,
                      label=f'Median: {median_val:.2f}')

            # Add 90% credible interval
            ax.axvline(credible_05, color='darkblue', linestyle=':', linewidth=1.5)
            ax.axvline(credible_95, color='darkblue', linestyle=':', linewidth=1.5,
                      label=f'90% CI: [{credible_05:.2f}, {credible_95:.2f}]')

            # Add summary statistics as text
            stats_text = (
                f"Mean: {mean_val:.3f}\n"
                f"Median: {median_val:.3f}\n"
                f"Std Dev: {std_val:.3f}\n"
                f"90% CI: [{credible_05:.3f}, {credible_95:.3f}]"
            )
            ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=10,
                    verticalalignment='top', horizontalalignment='right',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))

            # Formatting
            ax.set_xlabel(f'Correlation ω({element_labels[i][0]},{element_labels[i][1]})', fontsize=12)
            ax.set_ylabel('Density', fontsize=12)
            ax.set_title(f'Posterior of ω({element_labels[i][0]},{element_labels[i][1]})', fontsize=14)
            ax.legend(fontsize=10)

            # Set x-axis limits to valid correlation range
            ax.set_xlim(-1.1, 1.1)

        # Save the distributions plot
        dist_path = os.path.join(output_dir, 'omega_posterior_distributions.png')
        fig2.tight_layout()
        fig2.savefig(dist_path, dpi=300, bbox_inches='tight')
        print(f"Distributions plot saved to {dist_path}")

        if show_plot:
            plt.show()

        plt.close(fig2)

        # --- PLOT 4: Compare to target matrix if provided ---
        target_matrix = np.array([
            [1.0, 0.2, -0.2],  # Influence -> State O
            [0.2, 1.0, 0.1],   # Influence -> State S
            [-0.2, 0.1, 1.0]   # Influence -> State R
        ])

        fig4, ax4 = plt.subplots(figsize=(10, 6))

        # Create a plot to compare posterior mean with target
        labels = ['ω(O,O)', 'ω(O,S)', 'ω(O,R)', 'ω(S,O)', 'ω(S,S)', 'ω(S,R)', 'ω(R,O)', 'ω(R,S)', 'ω(R,R)']
        x = np.arange(len(labels))
        width = 0.35

        # Flatten matrices for bar chart
        posterior_mean_flat = omega_mean.flatten()
        target_flat = target_matrix.flatten()

        # Create bars
        ax4.bar(x - width/2, posterior_mean_flat, width, label='Posterior Mean', color='blue', alpha=0.7)
        ax4.bar(x + width/2, target_flat, width, label='Target Matrix', color='green', alpha=0.7)

        # Add labels and formatting
        ax4.set_ylabel('Correlation Value', fontsize=12)
        ax4.set_title('Comparison of Posterior Mean and Target Omega Matrix', fontsize=14)
        ax4.set_xticks(x)
        ax4.set_xticklabels(labels, rotation=45, ha='right')
        ax4.legend()
        ax4.set_ylim(-1.1, 1.1)
        ax4.axhline(0, color='black', linestyle='-', alpha=0.2)

        # Add grid lines
        ax4.grid(True, linestyle='--', alpha=0.3)

        plt.tight_layout()
        compare_path = os.path.join(output_dir, 'omega_posterior_vs_target.png')
        fig4.savefig(compare_path, dpi=300, bbox_inches='tight')
        print(f"Comparison plot saved to {compare_path}")

        if show_plot:
            plt.show()

        plt.close(fig4)

        return omega_samples, omega_mean

    except Exception as e:
        print(f"Error plotting omega matrix: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Adjust these paths to your saved posterior samples
    samples_file = "mcmc_inference_results_aligned/posterior_samples.pkl"
    output_dir = "mcmc_inference_results_aligned"

    # Run the plotting functions
    plot_smoothing_factor_posterior(samples_file, output_dir)
    plot_omega_posterior(samples_file, output_dir)
