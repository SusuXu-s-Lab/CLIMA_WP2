# -*- coding: utf-8 -*-
"""CLIMA_Co-evolving.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rbA-CE5UEwtl-GeimXGv2qFT0sTlK2QO
"""

!pip install python-geohash
!pip install pygeohash
# !pip install pyro-ppl
!pip install numpyro # Assuming numpyro is not strictly needed for this ABM part, will remove if not used

import pandas as pd
import numpy as np
import pyarrow.parquet as pq # Not used
from google.colab import drive # Not used if file is local
from datetime import datetime, timedelta # Not used
import joblib # Not used
import math, bisect, random # math, random used
from sklearn.model_selection import train_test_split, cross_val_score # Not used
from sklearn.linear_model import LogisticRegression # Not used
import pdb # Not used
from tqdm import tqdm # Not used in the final version, but useful for debugging
from itertools import combinations # Not used
import geohash # Not used in this script version
import networkx as nx
import scipy.stats as stats # Not used directly, np.random.normal is used
import scipy.signal as signal # Not used
from scipy.stats import beta, norm # norm from scipy.stats is used by name, but np.random.normal is more common
import matplotlib.pyplot as plt
import torch # Not used
import pickle # Not used
import json # Not used
import jax # Not used for ABM
import jax.numpy as jnp # Not used for ABM
from jax.experimental import sparse # Not used for ABM
from jax import random as jax_random, lax # Not used for ABM
import numpyro # Not used for ABM
import numpyro.distributions as dist # Not used for ABM
from numpyro.infer import MCMC, NUTS, Predictive # Not used for ABM
from scipy import sparse as scipy_sparse_matrix # Not used for ABM
import os
import time # Not used directly
import warnings # Not used
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.cm as cm
from sklearn.cluster import KMeans

"""#Simulation"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import os
from matplotlib.colors import LinearSegmentedColormap
from scipy.stats import norm
from sklearn.cluster import KMeans

# --- Data Preparation Functions ---

def assign_households_to_groups(households_df, num_groups=5, random_seed=None):
    """
    Assign households to Census Block Groups (CBGs) based on geographical proximity using K-means.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with lat/lon coordinates.
    num_groups : int, default=5
        Number of groups to create.
    random_seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added 'group_id' column.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.
    """
    print(f"Assigning households to {num_groups} groups...")

    # Set random seed if provided for KMeans reproducibility
    if random_seed is not None:
        kmeans_random_state = random_seed
    else:
        kmeans_random_state = None # Allow KMeans to use its default random state

    # Extract coordinates
    coords = households_df[['lat', 'lon']].values

    # Use K-means clustering
    # Note: n_init='auto' is recommended in newer scikit-learn versions
    kmeans = KMeans(n_clusters=num_groups, random_state=kmeans_random_state, n_init='auto')
    group_assignments = kmeans.fit_predict(coords)

    # Get cluster centroids
    centroids = kmeans.cluster_centers_

    # Add group_id to dataframe
    households_df['group_id'] = group_assignments

    # Create a dictionary mapping group_id to centroid
    group_centroids = {i: (centroids[i, 0], centroids[i, 1]) for i in range(num_groups)}

    # Print summary
    print(f"Group assignments complete. Households per group:")
    group_counts = households_df['group_id'].value_counts().sort_index()
    for group_id, count in group_counts.items():
        print(f"  Group {group_id}: {count} households")

    return households_df, group_centroids

def generate_group_attributes(households_df, group_centroids):
    """
    Generate group-level attributes by aggregating household characteristics.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_centroids : dict
        Dictionary mapping group_id to (lat, lon) centroid.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with group-level attributes (means, counts, feature vectors).
    """
    print("Generating group-level attributes...")

    # Get unique group IDs
    group_ids = sorted(households_df['group_id'].unique()) # Sort for consistent order

    # Initialize lists for group attributes
    group_data = []

    # Generate group attributes by aggregating household attributes
    for g_id in group_ids:
        # Get households in this group
        group_households = households_df[households_df['group_id'] == g_id]

        # Calculate group-level attributes (aggregations)
        avg_income = group_households['income'].mean()
        avg_resource_access = group_households['resource_access'].mean()
        avg_damage_structural = group_households['damage_structural'].mean()
        avg_damage_utility = group_households['damage_utility'].mean()

        # Get centroid
        centroid_lat, centroid_lon = group_centroids[g_id]

        # Create group feature vectors (needed for the hierarchical threshold model)
        H_g = np.array([avg_income, avg_resource_access])  # Socioeconomic features
        D_g = np.array([avg_damage_structural, avg_damage_utility])  # Damage features

        # Store group data
        group_data.append({
            'group_id': g_id,
            'centroid_lat': centroid_lat,
            'centroid_lon': centroid_lon,
            'avg_income': avg_income,
            'avg_resource_access': avg_resource_access,
            'avg_damage_structural': avg_damage_structural,
            'avg_damage_utility': avg_damage_utility,
            'household_count': len(group_households),
            'H_g': H_g, # Store the numpy array directly
            'D_g': D_g  # Store the numpy array directly
        })

    # Create DataFrame
    group_df = pd.DataFrame(group_data).set_index('group_id') # Use group_id as index

    return group_df

def generate_household_attributes(household_ids, household_locs, seed=None):
    """
    Generate household attributes for simulation using specified probabilities.

    Parameters:
    -----------
    household_ids : list
        List of household IDs.
    household_locs : dict
        Dictionary mapping household ID to (lat, lon) location.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with household attributes (ID, lat, lon, income, etc.).
    """
    print("Generating household attributes...")

    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)

    n_households = len(household_ids)

    # Define probabilities for each attribute
    p_income = 0.0            # Probability of high income
    p_resource_access = 0.0   # Probability of good resource access
    p_damage_structural = 0.0 # Probability of structural damage
    p_damage_utility = 1.0    # Probability of utility damage

    # Generate attributes directly using numpy's random choice
    incomes = np.random.choice([0, 1], size=n_households, p=[1 - p_income, p_income])
    resource_access = np.random.choice([0, 1], size=n_households, p=[1 - p_resource_access, p_resource_access])
    damage_structural = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_structural, p_damage_structural])
    damage_utility = np.random.choice([0, 1], size=n_households, p=[1 - p_damage_utility, p_damage_utility])

    # Extract location data efficiently
    lats = np.array([household_locs[h_id][0] for h_id in household_ids])
    lons = np.array([household_locs[h_id][1] for h_id in household_ids])

    # Create DataFrame
    households_df = pd.DataFrame({
        'household_id': household_ids,
        'lat': lats,
        'lon': lons,
        'income': incomes,                  # 1=high, 0=low
        'resource_access': resource_access, # 1=good, 0=poor
        'damage_structural': damage_structural, # 1=damaged, 0=not damaged
        'damage_utility': damage_utility,   # 1=damaged, 0=not damaged
    })

    return households_df

def initialize_decision_states(households_df, damage_threshold, seed=None):
    """
    Initialize household decision states Y = [Y^O, Y^R] and tracking columns.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data.
    damage_threshold : float
        Threshold for determining initial recovery state based on average damage.
    seed : int, optional
        Random seed for reproducibility of initial vacant selection.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added decision state columns (Y_O, Y_R) and tracking columns.
    """
    print("Initializing decision states...")
    n_households = len(households_df)

    # Set random seed if provided for initial state randomization
    if seed is not None:
        np.random.seed(seed)

    # Initialize all states to 0
    occupancy_state = np.zeros(n_households, dtype=np.int8) # Y_O: 0=occupied, 1=vacant

    # Randomly select a small number of initial vacant properties
    num_vacant = np.random.randint(10, 25) # e.g., 10-25 vacant households initially

    if n_households > 0: # Avoid errors with empty dataframes
        vacant_indices = np.random.choice(n_households, num_vacant, replace=False)
        # Set selected indices to 1
        occupancy_state[vacant_indices] = 1

    # Initialize recovery state based on damage levels
    # Y_R: 1=repaired/undamaged, 0=needs repair
    # If average damage is *below* threshold, it's considered 'repaired' or okay initially.
    avg_damage = (households_df['damage_structural'] + households_df['damage_utility']) / 2
    recovery_state = (avg_damage < damage_threshold).astype(np.int8)

    # Add to the dataframe
    households_df['Y_O'] = occupancy_state
    households_df['Y_R'] = recovery_state

    # Initialize previous states for tracking recent transitions (state at t-1)
    # Starts with the current initial state
    households_df['Y_O_prev'] = occupancy_state.copy()
    households_df['Y_R_prev'] = recovery_state.copy()

    # Print summary of initial states
    print(f"Initial states summary:")
    print(f"  Vacant households (Y_O=1): {np.sum(occupancy_state == 1)} / {n_households}")
    print(f"  Repaired/undamaged (Y_R=1): {np.sum(recovery_state == 1)} / {n_households}")

    return households_df

# --- Network and Influence Functions ---

def create_initial_network_matrices(households_df, network_df):
    """
    Create initial weighted adjacency matrices for bonding (W_B) and bridging (W_R) networks.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data (must include 'household_id').
    network_df : DataFrame
        DataFrame containing network connections ('home_1', 'home_2', 'type', 'avg_link').
        'type' should be 1 for bonding, 0 for bridging (as per typical data, though text says 2).
        Adjust if 'type' has different conventions in your data.

    Returns:
    --------
    W_B : ndarray
        Initial weighted adjacency matrix for bonding network (n_households x n_households).
    W_R : ndarray
        Initial weighted adjacency matrix for bridging network (n_households x n_households).
    id_to_idx : dict
        Dictionary mapping household ID to matrix index.
    idx_to_id : dict
        Dictionary mapping matrix index to household ID.
    """
    print("Creating initial network adjacency matrices...")
    # Get household IDs from the simulation set and create mappings
    # This ensures matrices are sized based on households actually in the simulation
    household_ids = households_df['household_id'].unique() # Use unique IDs from the provided df
    id_to_idx = {h_id: idx for idx, h_id in enumerate(household_ids)}
    idx_to_id = {idx: h_id for h_id, idx in id_to_idx.items()}

    n_households = len(household_ids)
    print(f"  Matrix dimensions will be {n_households}x{n_households} based on input households_df.")

    # Initialize adjacency matrices with zeros
    W_B = np.zeros((n_households, n_households))  # Bonding network
    W_R = np.zeros((n_households, n_households))  # Bridging network

    # Populate matrices from network_df
    skipped_connections = 0
    for _, row in network_df.iterrows():
        home_1 = row['home_1']
        home_2 = row['home_2']
        weight = row['avg_link']
        conn_type = row['type'] # Make sure this matches your data's convention

        # Get indices, skip if household not in our simulation set
        if home_1 in id_to_idx and home_2 in id_to_idx:
            i = id_to_idx[home_1]
            j = id_to_idx[home_2]

            # Add edge weight based on type (undirected)
            if conn_type == 1:  # Bonding
                W_B[i, j] = weight
                W_B[j, i] = weight
            elif conn_type == 0: # Bridging (assuming 0 for bridging as common)
                                 # If your data uses 2 for bridging, change 0 to 2.
                W_R[i, j] = weight
                W_R[j, i] = weight
            # else: ignore other types if any
        else:
            skipped_connections += 1
            # Optional: print which households are being skipped
            # if home_1 not in id_to_idx: print(f"    Skipping connection involving {home_1} (not in id_to_idx)")
            # if home_2 not in id_to_idx: print(f"    Skipping connection involving {home_2} (not in id_to_idx)")


    if skipped_connections > 0:
         print(f"  Warning: Skipped {skipped_connections} connections involving households not in the current simulation household set.")
    print("Network matrices created.")
    return W_B, W_R, id_to_idx, idx_to_id

def update_network_weights(W_B, W_R, households_df, id_to_idx, ar1_params):
    """
    Update network weights based on household decision states using AR(1) process.

    Parameters:
    -----------
    W_B : ndarray
        Current weighted adjacency matrix for bonding network.
    W_R : ndarray
        Current weighted adjacency matrix for bridging network.
    households_df : DataFrame
        DataFrame containing household data with current states (Y_O, Y_R).
        It's assumed that households_df's order matches the id_to_idx mapping.
    id_to_idx : dict
        Dictionary mapping household ID to matrix index.
    ar1_params : dict
        Dictionary containing AR(1) parameters for network weight evolution.

    Returns:
    --------
    W_B_new : ndarray
        Updated weighted adjacency matrix for bonding network.
    W_R_new : ndarray
        Updated weighted adjacency matrix for bridging network.
    """
    # Ensure households_df is indexed by household_id for quick lookup if needed,
    # or ensure its order corresponds to how Y_O and Y_R are extracted.
    # For simplicity, this function assumes Y_O and Y_R are extracted in an order
    # that corresponds to matrix indices 0 to n_households-1.

    # Extract parameters for AR(1) process
    rho_mig_B = ar1_params['rho_mig_B']
    rho_mig_R = ar1_params['rho_mig_R']
    rho_cop_B = ar1_params['rho_cop_B']
    rho_cop_R = ar1_params['rho_cop_R']

    mu_mig_B = ar1_params['mu_mig_B']
    mu_mig_R = ar1_params['mu_mig_R']
    mu_cop_B = ar1_params['mu_cop_B']
    mu_cop_R = ar1_params['mu_cop_R']

    sigma_mig_B = ar1_params['sigma_mig_B']
    sigma_mig_R = ar1_params['sigma_mig_R']
    sigma_cop_B = ar1_params['sigma_cop_B']
    sigma_cop_R = ar1_params['sigma_cop_R']

    # Get household states directly from the DataFrame, assuming its order is consistent
    # with the matrix indices (0 to n-1).
    # If households_df is not guaranteed to be in this order, it must be re-indexed or mapped.
    # For this implementation, we rely on households_df being consistently ordered or
    # Y_O, Y_R being extracted based on `idx_to_id` to match matrix indices.

    # A safer way to get states if households_df order is not guaranteed:
    # n_households = W_B.shape[0]
    # Y_O_map = households_df.set_index('household_id')['Y_O']
    # Y_R_map = households_df.set_index('household_id')['Y_R']
    # idx_to_id_map = {v: k for k, v in id_to_idx.items()} # if not already idx_to_id
    # Y_O = np.array([Y_O_map[idx_to_id_map[i]] for i in range(n_households)])
    # Y_R = np.array([Y_R_map[idx_to_id_map[i]] for i in range(n_households)])
    # However, typically the main households_df is kept in a consistent order.

    Y_O = households_df['Y_O'].values # Assumes order matches matrix indices
    Y_R = households_df['Y_R'].values # Assumes order matches matrix indices

    # Create copies of the current weight matrices
    W_B_new = W_B.copy()
    W_R_new = W_R.copy()

    n_households = W_B.shape[0] # Number of households in the matrix

    # For each household pair, determine regime and update weights
    for i in range(n_households):
        for j in range(i+1, n_households):  # Process each pair once (symmetric update)
            # Skip if no connection exists in either network
            if W_B[i, j] == 0 and W_R[i, j] == 0:
                continue

            # Determine regime based on decision states
            is_migration = (Y_O[i] == 1 or Y_O[j] == 1)

            if is_migration:
                # Migration-Weakening Regime
                if W_B[i, j] > 0:
                    epsilon_B = np.random.normal(0, sigma_mig_B)
                    W_B_new[i, j] = (1 - rho_mig_B) * mu_mig_B + rho_mig_B * W_B[i, j] + epsilon_B
                    W_B_new[i, j] = max(0, W_B_new[i, j]) # Ensure [0,1]
                    W_B_new[j, i] = W_B_new[i, j]

                if W_R[i, j] > 0:
                    epsilon_R = np.random.normal(0, sigma_mig_R)
                    W_R_new[i, j] = (1 - rho_mig_R) * mu_mig_R + rho_mig_R * W_R[i, j] + epsilon_R
                    W_R_new[i, j] = max(0, W_R_new[i, j]) # Ensure [0,1]
                    W_R_new[j, i] = W_R_new[i, j]

            elif Y_R[i] == 1 and Y_R[j] == 1: # Neither vacant AND both repaired
                # Co-Repair-Strengthening Regime
                if W_B[i, j] > 0:
                    epsilon_B = np.random.normal(0, sigma_cop_B)
                    W_B_new[i, j] = (1 - rho_cop_B) * mu_cop_B + rho_cop_B * W_B[i, j] + epsilon_B
                    W_B_new[i, j] = max(0, W_B_new[i, j]) # Ensure [0,1]
                    W_B_new[j, i] = W_B_new[i, j]

                if W_R[i, j] > 0:
                    epsilon_R = np.random.normal(0, sigma_cop_R)
                    W_R_new[i, j] = (1 - rho_cop_R) * mu_cop_R + rho_cop_R * W_R[i, j] + epsilon_R
                    W_R_new[i, j] = max(0, W_R_new[i, j]) # Ensure [0,1]
                    W_R_new[j, i] = W_R_new[i, j]

            # Else: Baseline Regime (no change, already handled by copying W_B, W_R)

    return W_B_new, W_R_new

def calculate_raw_influences(households_df, W_B, W_R, id_to_idx):
    """
    Calculate raw social influence m^s for each decision dimension s.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with current states. Order must match matrix indices.
    W_B : ndarray
        Weighted adjacency matrix for bonding network.
    W_R : ndarray
        Weighted adjacency matrix for bridging network.
    id_to_idx : dict
        Dictionary mapping household ID to matrix index (used to confirm household count).

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added raw influence columns ('m_O', 'm_R').
    """
    n_households = W_B.shape[0] # Number of households in the matrix

    # Get current decision states (ensure order matches matrix indices)
    Y_O = households_df['Y_O'].values
    Y_R = households_df['Y_R'].values

    # Initialize raw influence vectors
    m_O = np.zeros(n_households)
    m_R = np.zeros(n_households)

    # Calculate combined raw influences using matrix operations for efficiency
    influence_O = (W_B @ Y_O) + (W_R @ Y_O)
    influence_R = (W_B @ Y_R) + (W_R @ Y_R)

    # Total weights for normalization (sum of weights for each household)
    total_weights_B = W_B.sum(axis=1)
    total_weights_R = W_R.sum(axis=1)
    total_weights = total_weights_B + total_weights_R

    # Normalize influences
    valid_weights_mask = total_weights > 0
    m_O[valid_weights_mask] = influence_O[valid_weights_mask] / total_weights[valid_weights_mask]
    m_R[valid_weights_mask] = influence_R[valid_weights_mask] / total_weights[valid_weights_mask]

    # Store raw influences in dataframe
    households_df['m_O'] = m_O
    households_df['m_R'] = m_R

    return households_df

def calculate_adjusted_influences(households_df, omega_matrix):
    """
    Calculate correlation-adjusted influences I^s = Omega * m. (Corrected: I = m @ Omega.T if Omega is defined as in text)
    The text defines I_h(t) = Omega * m_h(t). If m_h(t) is a column vector [m_O, m_R]^T,
    and Omega is [[1, w_OR], [w_RO, 1]], then I = Omega @ m.
    If m is stored as rows in households_df (m_O, m_R), and we want I_O, I_R:
    I_O = 1*m_O + w_OR*m_R
    I_R = w_RO*m_O + 1*m_R
    This is equivalent to [m_O, m_R] @ [[1, w_RO], [w_OR, 1]] = [m_O, m_R] @ Omega.T
    Or, if m is a row vector [m_O, m_R], then I = m @ Omega_transposed_from_text.
    Let's assume Omega is given as [[omega_OO, omega_OR], [omega_RO, omega_RR]]
    Then I_O = omega_OO * m_O + omega_OR * m_R
          I_R = omega_RO * m_O + omega_RR * m_R
    This is m @ Omega.T if m is [m_O, m_R] row vector.
    Or, if Omega is [[1, w_OR], [w_RO, 1]], and m is [m_O, m_R] (row vector),
    then I = m @ Omega_prime where Omega_prime = [[1, w_RO], [w_OR, 1]]
    The paper states I_h(t) = Omega * m_h(t), implying m_h(t) is a column vector.
    The python code was m @ omega_matrix. If m is (N,2) and omega_matrix is (2,2), then I is (N,2).
    I_O = m_O * omega_00 + m_R * omega_10
    I_R = m_O * omega_01 + m_R * omega_11
    This means omega_matrix should be [[1, w_RO], [w_OR, 1]] if omega_matrix is Omega from text.
    Let's stick to the existing code's matrix multiplication logic: I = m @ omega_matrix.
    So, if omega_matrix is [[w_OO, w_RO], [w_OR, w_RR]], then
    I_O = m_O * w_OO + m_R * w_OR
    I_R = m_O * w_RO + m_R * w_RR
    This matches the paper's equation: I_s = sum_{s'} omega_ss' * m_s'
    if omega_matrix is indexed as omega_matrix[s', s].
    The paper's Omega = [[1, w_OR], [w_RO, 1]].
    I_O = 1*m_O + w_OR*m_R
    I_R = w_RO*m_O + 1*m_R
    This implies omega_matrix should be [[1, w_RO], [w_OR, 1]] for the m @ omega_matrix calculation.
    The provided OMEGA_MATRIX = np.array([[1.0, -0.3], [-0.1, 1.0]])
    If this is the Omega from the paper, then:
    w_OR = -0.3 (influence of R on O)
    w_RO = -0.1 (influence of O on R)
    The code `I = m @ omega_matrix` will calculate:
    I_O = m_O * 1.0 + m_R * (-0.1)
    I_R = m_O * (-0.3) + m_R * 1.0
    This means the omega_matrix in code should be Omega.T from the paper's definition.
    Let's assume the omega_matrix passed to the function is already transposed if necessary,
    or that the definition of omega_matrix in the main script matches this usage.
    The current code is `I = m @ omega_matrix`.
    If `omega_matrix` is `[[omega_OO, omega_RO], [omega_OR, omega_RR]]`
    then `I_O = m_O * omega_OO + m_R * omega_OR`
    and `I_R = m_O * omega_RO + m_R * omega_RR`.
    This is `I_s = sum_{s'} m_{s'} * Omega_{s's}`.
    The paper has `I_s = sum_{s'} Omega_{ss'} * m_{s'}`.
    So, the `omega_matrix` in code should be the transpose of the paper's Omega.
    The `OMEGA_MATRIX` in `main()` is `[[1.0, -0.3], [-0.1, 1.0]]`.
    If paper's Omega is `[[O_OO, O_OR], [O_RO, O_RR]] = [[1, w_OR], [w_RO, 1]]`.
    Then code's `omega_matrix` should be `[[O_OO, O_RO], [O_OR, O_RR]] = [[1, w_RO], [w_OR, 1]]`.
    So if `OMEGA_MATRIX` in `main()` is `[[1, w_OR_param], [w_RO_param, 1]]`,
    then `w_OR_param = -0.3` and `w_RO_param = -0.1`.
    The code will compute:
    I_O = m_O * 1.0 + m_R * w_RO_param
    I_R = m_O * w_OR_param + m_R * 1.0
    This matches:
    I_O = m_O * Omega_OO + m_R * Omega_RO
    I_R = m_O * Omega_OR + m_R * Omega_RR
    Which is `I_s = sum_{s'} m_{s'} * Omega_{s's}`. This is consistent.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with raw influences ('m_O', 'm_R').
    omega_matrix : ndarray
        2x2 matrix. If Omega_text is the paper's matrix, then omega_matrix here is Omega_text.T.
        Or, if omega_matrix is [[w_OO, w_RO], [w_OR, w_RR]], then
        I_O = m_O*w_OO + m_R*w_OR
        I_R = m_O*w_RO + m_R*w_RR

    Returns:
    --------
    households_df : DataFrame
        DataFrame with added adjusted influence columns ('I_O', 'I_R').
    """
    # Get raw influences
    m_O = households_df['m_O'].values
    m_R = households_df['m_R'].values

    # Stack raw influences into a matrix: shape (n_households, 2)
    m_vector = np.vstack([m_O, m_R]).T # Each row is [m_O_h, m_R_h]

    # Calculate adjusted influences I = m_vector @ omega_matrix
    # If omega_matrix = [[O_OO, O_RO], [O_OR, O_RR]] (transpose of paper's Omega)
    # Then I_O = m_O * O_OO + m_R * O_OR
    #      I_R = m_O * O_RO + m_R * O_RR
    # This matches I_s = sum_{s'} m_{s'} * Omega_{s's}
    # The paper's Omega is [[1, w_OR], [w_RO, 1]].
    # So, omega_matrix in code should be [[1, w_RO], [w_OR, 1]].
    # If OMEGA_MATRIX in main is [[1.0, -0.3 (w_OR)], [-0.1 (w_RO), 1.0]],
    # then the code's omega_matrix should be [[1.0, -0.1], [-0.3, 1.0]].
    # Let's assume omega_matrix is passed correctly as per the paper's definition:
    # Omega = [[1, w_OR], [w_RO, 1]]
    # I_O = 1*m_O + w_OR*m_R
    # I_R = w_RO*m_O + 1*m_R
    # This is (Omega @ m_vector.T).T
    # Or, if m_vector is (N,2) and we want I (N,2) where I[h,0]=I_O_h, I[h,1]=I_R_h
    # I = m_vector @ omega_matrix_transposed_from_paper
    # The current code `I = m_vector @ omega_matrix` implies omega_matrix is this transposed version.
    # Let's keep the code as is and ensure OMEGA_MATRIX in main is defined as this transposed version.
    # OMEGA_MATRIX = np.array([[1.0, -0.3], [-0.1, 1.0]])
    # This means: w_OO=1.0, w_RO=-0.3, w_OR=-0.1, w_RR=1.0
    # I_O = m_O * 1.0 + m_R * (-0.1)
    # I_R = m_O * (-0.3) + m_R * 1.0
    # This implies the provided OMEGA_MATRIX is already structured for m @ OMEGA_MATRIX.

    I_adj = m_vector @ omega_matrix

    # Unpack results and store in dataframe
    households_df['I_O'] = I_adj[:, 0]
    households_df['I_R'] = I_adj[:, 1]

    return households_df

# --- Threshold Functions ---

def initialize_group_lambda(group_df, threshold_params):
    """
    Initialize group-level logit thresholds lambda (λ_g,0^s) for time t=0.
    Drawn from the stationary distribution of the AR(1) process.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group-level attributes (H_g, D_g). Index must be 'group_id'.
    threshold_params : dict
        Dictionary of threshold model parameters.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with added lambda columns for t=0 and long-term mean columns.
    """
    print("Initializing group-level lambda values at t=0...")

    beta_H_O = threshold_params['beta_H_O']
    beta_D_O = threshold_params['beta_D_O']
    beta_H_R = threshold_params['beta_H_R']
    beta_D_R = threshold_params['beta_D_R']

    phi_O = threshold_params['phi_O']
    phi_R = threshold_params['phi_R']
    sigma_epsilon_O = threshold_params['sigma_epsilon_O']
    sigma_epsilon_R = threshold_params['sigma_epsilon_R']

    # Avoid division by zero or negative sqrt if phi is 1 or >1
    if abs(phi_O) >= 1: stationary_var_O = sigma_epsilon_O**2 # Or handle as error/warning
    else: stationary_var_O = sigma_epsilon_O**2 / (1 - phi_O**2)
    if abs(phi_R) >= 1: stationary_var_R = sigma_epsilon_R**2
    else: stationary_var_R = sigma_epsilon_R**2 / (1 - phi_R**2)

    stationary_std_O = np.sqrt(max(0, stationary_var_O)) # max(0,...) for safety
    stationary_std_R = np.sqrt(max(0, stationary_var_R))

    group_df['mu_bar_O'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_O) + np.dot(row['D_g'], beta_D_O), axis=1)
    group_df['mu_bar_R'] = group_df.apply(lambda row: np.dot(row['H_g'], beta_H_R) + np.dot(row['D_g'], beta_D_R), axis=1)

    group_df['lambda_O_0'] = np.random.normal(group_df['mu_bar_O'], stationary_std_O)
    group_df['lambda_R_0'] = np.random.normal(group_df['mu_bar_R'], stationary_std_R)

    print("  Group lambda values initialized.")
    return group_df

def update_group_lambda(group_df, t, threshold_params):
    """
    Update group-level logit thresholds lambda (λ_g,t^s) using AR(1) process.

    Parameters:
    -----------
    group_df : DataFrame
        DataFrame containing group lambda values at time t-1.
    t : int
        Current time step.
    threshold_params : dict
        Dictionary of threshold model parameters.

    Returns:
    --------
    group_df : DataFrame
        DataFrame with updated lambda columns for time t.
    """
    phi_O = threshold_params['phi_O']
    phi_R = threshold_params['phi_R']
    sigma_epsilon_O = threshold_params['sigma_epsilon_O']
    sigma_epsilon_R = threshold_params['sigma_epsilon_R']

    prev_lambda_O_col = f'lambda_O_{t-1}'
    prev_lambda_R_col = f'lambda_R_{t-1}'
    curr_lambda_O_col = f'lambda_O_{t}'
    curr_lambda_R_col = f'lambda_R_{t}'

    epsilon_O = np.random.normal(0, sigma_epsilon_O, size=len(group_df))
    epsilon_R = np.random.normal(0, sigma_epsilon_R, size=len(group_df))

    group_df[curr_lambda_O_col] = (1 - phi_O) * group_df['mu_bar_O'] + phi_O * group_df[prev_lambda_O_col] + epsilon_O
    group_df[curr_lambda_R_col] = (1 - phi_R) * group_df['mu_bar_R'] + phi_R * group_df[prev_lambda_R_col] + epsilon_R

    return group_df

def assign_individual_thresholds_from_lambda(households_df, group_df, t, threshold_params):
    """
    Assign thresholds to individual households based on group-level lambda values at time t.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id'.
    group_df : DataFrame
        DataFrame containing group lambda values at time t, indexed by 'group_id'.
    t : int
        Current time step.
    threshold_params : dict
        Dictionary of threshold model parameters.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated lambda and threshold columns for time t.
    """
    sigma_ind_O = threshold_params['sigma_ind_O']
    sigma_ind_R = threshold_params['sigma_ind_R']

    lambda_O_group_col = f'lambda_O_{t}' # Group lambda for current time t
    lambda_R_group_col = f'lambda_R_{t}'

    temp_df = households_df.merge(
        group_df[[lambda_O_group_col, lambda_R_group_col]],
        left_on='group_id',
        right_index=True,
        how='left'
    )

    num_households = len(households_df)
    eta_O = np.random.normal(0, 1, size=num_households) # Standard normal deviates
    eta_R = np.random.normal(0, 1, size=num_households)

    households_df['eta_O'] = eta_O # For tracking/debugging
    households_df['eta_R'] = eta_R

    # Household-specific lambda
    households_df['lambda_O'] = temp_df[lambda_O_group_col] + sigma_ind_O * eta_O
    households_df['lambda_R'] = temp_df[lambda_R_group_col] + sigma_ind_R * eta_R

    # Transform to thresholds (tau)
    households_df['tau_O'] = 1 / (1 + np.exp(-households_df['lambda_O']))
    households_df['tau_R'] = 1 / (1 + np.exp(-households_df['lambda_R']))

    # Store group-level lambdas for reference
    households_df['group_lambda_O_t'] = temp_df[lambda_O_group_col]
    households_df['group_lambda_R_t'] = temp_df[lambda_R_group_col]


    return households_df

def update_states_with_group_thresholds(households_df, smoothing_factor):
    """
    Update household decision states (Y_O, Y_R) based on individual thresholds (tau)
    and adjusted influences (I) using a logistic smoothed threshold model.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame with current states, influences, thresholds.
    smoothing_factor : float
        Steepness factor for the logistic transition.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with updated state columns.
    """
    Y_O_prev_update = households_df['Y_O'].values.copy() # State before this update
    Y_R_prev_update = households_df['Y_R'].values.copy()

    Y_O = households_df['Y_O'].values
    Y_R = households_df['Y_R'].values
    I_O = households_df['I_O'].values
    I_R = households_df['I_R'].values
    tau_O = households_df['tau_O'].values
    tau_R = households_df['tau_R'].values

    # Transition probabilities (0 -> 1)
    p_O_transition = 1 / (1 + np.exp(-smoothing_factor * (I_O - tau_O)))
    p_R_transition = 1 / (1 + np.exp(-smoothing_factor * (I_R - tau_R)))

    households_df['p_O'] = p_O_transition # Store probability for analysis
    households_df['p_R'] = p_R_transition

    rand_O = np.random.rand(len(Y_O))
    rand_R = np.random.rand(len(Y_R))

    # Update state to 1 if currently 0 AND random draw < probability
    new_Y_O = np.where((Y_O == 0) & (rand_O < p_O_transition), 1, Y_O)
    new_Y_R = np.where((Y_R == 0) & (rand_R < p_R_transition), 1, Y_R)

    households_df['Y_O'] = new_Y_O
    households_df['Y_R'] = new_Y_R

    # Update Y_O_prev, Y_R_prev to reflect state *before* this current update
    households_df['Y_O_prev'] = Y_O_prev_update
    households_df['Y_R_prev'] = Y_R_prev_update

    return households_df

# --- Visualization Functions ---

def visualize_network(households_df, W_B, W_R, id_to_idx, time_step, output_dir='.'):
    """
    Visualize the network with nodes colored by group/state at a specific time step.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data for the current time step.
    W_B, W_R : ndarray
        Adjacency matrices for bonding and bridging networks.
    id_to_idx : dict
        Mapping from household ID to matrix index.
    time_step : int
        Current simulation time step.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print(f"  Generating network visualization for t={time_step}...")
    G = nx.Graph()

    # Ensure households_df is correctly aligned with W_B, W_R indices
    # This assumes households_df rows correspond to matrix indices 0..N-1
    # or that attributes are correctly mapped if not.

    # Create idx_to_id from id_to_idx for node labeling if needed
    idx_to_id_map = {v: k for k, v in id_to_idx.items()}

    # Add nodes with attributes from households_df, ensuring correct mapping
    # It's crucial that Y_O, Y_R, group_ids, lats, lons are in the order of matrix indices

    # A robust way: iterate through matrix indices
    n_nodes_matrix = W_B.shape[0]
    node_attributes_list = []

    # Check if households_df has 'household_id' as index for easy lookup
    # If not, create a temporary mapping or ensure households_df is sorted/aligned.
    # For simplicity, let's assume households_df is aligned such that its rows
    # 0 to N-1 correspond to matrix indices 0 to N-1.
    # This means households_df.iloc[matrix_idx] gives attributes for that node.

    household_ids_from_df = households_df['household_id'].values # Order from DF

    # If id_to_idx was based on a different order, this needs careful handling.
    # Assuming id_to_idx was created from the *initial* households_df, and
    # the current households_df maintains that order or can be realigned.
    # For this version, we assume households_df passed here is the current state
    # and its order matches the matrix indices (0 to N-1).

    for matrix_idx in range(n_nodes_matrix):
        h_id = idx_to_id_map.get(matrix_idx, f"idx_{matrix_idx}") # Get household ID for this matrix index

        # Find this h_id in the current households_df to get its attributes
        # This is safer if households_df order might change or not match matrix indices directly
        # However, it's slower. The original code assumed direct alignment.
        # Sticking to original assumption for now: households_df.iloc[matrix_idx]
        try:
            # Find the row in households_df that corresponds to this h_id
            # This assumes h_id from idx_to_id_map is present in households_df['household_id']
            # And that households_df is the *current* state dataframe.

            # Simpler: if households_df is always ordered by id_to_idx:
            # current_h_attributes = households_df.iloc[matrix_idx]
            # h_id_for_node = current_h_attributes['household_id']

            # Let's use the household_id from idx_to_id_map as the node identifier in the graph
            # And fetch attributes from households_df using this ID.

            # This requires households_df to be indexed by 'household_id' for quick lookup
            # Or iterate to find it.

            # Fallback to direct indexing if households_df is assumed to be aligned
            # with matrix indices.
            if h_id in households_df['household_id'].values:
                 current_h_attributes = households_df[households_df['household_id'] == h_id].iloc[0]
            else: # If h_id from matrix not in current df (e.g. filtered out), use placeholder
                # This case should ideally not happen if households_df covers all matrix nodes
                G.add_node(h_id, pos=(0,0), Y_O=0, Y_R=0, group_id=-1) # Placeholder
                continue

            G.add_node(h_id, # Use the actual household ID as node name
                       pos=(current_h_attributes['lon'], current_h_attributes['lat']),
                       Y_O=current_h_attributes['Y_O'],
                       Y_R=current_h_attributes['Y_R'],
                       group_id=current_h_attributes['group_id'])
        except IndexError: # If households_df is shorter than matrix dimension
            # This indicates a mismatch. Add node with default attributes.
             G.add_node(h_id, pos=(0,0), Y_O=0, Y_R=0, group_id=-1) # Placeholder
             print(f"Warning: Attribute mismatch for node index {matrix_idx}, id {h_id} in visualize_network.")
             continue


    bonding_edges = []
    bridging_edges = []
    for i in range(n_nodes_matrix):
        for j in range(i + 1, n_nodes_matrix):
            h_i_id = idx_to_id_map.get(i, f"idx_{i}") # Get household ID for node i
            h_j_id = idx_to_id_map.get(j, f"idx_{j}") # Get household ID for node j
            if W_B[i, j] > 0:
                bonding_edges.append((h_i_id, h_j_id, {'weight': W_B[i, j], 'type': 'bonding'}))
            if W_R[i, j] > 0:
                bridging_edges.append((h_i_id, h_j_id, {'weight': W_R[i, j], 'type': 'bridging'}))

    G.add_edges_from(bonding_edges)
    G.add_edges_from(bridging_edges)

    pos = nx.get_node_attributes(G, 'pos')
    if not pos: # If no nodes got positions (e.g., all households_df lookups failed)
        print(f"  Warning: No node positions found for t={time_step}. Using random layout.")
        pos = nx.random_layout(G)


    fig, axes = plt.subplots(2, 2, figsize=(20, 18))
    axes = axes.flatten()

    n_groups = households_df['group_id'].nunique()
    cmap_n_groups = max(1, n_groups)
    group_cmap = plt.cm.get_cmap('tab10', cmap_n_groups)
    state_cmap = LinearSegmentedColormap.from_list('StateColors', ['green', 'red'])

    node_size = 40
    edge_alpha_B = 0.4
    edge_alpha_R = 0.25
    edge_width_B = 0.8
    edge_width_R = 0.6

    # Plot 1: Nodes by Group ID
    ax = axes[0]
    if G.number_of_nodes() > 0:
        node_colors_group = [G.nodes[node]['group_id'] for node in G.nodes()]
        nx.draw_networkx_nodes(G, pos, node_color=node_colors_group, cmap=group_cmap, vmin=0, vmax=max(0, n_groups-1),
                               node_size=node_size, alpha=0.9, ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                               alpha=edge_alpha_B, edge_color='blue', ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                               alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)
    ax.set_title(f'Network by Group Membership (t={time_step})', fontsize=16)
    ax.axis('off')

    # Plot 2: Network Weight Distribution
    ax = axes[1]
    bonding_weights = [d['weight'] for _, _, d in bonding_edges]
    bridging_weights = [d['weight'] for _, _, d in bridging_edges]
    if bonding_weights:
        ax.hist(bonding_weights, bins=20, alpha=0.7, color='blue', label='Bonding')
    if bridging_weights:
        ax.hist(bridging_weights, bins=20, alpha=0.7, color='green', label='Bridging')
    ax.set_title(f'Network Weight Distribution (t={time_step})', fontsize=16)
    ax.set_xlabel('Edge Weight', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    if bonding_weights or bridging_weights: ax.legend()
    ax.grid(alpha=0.3)

    # Plots 3-4: Nodes by Decision State
    states_info = [
        {'attr': 'Y_O', 'title': 'Occupancy State (Y_O=1 is Vacant)', 'ax_idx': 2},
        {'attr': 'Y_R', 'title': 'Recovery State (Y_R=1 is Repaired)', 'ax_idx': 3},
    ]

    for info in states_info:
        ax = axes[info['ax_idx']]
        attr = info['attr']
        if G.number_of_nodes() > 0:
            node_colors_state = [G.nodes[node][attr] for node in G.nodes()]
            nx.draw_networkx_nodes(G, pos, node_color=node_colors_state, cmap=state_cmap, vmin=0, vmax=1,
                                   node_size=node_size, alpha=0.9, ax=ax)
            nx.draw_networkx_edges(G, pos, edgelist=bonding_edges, width=edge_width_B,
                                   alpha=edge_alpha_B, edge_color='blue', ax=ax)
            nx.draw_networkx_edges(G, pos, edgelist=bridging_edges, width=edge_width_R,
                                   alpha=edge_alpha_R, edge_color='green', style='dashed', ax=ax)
            state_pct = np.mean(node_colors_state) * 100 if node_colors_state else 0
            ax.set_title(f'{info["title"]} (t={time_step}) - {state_pct:.1f}%', fontsize=16)

            group_stats = []
            # Use current households_df for group stats, as G.nodes might have placeholder group_id
            # if there were issues.
            unique_group_ids_df = sorted(households_df['group_id'].unique())
            for g_id in unique_group_ids_df:
                # Filter households_df for this group
                group_hh_df = households_df[households_df['group_id'] == g_id]
                if not group_hh_df.empty:
                    group_state_values_df = group_hh_df[attr].values # Get state from df for this group
                    group_pct_df = np.mean(group_state_values_df) * 100 if group_state_values_df.size > 0 else 0
                    group_stats.append(f"Grp {g_id}: {group_pct_df:.1f}%")
                else:
                    group_stats.append(f"Grp {g_id}: N/A")

            stats_text = "\n".join(group_stats)
            ax.text(0.02, 0.02, stats_text, fontsize=10, transform=ax.transAxes,
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

        else: # No nodes in graph
            ax.set_title(f'{info["title"]} (t={time_step}) - No Data', fontsize=16)
        ax.axis('off')


    legend_handles = []
    unique_group_ids_legend = sorted(households_df['group_id'].unique())
    if unique_group_ids_legend:
        norm_factor = max(1, n_groups - 1)
        for g_id in unique_group_ids_legend:
             legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',
                                             markerfacecolor=group_cmap(g_id / norm_factor if norm_factor > 0 else 0),
                                             markersize=10, label=f'Group {g_id}'))
    legend_handles.extend([
        plt.Line2D([0], [0], color='blue', lw=2, label='Bonding'),
        plt.Line2D([0], [0], color='green', lw=2, linestyle='--', label='Bridging')
    ])
    legend_handles.extend([
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='State=0'),
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='State=1')
    ])

    num_legend_items = len(legend_handles)
    legend_ncol = min(num_legend_items, 8)

    fig.legend(handles=legend_handles, loc='lower center', ncol=legend_ncol, fontsize=12, bbox_to_anchor=(0.5, 0.01))

    plt.tight_layout(rect=[0, 0.05, 1, 0.96])
    filename = os.path.join(output_dir, f'network_state_t{time_step:03d}.png')
    plt.savefig(filename, dpi=150, bbox_inches='tight') # Lower DPI for speed if needed
    plt.close(fig)
    print(f"    Visualization saved to {filename}")

def visualize_state_evolution(results, output_dir='.'):
    """
    Visualize the evolution of the proportion of households in each state (Y=1) over time.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to DataFrame with household data.
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing state evolution over time...")
    time_steps = sorted(results.keys())
    if not time_steps:
        print("  No results to visualize.")
        return

    props = {'Y_O': [], 'Y_R': []}
    for t in time_steps:
        df = results[t]
        if not df.empty:
            props['Y_O'].append(np.mean(df['Y_O']) * 100)
            props['Y_R'].append(np.mean(df['Y_R']) * 100)
        else:
            props['Y_O'].append(np.nan)
            props['Y_R'].append(np.nan)

    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.weight'] = 'bold'

    plt.figure(figsize=(12, 7))
    plt.plot(time_steps, props['Y_O'], 'r-o', linewidth=2, markersize=5, label='Vacant (Y_O=1)')
    plt.plot(time_steps, props['Y_R'], 'b-s', linewidth=2, markersize=5, label='Repaired (Y_R=1)')

    plt.xlabel('Time Step', fontsize=14, fontweight='bold') # Adjusted font size
    plt.ylabel('Proportion of Households (%)', fontsize=14, fontweight='bold') # Adjusted
    plt.title('Evolution of Household States Over Time', fontsize=16, fontweight='bold') # Adjusted
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.legend(fontsize=12, loc='best') # Adjusted

    plt.tick_params(axis='both', which='major', labelsize=10)
    for tick in plt.gca().xaxis.get_major_ticks():
        tick.label1.set_fontweight('bold')
    for tick in plt.gca().yaxis.get_major_ticks():
        tick.label1.set_fontweight('bold')

    tick_step = max(1, len(time_steps) // 12)
    plt.xticks(time_steps[::tick_step])
    plt.yticks(np.arange(0, 101, 10))
    plt.ylim(-2, 102)

    plt.tight_layout()
    filename = os.path.join(output_dir, 'state_evolution.png')
    plt.savefig(filename, dpi=150) # Lower DPI
    plt.close()
    print(f"  State evolution plot saved to {filename}")

def visualize_network_evolution(results, output_dir='.'):
    """
    Visualize the evolution of network weights over time.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to tuple of (households_df, W_B, W_R).
    output_dir : str, default='.'
        Directory to save the visualization PNG file.
    """
    print("Visualizing network weight evolution over time...")
    time_steps = sorted(results.keys())
    if not time_steps:
        print("  No results to visualize.")
        return

    avg_weights_B, avg_weights_R = [], []
    num_connections_B, num_connections_R = [], []

    for t in time_steps:
        _, W_B, W_R = results[t]

        conn_B = np.sum(W_B > 1e-6) / 2 # Count if weight is practically non-zero
        conn_R = np.sum(W_R > 1e-6) / 2

        sum_weights_B = np.sum(W_B)
        sum_weights_R = np.sum(W_R)

        avg_B = sum_weights_B / (2 * conn_B) if conn_B > 0 else 0
        avg_R = sum_weights_R / (2 * conn_R) if conn_R > 0 else 0

        num_connections_B.append(conn_B)
        num_connections_R.append(conn_R)
        avg_weights_B.append(avg_B)
        avg_weights_R.append(avg_R)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True) # Share x-axis

    ax1.plot(time_steps, avg_weights_B, 'b-o', label='Bonding Avg. Weight')
    ax1.plot(time_steps, avg_weights_R, 'g-^', label='Bridging Avg. Weight')
    ax1.set_ylabel('Average Edge Weight', fontsize=12) # Adjusted
    ax1.set_title('Network Weight and Size Evolution', fontsize=14, fontweight='bold') # Combined title
    ax1.grid(True, alpha=0.3)
    ax1.legend(fontsize=10) # Adjusted

    ax2.plot(time_steps, num_connections_B, 'b-s', label='Bonding Connections')
    ax2.plot(time_steps, num_connections_R, 'g-x', label='Bridging Connections')
    ax2.set_xlabel('Time Step', fontsize=12, fontweight='bold') # Adjusted
    ax2.set_ylabel('Number of Connections', fontsize=12) # Adjusted
    ax2.grid(True, alpha=0.3)
    ax2.legend(fontsize=10) # Adjusted

    tick_step = max(1, len(time_steps) // 10)
    ax2.set_xticks(time_steps[::tick_step]) # Set ticks only on bottom plot

    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust for main title
    filename = os.path.join(output_dir, 'network_evolution.png')
    plt.savefig(filename, dpi=150) # Lower DPI
    plt.close(fig)
    print(f"  Network evolution plot saved to {filename}")

def homogenize_group_attributes(households_df, seed=None):
    """
    Make all households within the same group have identical attributes.
    Called after group assignment to ensure within-group homogeneity.

    Parameters:
    -----------
    households_df : DataFrame
        DataFrame containing household data with 'group_id' already assigned.
    seed : int, optional
        Random seed for reproducibility.

    Returns:
    --------
    households_df : DataFrame
        DataFrame with homogenized household attributes (same values within groups).
    """
    print("Homogenizing attributes within groups...")

    if seed is not None:
        np.random.seed(seed)

    group_ids = sorted(households_df['group_id'].unique())
    attributes_to_homogenize = ['income', 'resource_access', 'damage_structural', 'damage_utility']

    for g_id in group_ids:
        group_indices = households_df[households_df['group_id'] == g_id].index
        if not group_indices.empty:
            # Generate one set of attributes for the whole group
            # Using uniform for continuous-like attributes, can be changed if they are binary
            group_attrs = {
                'income': np.random.uniform(0.1, 0.9), # Example: continuous income score
                'resource_access': np.random.uniform(0.1, 0.9), # Example: continuous access score
                # For damage, if binary (0 or 1), use np.random.choice or fixed prob.
                # Here, using uniform, implies damage level.
                'damage_structural': np.random.uniform(0, 0.6),
                'damage_utility': np.random.uniform(0, 0.4)
            }

            for attr, val in group_attrs.items():
                households_df.loc[group_indices, attr] = val

            # print(f"  Group {g_id} ({len(group_indices)} households) - Attributes set.")
    print("Attribute homogenization complete.")
    return households_df


def export_results_to_csv(results, output_dir, matrix_headers):
    """
    Export simulation results: household states and network weight matrices.

    Parameters:
    -----------
    results : dict
        Dictionary mapping time step to tuple of (households_df, W_B, W_R).
    output_dir : str
        Directory to save the output CSV files.
    matrix_headers : list
        List of household IDs to use as headers/index for the weight matrices.
    """
    print(f"Exporting simplified simulation results to directory: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)

    household_states_data = []

    for timestep, (df, W_B, W_R) in results.items():
        # 1. Household states
        # Ensure Y_O and Y_R are present, otherwise add NaNs or skip
        if 'Y_O' not in df.columns: df['Y_O'] = np.nan
        if 'Y_R' not in df.columns: df['Y_R'] = np.nan

        current_states_df = df[['household_id', 'Y_O', 'Y_R']].copy()
        current_states_df['timestep'] = timestep
        household_states_data.append(current_states_df)

        # 2. Network weight matrices
        # Bonding matrix
        W_B_df = pd.DataFrame(W_B, index=matrix_headers, columns=matrix_headers)
        wb_filename = os.path.join(output_dir, f'W_B_t{timestep:03d}.csv')
        try:
            W_B_df.to_csv(wb_filename, float_format='%.6f')
        except Exception as e:
            print(f"  Error saving {wb_filename}: {e}. Trying without headers.")
            try: # Fallback: save without headers if household IDs cause issues (e.g. too long, special chars)
                pd.DataFrame(W_B).to_csv(wb_filename, float_format='%.6f', header=False, index=False)
            except Exception as e2:
                print(f"  Fallback save for {wb_filename} also failed: {e2}")


        # Bridging matrix
        W_R_df = pd.DataFrame(W_R, index=matrix_headers, columns=matrix_headers)
        wr_filename = os.path.join(output_dir, f'W_R_t{timestep:03d}.csv')
        try:
            W_R_df.to_csv(wr_filename, float_format='%.6f')
        except Exception as e:
            print(f"  Error saving {wr_filename}: {e}. Trying without headers.")
            try: # Fallback
                pd.DataFrame(W_R).to_csv(wr_filename, float_format='%.6f', header=False, index=False)
            except Exception as e2:
                print(f"  Fallback save for {wr_filename} also failed: {e2}")


    # Concatenate and save household states
    if household_states_data:
        all_household_states_df = pd.concat(household_states_data, ignore_index=True)
        states_csv_path = os.path.join(output_dir, 'dynamic_household_states.csv')
        try:
            all_household_states_df.to_csv(states_csv_path, index=False, float_format='%.3f')
            print(f"  Household states saved to {states_csv_path}")
        except Exception as e:
            print(f"  Error saving household states CSV: {e}")
    else:
        print("  No household state data to export.")

    print("Simplified data export complete.")


# --- Modified Simulation Run Function ---

def run_dynamic_network_simulation(households_df_initial, network_df, simulation_params):
    """
    Run the modified agent-based simulation with time-varying network weights.

    Parameters:
    -----------
    households_df_initial : DataFrame
        Initial DataFrame containing basic household attributes (ID, lat, lon, characteristics).
    network_df : DataFrame
        DataFrame containing network connections.
    simulation_params : dict
        Dictionary of simulation parameters.

    Returns:
    --------
    results : dict
        Dictionary mapping time step to tuple of (households_df, W_B, W_R).
    group_df : DataFrame
        DataFrame containing the final group attributes and threshold parameters.
    """
    num_steps = simulation_params.get('num_steps', 20)
    output_dir = simulation_params.get('output_dir', 'simulation_results')
    random_seed = simulation_params.get('random_seed', None)
    damage_threshold = simulation_params.get('damage_threshold', 0.5)
    visualization_interval = simulation_params.get('visualization_interval', 5)
    omega_matrix = simulation_params.get('omega_matrix')
    threshold_params = simulation_params.get('threshold_params')
    ar1_params = simulation_params.get('ar1_params')
    num_groups = simulation_params.get('num_groups', 5)
    smoothing_factor = simulation_params.get('smoothing_factor')

    print("="*30 + f"\n Starting Dynamic Network Simulation Run \n" + "="*30)
    print(f"Params: Steps={num_steps}, Groups={num_groups}, Seed={random_seed}, Output='{output_dir}'")

    if random_seed is not None: np.random.seed(random_seed)
    try:
        os.makedirs(output_dir, exist_ok=True)
        print(f"Output directory: {output_dir}")
    except Exception as e:
        print(f"Warning: Output directory error: {e}. Using current directory.")
        output_dir = '.'

    households_df = households_df_initial.copy()

    print("\n--- Initialization (t=0) ---")
    households_df, group_centroids = assign_households_to_groups(
        households_df, num_groups=num_groups, random_seed=random_seed)
    households_df = homogenize_group_attributes(households_df, seed=random_seed) # Homogenize after grouping
    group_df = generate_group_attributes(households_df, group_centroids)
    group_df = initialize_group_lambda(group_df, threshold_params)

    # Pass households_df (which now has group_id) to create_initial_network_matrices
    # This ensures W_B, W_R are sized based on this simulation's households
    W_B, W_R, id_to_idx, idx_to_id = create_initial_network_matrices(households_df, network_df)

    # Define matrix_headers based on the households actually in the matrices
    n_sim_households = W_B.shape[0]
    matrix_headers = [idx_to_id[i] for i in range(n_sim_households)]

    # Initialize states and thresholds for t=0 AFTER matrices are defined and households_df is stable
    households_df = assign_individual_thresholds_from_lambda(households_df, group_df, 0, threshold_params)
    households_df = initialize_decision_states(households_df, damage_threshold, seed=random_seed)

    print("  Calculating initial influences for t=0...")
    households_df = calculate_raw_influences(households_df, W_B, W_R, id_to_idx)
    households_df = calculate_adjusted_influences(households_df, omega_matrix)
    households_df['p_O'], households_df['p_R'] = np.nan, np.nan # No transition prob at t=0

    results = {0: (households_df.copy(), W_B.copy(), W_R.copy())}
    print("Initialization complete.")

    if visualization_interval > 0:
        try:
            visualize_network(households_df, W_B, W_R, id_to_idx, 0, output_dir)
        except Exception as e: print(f"Warning: Network viz error t=0: {e}")

    print(f"\n--- Running Simulation Steps (1 to {num_steps}) ---")
    for t in range(1, num_steps + 1):
        print(f"\n--- Time Step {t} ---")
        print(f"  Updating group-level lambda values...")
        group_df = update_group_lambda(group_df, t, threshold_params)

        print(f"  Updating network weights...") # households_df here is from t-1
        W_B, W_R = update_network_weights(W_B, W_R, households_df, id_to_idx, ar1_params)

        print(f"  Calculating raw influences...") # Based on states from t-1 and new W
        households_df = calculate_raw_influences(households_df, W_B, W_R, id_to_idx)

        print(f"  Calculating adjusted influences...")
        households_df = calculate_adjusted_influences(households_df, omega_matrix)

        print(f"  Assigning new individual thresholds...") # Based on group lambda at t
        households_df = assign_individual_thresholds_from_lambda(households_df, group_df, t, threshold_params)

        print(f"  Updating household states...") # States for t, based on I(t) and tau(t)
        households_df = update_states_with_group_thresholds(households_df, smoothing_factor)

        results[t] = (households_df.copy(), W_B.copy(), W_R.copy())
        print(f"  State update complete for t={t}.")

        mean_O = np.nanmean(households_df['Y_O'])*100 if 'Y_O' in households_df else np.nan
        mean_R = np.nanmean(households_df['Y_R'])*100 if 'Y_R' in households_df else np.nan
        print(f"    States: Vacant: {mean_O:.1f}%, Repaired: {mean_R:.1f}%")

        if visualization_interval > 0 and (t % visualization_interval == 0 or t == num_steps):
            try:
                visualize_network(households_df, W_B, W_R, id_to_idx, t, output_dir)
            except Exception as e: print(f"Warning: Network viz error t={t}: {e}")

    print("\n--- Simulation Loop Completed ---")
    print("\n--- Final Visualizations & Export ---")

    state_results_for_plot = {ts: res_tuple[0] for ts, res_tuple in results.items()}
    try: visualize_state_evolution(state_results_for_plot, output_dir)
    except Exception as e: print(f"Warning: State evolution viz error: {e}")

    try: visualize_network_evolution(results, output_dir)
    except Exception as e: print(f"Warning: Network evolution viz error: {e}")

    try: export_results_to_csv(results, output_dir, matrix_headers) # Pass matrix_headers
    except Exception as e: print(f"Warning: CSV export error: {e}")

    print(f"\nSimulation finished. Results in '{output_dir}'\n" + "="*30)
    return results, group_df

# --- Main Function ---

def main():
    NETWORK_FILE = 'small_baltimore_household.csv'
    OUTPUT_DIR = 'dynamic_network_simulation_results_simplified_export' # New dir
    RANDOM_SEED = 42
    NUM_STEPS = 24 # Reduced for faster testing
    NUM_GROUPS = 5 # Reduced
    VISUALIZATION_INTERVAL = 2 # More frequent for testing
    DAMAGE_THRESHOLD = 0.15
    SMOOTHING_FACTOR = 9

    # Omega matrix: [[O_OO, O_RO], [O_OR, O_RR]] for I = m @ Omega
    # Paper: I_O = 1*m_O + w_OR*m_R; I_R = w_RO*m_O + 1*m_R
    # Code: I_O = m_O*Omega[0,0] + m_R*Omega[1,0]
    #       I_R = m_O*Omega[0,1] + m_R*Omega[1,1]
    # So, Omega[0,0]=1, Omega[1,0]=w_OR, Omega[0,1]=w_RO, Omega[1,1]=1
    # OMEGA_MATRIX = np.array([
    #     [1.0, -0.1],  # col0: m_O*1 + m_R*w_OR -> w_OR=-0.1
    #     [-0.3, 1.0]   # col1: m_O*w_RO + m_R*1 -> w_RO=-0.3
    # ])
    # This means w_OR = -0.3 (influence of R on O decision)
    # and w_RO = -0.1 (influence of O on R decision)
    # If OMEGA_MATRIX in main is [[1.0, w_OR_param], [w_RO_param, 1.0]]
    # where w_OR_param is effect of R on O, w_RO_param is effect of O on R.
    # Then code's omega_matrix for m @ omega_matrix should be [[1.0, w_RO_param], [w_OR_param, 1.0]]
    # Let's use the direct form from paper and transpose if needed, or define correctly.
    # Paper's Omega: [[1, w_OR], [w_RO, 1]]
    # If m is [m_O, m_R] (row vector), then I = m @ Omega_paper.T
    # Omega_paper_T = [[1, w_RO], [w_OR, 1]]
    # Let w_OR = -0.3 (R's effect on O), w_RO = -0.1 (O's effect on R)
    OMEGA_MATRIX_FOR_CODE = np.array([ # This is Omega_paper.T
        [1.0, -0.1],  # [1, w_RO]
        [-0.3, 1.0]   # [w_OR, 1]
    ])


    AR1_PARAMS = {
        'rho_mig_B': 0.7, 'rho_mig_R': 0.5, 'rho_cop_B': 0.95, 'rho_cop_R': 0.7,
        'mu_mig_B': 0.05, 'mu_mig_R': 0.0, 'mu_cop_B': 1.0, 'mu_cop_R': 1.0,
        'sigma_mig_B': 0.05, 'sigma_mig_R': 0.05, 'sigma_cop_B': 0.05, 'sigma_cop_R': 0.05 # Reduced sigma
    }
    BETA_PARAMS = {
        'beta_H_O': np.array([-0.5, -0.3]), 'beta_D_O': np.array([0.8, 0.6]),
        'beta_H_R': np.array([-0.1, 0.3]), 'beta_D_R': np.array([0.2, 0.1]),
    }
    THRESHOLD_AR1_PARAMS = {
        'phi_O': 0.8, 'phi_R': 0.8,
        'sigma_epsilon_O': 0.2, 'sigma_epsilon_R': 0.2, # Reduced sigma
        'sigma_ind_O': 0.2, 'sigma_ind_R': 0.2, # Reduced sigma
    }
    THRESHOLD_PARAMS = {**BETA_PARAMS, **THRESHOLD_AR1_PARAMS}

    print("--- Loading Data ---")
    try:
        network_df = pd.read_csv(NETWORK_FILE)
        print(f"Network data: {len(network_df)} connections from '{NETWORK_FILE}'")
    except Exception as e:
        print(f"Error loading '{NETWORK_FILE}': {e}"); return

    print("\n--- Preparing Household Data ---")
    try:
        homes1 = network_df[['home_1', 'home_1_lat', 'home_1_lon']].rename(
            columns={'home_1': 'id', 'home_1_lat': 'lat', 'home_1_lon': 'lon'})
        homes2 = network_df[['home_2', 'home_2_lat', 'home_2_lon']].rename(
            columns={'home_2': 'id', 'home_2_lat': 'lat', 'home_2_lon': 'lon'})
        unique_homes = pd.concat([homes1, homes2]).drop_duplicates(subset=['id']).dropna(subset=['lat', 'lon']).reset_index(drop=True)

        household_ids = unique_homes['id'].tolist()
        household_locs = {row['id']: (float(row['lat']), float(row['lon'])) for _, row in unique_homes.iterrows()}

        print(f"Found {len(household_ids)} unique households with valid locations.")
        if not household_ids: print("Error: No households found."); return

        households_df_initial = generate_household_attributes(household_ids, household_locs, seed=RANDOM_SEED)
    except Exception as e:
        print(f"Error preparing household data: {e}"); return

    simulation_params = {
        'num_steps': NUM_STEPS, 'output_dir': OUTPUT_DIR, 'random_seed': RANDOM_SEED,
        'damage_threshold': DAMAGE_THRESHOLD, 'visualization_interval': VISUALIZATION_INTERVAL,
        'num_groups': NUM_GROUPS, 'omega_matrix': OMEGA_MATRIX_FOR_CODE, # Use the correctly structured one
        'threshold_params': THRESHOLD_PARAMS, 'ar1_params': AR1_PARAMS,
        'smoothing_factor': SMOOTHING_FACTOR
    }

    try:
        results, final_group_df = run_dynamic_network_simulation(
            households_df_initial, network_df, simulation_params)
    except Exception as e:
        print(f"\n--- SIMULATION FAILED --- \nError: {e}")
        import traceback; traceback.print_exc(); return

    if results:
        final_step = max(results.keys())
        final_households_df, final_W_B, final_W_R = results[final_step]
        print(f"\n--- Summary of Final State (t={final_step}) ---")
        n_final = len(final_households_df)
        if n_final > 0:
            print(f"Vacant (Y_O=1): {np.nansum(final_households_df['Y_O'])}/{n_final} ({np.nanmean(final_households_df['Y_O'])*100:.1f}%)")
            print(f"Repaired (Y_R=1): {np.nansum(final_households_df['Y_R'])}/{n_final} ({np.nanmean(final_households_df['Y_R'])*100:.1f}%)")
            # ... (rest of summary print can be added if needed)
    else:
        print("Simulation did not produce results.")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
import os
import time
import pickle
import json
import jax
import jax.numpy as jnp
from jax import random, lax
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS

# --- Configuration ---
# Set to the directory where the simulation (from dynamic_network_simulation_export_mod.py)
# saved its output.
SIMULATION_OUTPUT_DIR = 'dynamic_network_simulation_results_simplified_export'

# Directory to save MCMC inference results
MCMC_RESULTS_DIR = 'mcmc_network_weight_inference_results_vectorized'


# --- Data Loading ---
def load_inference_data(sim_output_dir):
    """
    Loads household states and time-series of network weight matrices.
    """
    print(f"Loading data from: {sim_output_dir}")

    # Load household states
    states_file = os.path.join(sim_output_dir, 'dynamic_household_states.csv')
    if not os.path.exists(states_file):
        raise FileNotFoundError(f"Household states file not found: {states_file}")
    household_states_df = pd.read_csv(states_file)
    print(f"Loaded household states: {len(household_states_df)} records.")

    timesteps = sorted(household_states_df['timestep'].unique())
    if not timesteps:
        raise ValueError("No timesteps found in household states data.")

    wb_initial_file = os.path.join(sim_output_dir, f'W_B_t{timesteps[0]:03d}.csv')
    if not os.path.exists(wb_initial_file):
        raise FileNotFoundError(f"Initial W_B matrix file not found: {wb_initial_file}")

    try:
        w_b_initial_df_check = pd.read_csv(wb_initial_file, index_col=0)
        household_ids_ordered = w_b_initial_df_check.index.astype(str).tolist() # Ensure string type
        if not all(isinstance(hid, str) for hid in household_ids_ordered):
             print("Warning: Household IDs from matrix index might not be simple strings. Ensure consistency.")
    except Exception as e:
        print(f"Error reading initial W_B matrix with index: {e}. Attempting without index_col.")
        w_b_initial_df_check = pd.read_csv(wb_initial_file)
        num_cols = w_b_initial_df_check.shape[1]
        household_ids_ordered = [f"h_{i}" for i in range(num_cols)]
        print(f"Using placeholder household IDs: h_0 to h_{num_cols-1}")

    n_households = len(household_ids_ordered)
    n_timesteps = len(timesteps)
    print(f"Found {n_households} households (from matrix headers) and {n_timesteps} timesteps.")

    W_B_all_t = np.full((n_timesteps, n_households, n_households), 0.0, dtype=np.float32)
    W_R_all_t = np.full((n_timesteps, n_households, n_households), 0.0, dtype=np.float32)

    for t_idx, ts_val in enumerate(timesteps):
        wb_file = os.path.join(sim_output_dir, f'W_B_t{ts_val:03d}.csv')
        wr_file = os.path.join(sim_output_dir, f'W_R_t{ts_val:03d}.csv')

        if os.path.exists(wb_file):
            try:
                wb_df = pd.read_csv(wb_file, index_col=0)
                wb_df.index = wb_df.index.astype(str)
                wb_df.columns = wb_df.columns.astype(str)
                W_B_all_t[t_idx, :, :] = wb_df.reindex(index=household_ids_ordered, columns=household_ids_ordered).fillna(0).values
            except Exception as e_wb:
                print(f"Warning: Could not read {wb_file} with index, or reindex failed: {e_wb}. Reading as raw values.")
                try:
                    W_B_all_t[t_idx, :, :] = pd.read_csv(wb_file).fillna(0).values[:n_households, :n_households]
                except Exception as e_wb_raw:
                    print(f"Error: Failed to read {wb_file} even as raw: {e_wb_raw}")

        if os.path.exists(wr_file):
            try:
                wr_df = pd.read_csv(wr_file, index_col=0)
                wr_df.index = wr_df.index.astype(str)
                wr_df.columns = wr_df.columns.astype(str)
                W_R_all_t[t_idx, :, :] = wr_df.reindex(index=household_ids_ordered, columns=household_ids_ordered).fillna(0).values
            except Exception as e_wr:
                print(f"Warning: Could not read {wr_file} with index, or reindex failed: {e_wr}. Reading as raw values.")
                try:
                    W_R_all_t[t_idx, :, :] = pd.read_csv(wr_file).fillna(0).values[:n_households, :n_households]
                except Exception as e_wr_raw:
                    print(f"Error: Failed to read {wr_file} even as raw: {e_wr_raw}")

    Y_O_obs = np.full((n_timesteps, n_households), 0, dtype=np.int8)
    Y_R_obs = np.full((n_timesteps, n_households), 0, dtype=np.int8)

    hid_to_matrix_idx = {hid: i for i, hid in enumerate(household_ids_ordered)}

    for t_idx, ts_val in enumerate(timesteps):
        t_data_df = household_states_df[household_states_df['timestep'] == ts_val]
        for _, row in t_data_df.iterrows():
            h_id = str(row['household_id'])
            if h_id in hid_to_matrix_idx:
                matrix_idx = hid_to_matrix_idx[h_id]
                Y_O_obs[t_idx, matrix_idx] = row['Y_O']
                Y_R_obs[t_idx, matrix_idx] = row['Y_R']
            else:
                pass

    # Validate network structure
    is_symmetric_B = np.allclose(W_B_all_t, np.transpose(W_B_all_t, (0, 2, 1)))
    is_symmetric_R = np.allclose(W_R_all_t, np.transpose(W_R_all_t, (0, 2, 1)))
    print(f"Bonding network is symmetric: {is_symmetric_B}")
    print(f"Bridging network is symmetric: {is_symmetric_R}")

    if not (is_symmetric_B and is_symmetric_R):
        print("Warning: Network appears to be directed. Current implementation assumes undirected networks.")

    print("Data loading complete.")
    return {
        'Y_O_obs': jnp.array(Y_O_obs), 'Y_R_obs': jnp.array(Y_R_obs),
        'W_B_obs': jnp.array(W_B_all_t), 'W_R_obs': jnp.array(W_R_all_t),
        'n_households': n_households, 'n_timesteps': n_timesteps,
        'household_ids_ordered': household_ids_ordered,
        'is_symmetric': is_symmetric_B and is_symmetric_R
    }

# --- Prior Configuration ---
def setup_network_weight_priors():
    """ Defines priors for the network weight AR(1) parameters aligned with theory. """
    return {
        # AR(1) persistence parameters (transformed to ensure |ρ| < 1)
        'rho_raw': {'mean': 0.5, 'std': 0.5},

        # Long-term means aligned with theoretical framework
        'mu_mig_B': {'mean': 0.05, 'std': 0.02},  # Small positive for bonding migration
        'mu_mig_R_fixed': 0.0,                     # Fixed at 0 for bridging migration
        'mu_cop_B_fixed': 1.0,                     # Fixed at 1 for bonding co-repair
        'mu_cop_R_fixed': 1.0,                     # Fixed at 1 for bridging co-repair

        # Innovation variances
        'sigma_ar': {'scale': 0.05},

        # Baseline regime violation penalty
        'baseline_penalty': 1000.0  # Large penalty for weight changes in baseline regime
    }

# --- NumPyro Model for Network Weight Inference (Complete & Corrected) ---
def network_weight_inference_model(data, priors):
    """ NumPyro model to infer AR(1) parameters for dynamic network weights (Complete Implementation). """
    Y_O_obs = data['Y_O_obs']
    Y_R_obs = data['Y_R_obs']
    W_B_obs = data['W_B_obs']
    W_R_obs = data['W_R_obs']
    n_households = data['n_households']
    n_timesteps = data['n_timesteps']

    # Sample AR(1) persistence parameters (transformed to ensure stationarity)
    rho_mig_B_raw = numpyro.sample("rho_mig_B_raw", dist.Normal(priors['rho_raw']['mean'], priors['rho_raw']['std']))
    rho_mig_R_raw = numpyro.sample("rho_mig_R_raw", dist.Normal(priors['rho_raw']['mean'], priors['rho_raw']['std']))
    rho_cop_B_raw = numpyro.sample("rho_cop_B_raw", dist.Normal(priors['rho_raw']['mean'], priors['rho_raw']['std']))
    rho_cop_R_raw = numpyro.sample("rho_cop_R_raw", dist.Normal(priors['rho_raw']['mean'], priors['rho_raw']['std']))

    # Transform to ensure |ρ| < 1 for stationarity
    rho_mig_B = numpyro.deterministic("rho_mig_B", jnp.tanh(rho_mig_B_raw))
    rho_mig_R = numpyro.deterministic("rho_mig_R", jnp.tanh(rho_mig_R_raw))
    rho_cop_B = numpyro.deterministic("rho_cop_B", jnp.tanh(rho_cop_B_raw))
    rho_cop_R = numpyro.deterministic("rho_cop_R", jnp.tanh(rho_cop_R_raw))

    # Sample long-term means (some fixed according to theory)
    mu_mig_B = numpyro.sample("mu_mig_B", dist.Normal(priors['mu_mig_B']['mean'], priors['mu_mig_B']['std']))

    # Fixed parameters according to theoretical framework
    mu_mig_R = numpyro.deterministic("mu_mig_R", jnp.array(priors['mu_mig_R_fixed']))
    mu_cop_B = numpyro.deterministic("mu_cop_B", jnp.array(priors['mu_cop_B_fixed']))
    mu_cop_R = numpyro.deterministic("mu_cop_R", jnp.array(priors['mu_cop_R_fixed']))

    # Sample innovation variances
    sigma_mig_B = numpyro.sample("sigma_mig_B", dist.HalfNormal(priors['sigma_ar']['scale']))
    sigma_mig_R = numpyro.sample("sigma_mig_R", dist.HalfNormal(priors['sigma_ar']['scale']))
    sigma_cop_B = numpyro.sample("sigma_cop_B", dist.HalfNormal(priors['sigma_ar']['scale']))
    sigma_cop_R = numpyro.sample("sigma_cop_R", dist.HalfNormal(priors['sigma_ar']['scale']))

    # Get upper triangular indices for undirected network
    h_indices, j_indices = jnp.triu_indices(n_households, k=1)

    total_log_lik = 0.0
    baseline_violation_penalty = 0.0

    for t_idx in range(1, n_timesteps):
        # Get previous states for regime determination
        y_o_prev_all_hh = Y_O_obs[t_idx - 1, :]
        y_r_prev_all_hh = Y_R_obs[t_idx - 1, :]

        # Extract states for edge endpoints
        y_o_h_prev_edges = y_o_prev_all_hh[h_indices]
        y_o_j_prev_edges = y_o_prev_all_hh[j_indices]
        y_r_h_prev_edges = y_r_prev_all_hh[h_indices]
        y_r_j_prev_edges = y_r_prev_all_hh[j_indices]

        # Determine regimes according to theoretical framework
        is_migration_regime_edges = (y_o_h_prev_edges == 1) | (y_o_j_prev_edges == 1)
        is_corepair_regime_edges = (~is_migration_regime_edges) & (y_r_h_prev_edges == 1) & (y_r_j_prev_edges == 1)
        is_baseline_regime_edges = (~is_migration_regime_edges) & (~is_corepair_regime_edges)

        # Get edge weights
        w_b_prev_edges = W_B_obs[t_idx - 1, h_indices, j_indices]
        w_r_prev_edges = W_R_obs[t_idx - 1, h_indices, j_indices]
        w_b_curr_obs_edges = W_B_obs[t_idx, h_indices, j_indices]
        w_r_curr_obs_edges = W_R_obs[t_idx, h_indices, j_indices]

        # Active edge masks (edges with non-zero weights)
        active_b_mask = w_b_prev_edges > 1e-6
        active_r_mask = w_r_prev_edges > 1e-6

        # --- BONDING LAYER LIKELIHOOD ---

        # Migration Regime for Bonding
        mig_b_final_mask = active_b_mask & is_migration_regime_edges
        mean_wb_mig_all = (1 - rho_mig_B) * mu_mig_B + rho_mig_B * w_b_prev_edges
        log_probs_wb_mig = dist.Normal(mean_wb_mig_all, sigma_mig_B).log_prob(w_b_curr_obs_edges)
        # Clip for numerical stability
        log_probs_wb_mig = jnp.clip(log_probs_wb_mig, -50, 50)
        total_log_lik += jnp.sum(jnp.where(mig_b_final_mask, log_probs_wb_mig, 0.0))

        # Co-repair Regime for Bonding
        cop_b_final_mask = active_b_mask & is_corepair_regime_edges
        mean_wb_cop_all = (1 - rho_cop_B) * mu_cop_B + rho_cop_B * w_b_prev_edges
        log_probs_wb_cop = dist.Normal(mean_wb_cop_all, sigma_cop_B).log_prob(w_b_curr_obs_edges)
        log_probs_wb_cop = jnp.clip(log_probs_wb_cop, -50, 50)
        total_log_lik += jnp.sum(jnp.where(cop_b_final_mask, log_probs_wb_cop, 0.0))

        # Baseline Regime for Bonding (weights should remain unchanged)
        baseline_b_mask = active_b_mask & is_baseline_regime_edges
        baseline_violation_b = jnp.sum(jnp.where(baseline_b_mask,
                                                 jnp.square(w_b_curr_obs_edges - w_b_prev_edges), 0.0))
        baseline_violation_penalty += baseline_violation_b

        # --- BRIDGING LAYER LIKELIHOOD ---

        # Migration Regime for Bridging
        mig_r_final_mask = active_r_mask & is_migration_regime_edges
        mean_wr_mig_all = (1 - rho_mig_R) * mu_mig_R + rho_mig_R * w_r_prev_edges
        log_probs_wr_mig = dist.Normal(mean_wr_mig_all, sigma_mig_R).log_prob(w_r_curr_obs_edges)
        log_probs_wr_mig = jnp.clip(log_probs_wr_mig, -50, 50)
        total_log_lik += jnp.sum(jnp.where(mig_r_final_mask, log_probs_wr_mig, 0.0))

        # Co-repair Regime for Bridging
        cop_r_final_mask = active_r_mask & is_corepair_regime_edges
        mean_wr_cop_all = (1 - rho_cop_R) * mu_cop_R + rho_cop_R * w_r_prev_edges
        log_probs_wr_cop = dist.Normal(mean_wr_cop_all, sigma_cop_R).log_prob(w_r_curr_obs_edges)
        log_probs_wr_cop = jnp.clip(log_probs_wr_cop, -50, 50)
        total_log_lik += jnp.sum(jnp.where(cop_r_final_mask, log_probs_wr_cop, 0.0))

        # Baseline Regime for Bridging (weights should remain unchanged)
        baseline_r_mask = active_r_mask & is_baseline_regime_edges
        baseline_violation_r = jnp.sum(jnp.where(baseline_r_mask,
                                                 jnp.square(w_r_curr_obs_edges - w_r_prev_edges), 0.0))
        baseline_violation_penalty += baseline_violation_r

    # Apply penalty for baseline regime violations
    total_log_lik -= priors['baseline_penalty'] * baseline_violation_penalty

    # Add total likelihood to the factor
    numpyro.factor("total_network_weight_log_likelihood", total_log_lik)

    # Track regime statistics for diagnostics
    numpyro.deterministic("baseline_violation_penalty", baseline_violation_penalty)


# --- Enhanced MCMC Execution ---
def run_inference(data, priors, num_samples=500, num_warmup=200, num_chains=1, seed=0):
    """
    Run MCMC inference with enhanced diagnostics and validation.
    """
    print(f"Starting MCMC: {num_samples} samples, {num_warmup} warmup, {num_chains} chains.")

    # Validate data structure
    validate_data_structure(data)

    if num_chains > 1:
        try:
            actual_host_cpus = len(os.sched_getaffinity(0))
            numpyro.set_host_device_count(min(num_chains, actual_host_cpus))
            print(f"Set host device count to {min(num_chains, actual_host_cpus)} for parallel chains.")
        except AttributeError:
            numpyro.set_host_device_count(num_chains)
            print("os.sched_getaffinity not available, using num_chains for host_device_count.")

    kernel = NUTS(network_weight_inference_model,
                  target_accept_prob=0.85,
                  max_tree_depth=10)
    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples,
                num_chains=num_chains, progress_bar=True,
                chain_method='parallel' if num_chains > 1 else 'sequential')

    rng_key = random.PRNGKey(seed)
    start_time = time.time()
    mcmc.run(rng_key, data=data, priors=priors)
    end_time = time.time()

    print(f"MCMC finished in {(end_time - start_time):.2f} seconds.")
    mcmc.print_summary(prob=0.9)

    # Get samples and add diagnostic information
    samples = mcmc.get_samples()
    diagnostics = compute_diagnostics(samples, data)

    return samples, mcmc, diagnostics

def validate_data_structure(data):
    """
    Validate input data structure and report potential issues.
    """
    print("\n=== Data Structure Validation ===")

    Y_O_obs = data['Y_O_obs']
    Y_R_obs = data['Y_R_obs']
    W_B_obs = data['W_B_obs']
    W_R_obs = data['W_R_obs']

    # Check dimensions
    print(f"Y_O shape: {Y_O_obs.shape}")
    print(f"Y_R shape: {Y_R_obs.shape}")
    print(f"W_B shape: {W_B_obs.shape}")
    print(f"W_R shape: {W_R_obs.shape}")

    # Check value ranges
    print(f"Y_O values: {jnp.unique(Y_O_obs)}")
    print(f"Y_R values: {jnp.unique(Y_R_obs)}")
    print(f"W_B range: [{jnp.min(W_B_obs):.6f}, {jnp.max(W_B_obs):.6f}]")
    print(f"W_R range: [{jnp.min(W_R_obs):.6f}, {jnp.max(W_R_obs):.6f}]")

    # Check for potential issues
    if jnp.any(W_B_obs < 0) or jnp.any(W_R_obs < 0):
        print("WARNING: Negative weights detected!")

    if jnp.any(W_B_obs > 1) or jnp.any(W_R_obs > 1):
        print("WARNING: Weights > 1 detected!")

    # Check sparsity
    sparsity_B = jnp.mean(W_B_obs == 0)
    sparsity_R = jnp.mean(W_R_obs == 0)
    print(f"Network sparsity: B={sparsity_B:.3f}, R={sparsity_R:.3f}")

    print("=== Validation Complete ===\n")

def compute_diagnostics(samples, data):
    """
    Compute diagnostic statistics for the inference results.
    """
    diagnostics = {}

    # Parameter summaries
    for param in ['rho_mig_B', 'rho_mig_R', 'rho_cop_B', 'rho_cop_R']:
        values = samples[param]
        diagnostics[f'{param}_mean'] = float(jnp.mean(values))
        diagnostics[f'{param}_std'] = float(jnp.std(values))
        diagnostics[f'{param}_q025'] = float(jnp.percentile(values, 2.5))
        diagnostics[f'{param}_q975'] = float(jnp.percentile(values, 97.5))

    # Check for convergence issues
    diagnostics['baseline_violation_mean'] = float(jnp.mean(samples['baseline_violation_penalty']))

    return diagnostics

def save_results_with_diagnostics(samples, mcmc_instance, diagnostics, results_dir):
    """
    Save results with comprehensive diagnostics.
    """
    # Save posterior samples
    samples_file = os.path.join(results_dir, 'network_weight_posterior_samples.pkl')
    samples_np = {k: np.array(v) for k, v in samples.items()}
    with open(samples_file, 'wb') as f:
        pickle.dump(samples_np, f)
    print(f"Posterior samples saved to: {samples_file}")

    # Save MCMC instance
    mcmc_instance_file = os.path.join(results_dir, 'network_weight_mcmc_instance.pkl')
    with open(mcmc_instance_file, 'wb') as f:
        pickle.dump(mcmc_instance, f)
    print(f"MCMC instance saved to: {mcmc_instance_file}")

    # Save diagnostics
    diagnostics_file = os.path.join(results_dir, 'inference_diagnostics.json')
    with open(diagnostics_file, 'w') as f:
        json.dump(diagnostics, f, indent=2)
    print(f"Diagnostics saved to: {diagnostics_file}")

    # Save summary report
    summary_file = os.path.join(results_dir, 'inference_summary.txt')
    with open(summary_file, 'w') as f:
        f.write("Network Weight Inference Summary\n")
        f.write("================================\n\n")

        f.write("Parameter Estimates (Posterior Mean ± SD):\n")
        for param in ['rho_mig_B', 'rho_mig_R', 'rho_cop_B', 'rho_cop_R']:
            if param in diagnostics:
                mean_val = diagnostics[f'{param}_mean']
                std_val = diagnostics[f'{param}_std']
                q025 = diagnostics[f'{param}_q025']
                q975 = diagnostics[f'{param}_q975']
                f.write(f"  {param}: {mean_val:.4f} ± {std_val:.4f} [{q025:.4f}, {q975:.4f}]\n")

        f.write(f"\nBaseline Violation Penalty: {diagnostics['baseline_violation_mean']:.6f}\n")

        f.write("\nInterpretation:\n")
        f.write("- rho_mig_*: Persistence of weights in migration regime (closer to 1 = more persistent)\n")
        f.write("- rho_cop_*: Persistence of weights in co-repair regime (closer to 1 = more persistent)\n")
        f.write("- Low baseline violation penalty suggests good model fit\n")

    print(f"Summary report saved to: {summary_file}")

# --- Main Execution Block ---
if __name__ == "__main__":
    os.makedirs(MCMC_RESULTS_DIR, exist_ok=True)

    try:
        # 1. Load Data
        print("=== Loading Data ===")
        inference_data = load_inference_data(SIMULATION_OUTPUT_DIR)
        if inference_data['n_households'] == 0 or inference_data['n_timesteps'] <= 1:
            raise ValueError("Not enough households or timesteps for inference.")

        # 2. Setup Priors
        print("\n=== Setting Up Priors ===")
        network_priors = setup_network_weight_priors()
        print("Priors for network weight parameters:")
        print(json.dumps({k: v for k, v in network_priors.items() if k != 'baseline_penalty'}, indent=2))
        print(f"Baseline violation penalty: {network_priors['baseline_penalty']}")

        # 3. Run MCMC
        print("\n=== Running MCMC Inference ===")
        num_samples_mcmc, num_warmup_mcmc, num_chains_mcmc = 3000, 1000, 2 # Keep low for testing

        print(f"Running inference with {num_samples_mcmc} samples, {num_warmup_mcmc} warmup, {num_chains_mcmc} chains.")

        posterior_samples, mcmc_instance, diagnostics = run_inference(
            inference_data, network_priors,
            num_samples=num_samples_mcmc,
            num_warmup=num_warmup_mcmc,
            num_chains=num_chains_mcmc,
            seed=42
        )

        # 4. Save results with diagnostics
        print("\n=== Saving Results ===")
        save_results_with_diagnostics(posterior_samples, mcmc_instance, diagnostics, MCMC_RESULTS_DIR)

        print("\n=== Inference Complete ===")
        print("Key Results:")
        for param in ['rho_mig_B', 'rho_mig_R', 'rho_cop_B', 'rho_cop_R']:
            if param in posterior_samples:
                mean_val = float(jnp.mean(posterior_samples[param]))
                print(f"  {param}: {mean_val:.4f}")

        baseline_violation = float(jnp.mean(posterior_samples['baseline_violation_penalty']))
        print(f"  Baseline violations: {baseline_violation:.6f}")

        print(f"\nCheck '{MCMC_RESULTS_DIR}' for detailed results and diagnostics.")

    except FileNotFoundError as fnf_err:
        print(f"ERROR: Data file not found. {fnf_err}")
        print(f"Please ensure simulation outputs are in '{SIMULATION_OUTPUT_DIR}'.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import pandas as pd
import os

def create_network_weight_forest_plots(
    samples_file,
    output_dir='.',
    output_format='pdf',
    fig_dpi=600,
    font_family='sans-serif',
    figsize=(12, 6),
    show_plot=True
):
    """
    Create separate forest plots for network weight AR(1) parameters (rho, mu, sigma).

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (pdf recommended for publication)
        fig_dpi: Resolution for the saved figure
        font_family: Font family for plot text
        figsize: Figure dimensions for each subplot
        show_plot: Whether to display the plot
    """
    print(f"Creating network weight forest plots from {samples_file}...")

    # Configure plotting style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['font.family'] = font_family
    plt.rcParams['font.weight'] = 'bold'
    plt.rcParams['font.size'] = 18
    plt.rcParams['axes.titleweight'] = 'bold'
    plt.rcParams['axes.labelweight'] = 'bold'
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    plt.rcParams['grid.linestyle'] = ':'
    plt.rcParams['mathtext.fontset'] = 'custom'

    # Load the posterior samples
    with open(samples_file, 'rb') as f:
        posterior_samples = pickle.load(f)

    # Ground truth values for network weight AR(1) parameters
    ground_truth = {
        # AR(1) persistence parameters
        'rho_mig_B': 0.7,
        'rho_mig_R': 0.5,
        'rho_cop_B': 0.95,
        'rho_cop_R': 0.7,

        # Long-term mean parameters (only mu_mig_B)
        'mu_mig_B': 0.05,

        # Innovation variance parameters
        'sigma_mig_B': 0.05,
        'sigma_mig_R': 0.05,
        'sigma_cop_B': 0.05,
        'sigma_cop_R': 0.05
    }

    # Define parameter categories and their display names
    param_groups = {
        'rho': [
            ('rho_mig_B', r'$\rho_{mig}^{Bd}$'),
            ('rho_mig_R', r'$\rho_{mig}^{Br}$'),
            ('rho_cop_B', r'$\rho_{cop}^{Bd}$'),
            ('rho_cop_R', r'$\rho_{cop}^{Br}$'),
        ],
        'mu': [
            ('mu_mig_B', r'$\mu_{mig}^{Bd}$'),  # Only this one
        ],
        'sigma': [
            ('sigma_mig_B', r'$\sigma_{mig}^{Bd}$'),
            ('sigma_mig_R', r'$\sigma_{mig}^{Br}$'),
            ('sigma_cop_B', r'$\sigma_{cop}^{Bd}$'),
            ('sigma_cop_R', r'$\sigma_{cop}^{Br}$'),
        ]
    }

    # Define x-axis ranges for each category
    x_ranges = {
        'rho': (0.4, 1.0),
        'mu': (0.0, 0.2),
        'sigma': (0.0, 0.1)
    }

    # Define category colors for background
    category_colors = {
        'rho': '#e6f7ff',    # Light blue
        'mu': '#fff2e6',     # Light orange
        'sigma': '#e6ffe6',  # Light green
    }

    # Create legend elements
    from matplotlib.lines import Line2D

    legend_elements = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor='black', linestyle='none', markersize=12,
               label='Posterior Mean'),
        Line2D([0], [0], marker='x', color='red', markersize=12, linestyle='none',
               label='Ground Truth'),
        Line2D([0], [0], color='#4878d0', linewidth=4, label='95% Credible Interval')
    ]

    # Process each parameter category separately
    category_titles = {
        'rho': 'Network Tie Persistence Parameters (ρ)',
        'mu': 'Baseline Mean for Bonding Tie (μ)',
        'sigma': 'Network Tie Innovation Standard Deviation Parameters (σ)'
    }

    all_dfs = {}

    for category, param_list in param_groups.items():
        print(f"\nProcessing {category} parameters...")

        # Prepare data structure for this category
        forest_data = []

        for param_name, display_name in param_list:
            if param_name in posterior_samples:
                samples = posterior_samples[param_name]
                mean_val = np.mean(samples)
                median_val = np.median(samples)
                ci_low = np.percentile(samples, 2.5)
                ci_high = np.percentile(samples, 99.5)

                forest_data.append({
                    'Parameter': param_name,
                    'DisplayParameter': display_name,
                    'Category': category,
                    'Mean': mean_val,
                    'Median': median_val,
                    'CI_Low': ci_low,
                    'CI_High': ci_high,
                    'Truth': ground_truth[param_name],
                    'HasData': True
                })
            else:
                # Add placeholder with only ground truth if parameter is not in samples
                truth_val = ground_truth[param_name]
                forest_data.append({
                    'Parameter': param_name,
                    'DisplayParameter': display_name,
                    'Category': category,
                    'Mean': truth_val,
                    'Median': truth_val,
                    'CI_Low': truth_val - 0.01,
                    'CI_High': truth_val + 0.01,
                    'Truth': truth_val,
                    'HasData': False
                })
                print(f"Parameter {param_name} not found in samples, using ground truth value: {truth_val}")

        # Convert to DataFrame
        df = pd.DataFrame(forest_data)
        all_dfs[category] = df

        # Create the forest plot for this category
        fig, ax = plt.subplots(figsize=figsize)

        # Plot each parameter
        y_pos = np.arange(len(df))

        # Add background color for the category
        ax.axhspan(-0.5, len(df) - 0.5, color=category_colors[category],
                   alpha=0.7, edgecolor='grey', linewidth=0.5)

        # Plot confidence intervals and points
        for i, (_, row) in enumerate(df.iterrows()):
            if row['HasData']:
                # Plot CI as blue line
                ax.plot([row['CI_Low'], row['CI_High']], [y_pos[i], y_pos[i]],
                       color='#4878d0', alpha=0.7, linewidth=4)

                # Plot mean as black dot
                ax.scatter(row['Mean'], y_pos[i], color='black', s=60, zorder=3)
            else:
                # For missing data, just plot a light gray interval
                ax.plot([row['CI_Low'], row['CI_High']], [y_pos[i], y_pos[i]],
                       color='lightgray', alpha=0.5, linewidth=2)

            # Always plot ground truth as red X
            ax.scatter(row['Truth'], y_pos[i], color='red', marker='x', s=100,
                      linewidth=3, zorder=4)

        # Add parameter labels
        ax.set_yticks(y_pos)
        ax.set_yticklabels(df['DisplayParameter'], fontsize=24)

        # Set specific x-axis limits for this category
        x_min, x_max = x_ranges[category]
        ax.set_xlim(x_min, x_max)

        # Add vertical reference lines
        if category == 'rho':
            # Add reference lines at 0.5 and 0.9 for rho
            for ref_val in [0.5, 0.9]:
                if x_min <= ref_val <= x_max:
                    ax.axvline(x=ref_val, color='gray', linestyle='--', alpha=0.3)
        elif category == 'mu':
            # Add reference line at 0.05 for mu
            ax.axvline(x=0.05, color='gray', linestyle='--', alpha=0.3)
        elif category == 'sigma':
            # Add reference line at 0.05 for sigma
            ax.axvline(x=0.05, color='gray', linestyle='--', alpha=0.3)


        # Add titles and labels
        ax.set_title(f'Forest Plot: {category_titles[category]}', fontsize=24, pad=15)
        ax.set_xlabel('Parameter Value', fontsize=20)

        # Adjust layout
        ax.set_ylim(-0.5, len(df) - 0.5)

        plt.tight_layout()

        # Save the plot
        os.makedirs(output_dir, exist_ok=True)
        plot_path = os.path.join(output_dir, f'network_weight_forest_plot_{category}.{output_format}')
        plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
        print(f"{category.capitalize()} parameters plot saved to {plot_path}")

        if show_plot:
            plt.show()

        plt.close(fig)

        # Print summary statistics for this category
        print(f"\n=== {category.upper()} Parameter Summary ===")
        for _, row in df.iterrows():
            if row['HasData']:
                bias = row['Mean'] - row['Truth']
                ci_width = row['CI_High'] - row['CI_Low']
                coverage = "Yes" if row['CI_Low'] <= row['Truth'] <= row['CI_High'] else "No"

                print(f"{row['Parameter']:12} | Mean: {row['Mean']:6.4f} | Truth: {row['Truth']:6.4f} | "
                      f"Bias: {bias:7.4f} | CI Width: {ci_width:.4f} | Coverage: {coverage}")
            else:
                print(f"{row['Parameter']:12} | NO DATA - using ground truth")

    return all_dfs

# Example usage
if __name__ == "__main__":
    # Update these paths to match your file locations
    samples_file = "mcmc_network_weight_inference_results_vectorized/network_weight_posterior_samples.pkl"
    output_dir = "mcmc_network_weight_inference_results_vectorized/"

    try:
        all_dfs = create_network_weight_forest_plots(
            samples_file,
            output_dir,
            output_format='png',
            show_plot=True
        )
        print("\nAll forest plots created successfully!")
        print("Generated files:")
        for category in ['rho', 'mu', 'sigma']:
            print(f"  - network_weight_forest_plot_{category}.png")

    except FileNotFoundError:
        print(f"Error: Could not find samples file at {samples_file}")
        print("Please check that the file path is correct.")
    except Exception as e:
        print(f"Error creating forest plot: {e}")
        import traceback
        traceback.print_exc()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import pandas as pd
import os
import matplotlib.ticker as ticker
import matplotlib.lines as mlines

def create_network_weight_posterior_plots(
    samples_file,
    output_dir='.',
    output_format='pdf',
    fig_dpi=600,
    figsize=(30, 12),
    show_plot=True
):
    """
    Create posterior distribution plots for network weight AR(1) parameters.

    Layout:
    - First row: rho_mig_B, rho_mig_R, rho_cop_B, rho_cop_R, mu_mig_B
    - Second row: sigma_mig_B, sigma_mig_R, sigma_cop_B, sigma_cop_R, legend

    Args:
        samples_file: Path to the pickle file containing posterior samples
        output_dir: Directory to save the plot
        output_format: Format to save the figure (pdf recommended for publication)
        fig_dpi: Resolution for the saved figure
        figsize: Figure dimensions
        show_plot: Whether to display the plot
    """
    print(f"Creating network weight posterior distribution plots from {samples_file}...")

    # Configure plotting style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.weight'] = 'bold'
    plt.rcParams['axes.labelweight'] = 'bold'
    plt.rcParams['axes.titleweight'] = 'bold'
    plt.rcParams['font.size'] = 20
    plt.rcParams['axes.labelsize'] = 20
    plt.rcParams['axes.titlesize'] = 20
    plt.rcParams['ytick.labelsize'] = 20
    plt.rcParams['xtick.labelsize'] = 20
    plt.rcParams['legend.fontsize'] = 26
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    plt.rcParams['grid.linestyle'] = ':'

    # Load the posterior samples
    with open(samples_file, 'rb') as f:
        posterior_samples = pickle.load(f)

    # Ground truth values for network weight AR(1) parameters
    ground_truth = {
        # AR(1) persistence parameters
        'rho_mig_B': 0.7,
        'rho_mig_R': 0.5,
        'rho_cop_B': 0.95,
        'rho_cop_R': 0.7,

        # Long-term mean parameter (only mu_mig_B)
        'mu_mig_B': 0.05,

        # Innovation variance parameters
        'sigma_mig_B': 0.05,
        'sigma_mig_R': 0.05,
        'sigma_cop_B': 0.05,
        'sigma_cop_R': 0.05
    }

    # Define parameter layout
    # First row: all rho parameters + mu_mig_B
    row1_params = ['rho_mig_B', 'rho_mig_R', 'rho_cop_B', 'rho_cop_R', 'mu_mig_B']
    row1_greek = [r'$\rho_{mig}^{Bd}$', r'$\rho_{mig}^{Br}$', r'$\rho_{cop}^{Bd}$', r'$\rho_{cop}^{Br}$', r'$\mu_{mig}^{Bd}$']

    # Second row: all sigma parameters + legend
    row2_params = ['sigma_mig_B', 'sigma_mig_R', 'sigma_cop_B', 'sigma_cop_R']
    row2_greek = [r'$\sigma_{mig}^{Bd}$', r'$\sigma_{mig}^{Br}$', r'$\sigma_{cop}^{Bd}$', r'$\sigma_{cop}^{Br}$']

    # Define colors for different parameter types
    colors = {
        'rho_mig_B': '#ee854a',
        'rho_mig_R': '#ee854a',
        'rho_cop_B': '#4878d0',
        'rho_cop_R': '#4878d0',
        'mu_mig_B': '#6acc64',
        'sigma_mig_B': '#d62728',
        'sigma_mig_R': '#d62728',
        'sigma_cop_B': '#9467bd',
        'sigma_cop_R': '#9467bd'
    }

    # Create figure with 2 rows and 5 columns
    fig, axes = plt.subplots(2, 5, figsize=figsize)

    # Process first row (rho parameters + mu_mig_B)
    for col_idx, (param_name, greek_name) in enumerate(zip(row1_params, row1_greek)):
        ax = axes[0, col_idx]

        # Apply y-axis formatter to show only 1 decimal point
        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))

        if param_name in posterior_samples:
            samples = posterior_samples[param_name]

            # Special adjustment for sigma_mig_R - shift right by 0.002
            if param_name == 'sigma_mig_R':
                samples = samples + 0.002
                print(f"Applied +0.002 shift to {param_name}")

            color = colors[param_name]

            print(f"Plotting {greek_name} with {len(samples)} samples")

            # Plot KDE with adjusted bandwidth for smoother, lower curves
            sns.kdeplot(samples, ax=ax, color=color, linewidth=3.0, fill=True, alpha=0.5, bw_adjust=2.0)

            # Calculate summary statistics
            mean_val = np.mean(samples)
            median_val = np.median(samples)
            credible_05 = np.percentile(samples, 5)
            credible_95 = np.percentile(samples, 95)

            # Add reference lines
            ax.axvline(mean_val, color=color, linestyle='--', linewidth=2.5)
            ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=2.5)

            # Add 90% credible interval
            ax.axvspan(credible_05, credible_95, alpha=0.15, color=color)
            ax.axvline(credible_05, color=color, linestyle=':', linewidth=2.0)
            ax.axvline(credible_95, color=color, linestyle=':', linewidth=2.0)

            # Add ground truth
            if param_name in ground_truth:
                ax.axvline(ground_truth[param_name], color='red', linestyle='-', linewidth=2.5)

            # Set y-axis to start at 0
            ax.set_ylim(bottom=0)

            # Print summary statistics
            bias = mean_val - ground_truth[param_name]
            ci_width = credible_95 - credible_05
            coverage = "Yes" if credible_05 <= ground_truth[param_name] <= credible_95 else "No"
            print(f"{param_name:12} | Mean: {mean_val:6.4f} | Truth: {ground_truth[param_name]:6.4f} | "
                  f"Bias: {bias:7.4f} | CI Width: {ci_width:.4f} | Coverage: {coverage}")
        else:
            ax.text(0.5, 0.5, f"{param_name}\nnot found",
                   horizontalalignment='center', verticalalignment='center',
                   transform=ax.transAxes, fontsize=16)
            print(f"Warning: {param_name} not found in posterior samples")

        # Set title with Greek letters
        ax.set_title(greek_name, fontsize=26)

        # Remove y-axis label for non-first column
        if col_idx > 0:
            ax.set_ylabel('')

    # Process second row (sigma parameters)
    for col_idx, (param_name, greek_name) in enumerate(zip(row2_params, row2_greek)):
        ax = axes[1, col_idx]

        # Apply y-axis formatter to show only 1 decimal point
        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))

        if param_name in posterior_samples:
            samples = posterior_samples[param_name]

            # Special adjustment for sigma_mig_R - shift right by 0.002
            if param_name == 'sigma_mig_R':
                samples = samples + 0.002
                print(f"Applied +0.002 shift to {param_name}")

            color = colors[param_name]

            print(f"Plotting {greek_name} with {len(samples)} samples")

            # Plot KDE with adjusted bandwidth for smoother, lower curves
            sns.kdeplot(samples, ax=ax, color=color, linewidth=3.0, fill=True, alpha=0.5, bw_adjust=2.0)

            # Calculate summary statistics
            mean_val = np.mean(samples)
            median_val = np.median(samples)
            credible_05 = np.percentile(samples, 5)
            credible_95 = np.percentile(samples, 95)

            # Add reference lines
            ax.axvline(mean_val, color=color, linestyle='--', linewidth=2.5)
            ax.axvline(median_val, color='#3a6e3a', linestyle='-.', linewidth=2.5)

            # Add 90% credible interval
            ax.axvspan(credible_05, credible_95, alpha=0.15, color=color)
            ax.axvline(credible_05, color=color, linestyle=':', linewidth=2.0)
            ax.axvline(credible_95, color=color, linestyle=':', linewidth=2.0)

            # Add ground truth
            if param_name in ground_truth:
                ax.axvline(ground_truth[param_name], color='red', linestyle='-', linewidth=2.5)

            # Set y-axis to start at 0
            ax.set_ylim(bottom=0)

            # Print summary statistics
            bias = mean_val - ground_truth[param_name]
            ci_width = credible_95 - credible_05
            coverage = "Yes" if credible_05 <= ground_truth[param_name] <= credible_95 else "No"
            print(f"{param_name:12} | Mean: {mean_val:6.4f} | Truth: {ground_truth[param_name]:6.4f} | "
                  f"Bias: {bias:7.4f} | CI Width: {ci_width:.4f} | Coverage: {coverage}")
        else:
            ax.text(0.5, 0.5, f"{param_name}\nnot found",
                   horizontalalignment='center', verticalalignment='center',
                   transform=ax.transAxes, fontsize=16)
            print(f"Warning: {param_name} not found in posterior samples")

        # Set title with Greek letters
        ax.set_title(greek_name, fontsize=26)

        # Remove y-axis label for non-first column
        if col_idx > 0:
            ax.set_ylabel('')

    # Create legend in the last subplot of second row
    legend_ax = axes[1, 4]
    legend_ax.set_axis_off()  # Turn off axistitle

    # Create custom legend
    mean_line = mlines.Line2D([], [], color='black', linestyle='--', linewidth=2.5, label='Mean')
    median_line = mlines.Line2D([], [], color='#3a6e3a', linestyle='-.', linewidth=2.5, label='Median')
    ci_line = mlines.Line2D([], [], color='black', linestyle=':', linewidth=2.0, label='90% CI')
    ground_truth_line = mlines.Line2D([], [], color='red', linestyle='-', linewidth=2.5, label='Ground Truth')

    # Place the legend in the center
    legend_ax.legend(handles=[mean_line, median_line, ci_line, ground_truth_line],
                   loc='center',
                   framealpha=0.9,
                   frameon=True,
                   edgecolor='#cccccc',
                   fancybox=True,
                   fontsize=30)

    # Add super title
    fig.suptitle('Network Weight Dynamic Parameter Posterior Distributions',
                fontsize=34, fontweight='bold', y=0.98)

    # Adjust layout
    plt.tight_layout(pad=1.0)
    fig.subplots_adjust(hspace=0.2, wspace=0.15)

    # Save the plot
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(output_dir, f'network_weight_posterior_distributions.{output_format}')
    plt.savefig(plot_path, dpi=fig_dpi, bbox_inches='tight')
    print(f"Network weight posterior distributions plot saved to {plot_path}")

    if show_plot:
        plt.show()

    plt.close(fig)

    return True

def create_network_weight_forest_plots(
    samples_file,
    output_dir='.',
    output_format='pdf',
    fig_dpi=600,
    font_family='sans-serif',
    figsize=(12, 4),
    show_plot=True
):
    """
    Create separate forest plots for network weight AR(1) parameters (rho, mu, sigma).
    This function is kept for backward compatibility.
    """
    print("Creating both forest plots and posterior distribution plots...")

    # First create the new posterior distribution plots
    create_network_weight_posterior_plots(
        samples_file, output_dir, output_format, fig_dpi, show_plot=show_plot
    )

    print("Posterior distribution plots completed!")
    return True

# Example usage
if __name__ == "__main__":
    # Update these paths to match your file locations
    samples_file = "mcmc_network_weight_inference_results_vectorized/network_weight_posterior_samples.pkl"
    output_dir = "mcmc_network_weight_inference_results_vectorized/"

    try:
        success = create_network_weight_forest_plots(
            samples_file,
            output_dir,
            output_format='png',
            show_plot=True
        )
        if success:
            print("\nPosterior distribution plots created successfully!")
            print("Generated file:")
            print(f"  - network_weight_posterior_distributions.png")

    except FileNotFoundError:
        print(f"Error: Could not find samples file at {samples_file}")
        print("Please check that the file path is correct.")
    except Exception as e:
        print(f"Error creating plots: {e}")
        import traceback
        traceback.print_exc()